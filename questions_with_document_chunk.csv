,text,question
0,"City Univ ersity of New Y ork (CUN Y) City Univ ersity of New Y ork (CUN Y) 
CUN Y Academic W orks CUN Y Academic W orks 
Student Theses Baruch College 
Spring 5-18-2020 
Emer ging T echnologies in Healthcar e: Analysis of UNOS Data Emer ging T echnologies in Healthcar e: Analysis of UNOS Data 
Through Machine Learning Through Machine Learning 
Reyhan Mer ekar 
CUN Y Bernar d M Baruch College 
How does access t o this work benefit y ou? Let us know! 
More information about this work at: https:/ /academicworks.cuny .edu/bb_etds/100 
Disco ver additional works at: https:/ /academicworks.cuny .edu 
This work is made publicly a vailable b y the City Univ ersity of New Y ork (CUN Y). 
Contact: AcademicW orks@cuny .edu",What is the title of the thesis written by Reyhan Merekar at CUNY Baruch College?
1,"Emerging Technologies in Healthcare : Analysis of UNOS Data Through Machine Learning  
 
 
by 
 
 
Reyhan Merekar  
 
 
Submitted to the Committee on Undergraduate Honors at Baruch College of the City University 
of New York in partial fulfillment of the requirements for the degree of Bachelor of Business 
Administration in Computer Information Systems with Honors  
 
 
April 27 th, 2020",What is the main focus of Reyhan Merekar's research project as outlined in the context information?
2,"Table of Contents  
 
Acknowledgements  ................................ ................................ ................................ ........................  i 
Abstract  ................................ ................................ ................................ ................................ ..........  ii 
Chapter 1: Introduction  ................................ ................................ ................................ ...............  1 
Chapter 2: Background & Additional Context  ................................ ................................ ..........  3 
Moving to Performance Based Medicine  ................................ ................................ ..............................  3 
Virtual Visits and Wearables  ................................ ................................ ................................ .................  4 
Leveraging Artificial Intelligence  ................................ ................................ ................................ ..........  4 
Emergence of Big Data in Healthcare  ................................ ................................ ................................ ... 5 
Chapter 3: Related Work  ................................ ................................ ................................ .............  7 
Chapter 4: Data & Methodology  ................................ ................................ ...............................  10 
Busi ness Understanding  ................................ ................................ ................................ ........................  11 
Data Understanding  ................................ ................................ ................................ ..............................  11 
Data Preparation  ................................ ................................ ................................ ................................ ... 11 
Model Building  ................................ ................................ ................................ ................................ ...... 12 
Discussion of Rele vant Algorithms  ................................ ................................ ................................ ....................  13 
Resampling Methods  ................................ ................................ ................................ ................................ ..........  17 
Model Evaluation  ................................ ................................ ................................ ................................ .. 18 
Key Metric: Area  Under the Receiver Operating Characteristic Curve (AUC)  ................................ .................  20 
Variable Analysis  ................................ ................................ ................................ ................................ ...............  20 
Chapter 5: Results ................................ ................................ ................................ .......................  21 
Clinically Validated Variables  ................................ ................................ ................................ .............  23 
Chapter 6: Discussion & Insights  ................................ ................................ ..............................  25 
Chapter 7: Conclusion  ................................ ................................ ................................ ................  28 
Chapter 8: Implications  ................................ ................................ ................................ ..............  29 
Data Priva cy ................................ ................................ ................................ ................................ ...........  29 
Data Standardization  ................................ ................................ ................................ ............................  29 
Existing Workforce  ................................ ................................ ................................ ...............................  30 
Chapter 9: Lim itations & Future Directions ................................ ................................ ............  31 
Can mortality be predicted independent of time period?  ................................ ................................ .. 31 
How robust will these models be? How will they scale?  ................................ ................................ .... 31 
Appendix  ................................ ................................ ................................ ................................ ...... 32 
References  ................................ ................................ ................................ ................................ .... 37",Explain the significance of leveraging artificial intelligence in the context of healthcare as discussed in Chapter 2.
3,"Merekar i  
 
Acknowledgements  
 First and foremost, I would like to thank my Faculty Advisor, Professor Arturo 
Castellanos, for providing feedback and guidance throughout this project and most of my 
undergraduate career. We have worked on a few projects together, and it has been a privil ege to 
learn from him throughout these four years. Additionally, I would like to thank my two Faculty 
Readers, Professors Kevin Craig and Zeda Li, for taking time to read my work and provide 
important feedback.  
 Special thanks to Claudio A . Bravo, Miguel A lvarez, and Mahek Shah for devoting time 
out of their busy schedules to provide valuable insight and context throughout this project.  
 Of course, I would like to thank my family for continuously supporting me through my 
undergraduate studies. All of this w ould not be possible without their belief in me.",Who is the Faculty Advisor mentioned in the acknowledgements section and what role did they play in the project?
4,"Merekar ii  
Abstract  
 The healthcare industry is primed for a massive transformation in the coming decades 
due to emerging technologies such as Artificial Intelligence (AI) and Machine Learning. Wi th a 
practical application to the UNOS (United Network of Organ Sharing) database, this Thesis 
seeks to investigate how Machine Learning and analytic methods may be used to predict one-
year heart transplantation outcomes. This study also sought to improve on predictive 
performances from prior studies by analyzing  both Donor and Recipient data. Models built wi th 
algorithms such as Stacking and Tree Boosting gave the highest performance, with AUC’s of 
0.681 0 and 0.68 04, respectively. In this work, a roadmap w as created that justifies the need for 
these technologies in healthcare. In application, the data was prepared, models were built using 
advanced algorithms, and important variables were selected. These steps were continuously done 
with validation from expe rienced clinicians. To yield greater insights in this study, the dataset 
was split row -wise by factors such as LVAD Support, Donor/Recipient Gender Combinations, 
and Time Period; this rendered 8 new datasets for analysis. This work explores the trade -off 
between interpretability and performance in applying analytic methods in a real -world problem 
in this domain. Finally, forward looking industry implications are discussed.",How did the study in the healthcare industry aim to use Machine Learning and analytic methods to predict one-year heart transplantation outcomes?
5,"Merekar 1 
 
Chapter 1: Introduction  
Heart transplantation has been carried out since the 1970s, but still remains one of the 
riskiest procedures today. Formally, a heart transplant is defined as the surgical replacement of 
the heart of a diseased individual with that of a healthy donor  (National Heart, Lung, and Blood 
Institute , “Heart Transplant” ). Typically, p atients who have end -stage heart failure, where the 
heart is severely damaged or weakened, undergo this procedure. Heart failure is caused by 
conditions such as coronary heart disease, hereditary conditions, and/or viral infections (National 
Heart, Lung, and Blood Institute , “Heart Transplant” ). A patient in need of a heart transplant can 
locate donor organs through the United Network for Organ Sharing. This private, non -profit 
organization manages the United States’  organ transplant system and provides a computerized 
national waiting list which assures equal access and fair distribution of organs as they become 
available  (United Network for Organ Sharing, “About UNOS”) . 
Several risk factors are associated with heart transplantation . The first is primary graft 
dysfunction (PGF), which occurs when the donor heart fails and is unable to function (National 
Heart, Lung, and Blood Institute , “Risk Factors” ). This is an immediate issue and usually leads 
to a quick time of de ath for the patient. It is also a major contributor to mortality and 
additionally, may lead other complications (Iyer et al. 1 -2). The patient’s immune system may 
also reject the newly transplanted heart within the first six months of transplantation. To c ombat 
this, the patient must take additional medicine to suppress the immune system. Long term side 
effects associated with this medicine include diabetes, osteoporosis, and kidney damage 
(National Heart, Lung, and Blood Institute , “Risk Factors”).","What is the definition of a heart transplant according to the National Heart, Lung, and Blood Institute?"
6,"Merekar 2 
 
The p resence of technology in healthcare, particularly data science, has begun to emerge 
within  the past few decades. Professionals are beginning to explore the implications of using data 
to provide reliable solutions. H eart transplantation  procedures  are prime d to increase in the 
coming years , which will be driven by the aging population in the United States. According to 
Primers in Medicine, the number of people older than 65 will “double  by 2060 ” (Gedela et al. 
19). Another key trend is heavy investment in AI  and Machine Learning. As reported by 
Accenture, AI investment by healthcare firms will increase to $6B by 2021  (Collier et al. 2).  As 
more people are at risk for end -stage heart failur e and other diseases, AI  and Machine Learning  
can be leveraged to incr ease predictive power  for heart transplantation outcomes and disease 
detection.  
This project aims to  use Machine Learning 1 techniques to predict one -year heart 
transplantation outcomes using queried data from the UNOS registry from 1990 -2016. It also 
strives to build on prior studies in this domain. Predictive models are constructed to help 
clinicians better understand underl ying patterns in Donor and Recipient data. Since the 
underlying models will be able to predict the likelihood of a patient’s survival after one year, 
they will allow the clinician (s) to take the appropriate course of action for treatment .
 
1. In this work, the terms Machine Learning and Data Mining are used interchangeably.","How is the presence of technology, particularly data science, impacting healthcare, specifically in the field of heart transplantation?"
7,"Merekar 3 
 
Chapter 2: Backgr ound & Additional Context  
 The imminence of technology within healthcare has grown rapidly and has been a driver 
of major industry shifts throughout the past decade. One example of such advancements is the 
emergence of AI, specifically, Machine Learning. F ormally defined,  it is the practice of using 
algorithms to build models that learn from any n-number of observations and try to emulate the 
underlying pattern of the phenomena (Beam and Kohane 1317). This concept allows the 
computer to autonomously make de cisions without instructions from the researcher and is better 
suited for higher dimensional datasets where relationships may not necessarily be linear (Beam 
and Kohane 1317). The following trends emphasize why AI will be prevalent and contribute to 
the ev olving landscape of healthcare/medicine.  
Moving to Performance Based Medicine  
The healthcare system in the United States is now moving toward a financial model based 
on value rather than volume. The onus is now on delivering excellent population health thr ough 
treating patients like members. Rather than accounting for revenue due to patient volume, this 
value -based model shows each visit as an expense rather than a source of revenue (Burrill, 
“Health Care Outlook for 2019: Five Trends That Could Impact Heal th Plans, Hospitals, and 
Patients ”). 
This shift will take time, as the current transition has not been entirely smooth. In the 
short term, healthcare firms may see financial hits before longer -term costs decrease. Despite 
this, the value -based model has been embraced as the best method in lowering healthcare costs 
while increasing the quality of care. Since patients are seeking the best care possible, Machine 
Learning can be a catalyst in  providing that, helping people live healthier lives (NEJM Catalyst, 
“What is Value Based Healthcare?”) .",Explain the concept of Machine Learning in the context of healthcare and how it differs from traditional methods of decision-making.
8,"Merekar 4 
 
Virtual Visits and Wearables  
 The general population dreads visiting the doctor, and often, waits until a later date when 
the condition has become more  severe to visit one. This mentality drives up costs for the patient. 
Virtual visits and telehealth serve as a basis to interact with a caregiver without attending the 
office. According to Steve Burrill of Deloitte, this technology helps them “see more pat ients, 
deal with rising clinical complexity, and support patients as they take a greater role in their own 
care.” There is much room to leverage this practice, as currently, only 14% of caregivers are 
utilizing this (Burrill, “ Health Care Outlook for 2019:  Five Trends That Could Impact Health 
Plans, Hospitals,  and Patients”).  
 As the popularity of wearables (e.g., Apple Watch, FitBit) grow, so does the data they 
transmit. The Internet of Medical Things (IoMT) is the health spin -off of the Internet of Thing s. 
This phenomenon can be explained as the “collection of medical, drug delivery devices and 
applications that connect to healthcare IT systems through online computer networks” (D et al. 
290). Medical devices equipped with Wi -Fi allow for machine -to-machi ne interactions. Such a 
phenomenon can help clinicians/healthcare professionals collect data points that may be used for 
disease prediction, patient status checks, and drug developments.  
Leveraging Artificial Intelligence  
 Artificial Intelligence lies at the center of all of these trends. For example, in a virtual 
visit, software can be used to track a person’s mood. Rather than meeting with someone, data 
from a patient’s Electronic Health Record (EHR) can help manage illnesses. Data from 
wearables  and tracking will be used to predict what a diagnosis may be, what drugs can be 
developed to help that will help the patient, and realistic timelines of treatment.",How can virtual visits and telehealth help caregivers in dealing with rising clinical complexity and supporting patients in taking a greater role in their own care?
9,"Merekar 5 
 
 AI aims to mimic human cognition. Rather than fully replace doctors, it must be used in a 
way where professionals are working with AI to enhance clinical decisions. Now, with various 
analytic methods and collections of EHR’s, this is becoming a reality. For example, it is already 
making waves in radiology, oncology, disease prediction/preventio n, and outcome prediction 
with AI system IBM Watson. The system includes underlying Machine Learning models but is 
also a pioneer in the field. According to Jiang et al., “99% of the treatment recommendations 
from Watson are coherent with the physician dec isions” (241). Several reviews have appeared in 
literature referencing similar analytic methods in healthcare; these have covered techniques, 
algorithms, and dataset evaluations. Additionally, research in this space is growing, as the 
number of published p apers has increased by nearly 300% from 2008 to 2015 (Srivastava et al. 
1665). This trend also contributes to the motivation for this study.  
Emergence of Big Data in Healthcare  
 Big data has an incredible potential to yield significant value in healthcar e. This will be 
driven by decreasing costs of data storage, access to powerful but remote cloud computing, 
proliferation of “smart” devices, and the increase in electronic communication. Take for example 
healthcare titan Kaiser Permanente, which consists o f approximately nine million members. The 
firm has the capability to manage up to 44 petabytes of data through its EHR. This is “4,400 
times the equivalent of the data stored in the Library of Congress” (Roski et al. 1115). This 
implies that there is a vas t amount of data readily available for analysis in healthcare.  
 Big data is typically understood as a combination of three concepts – volume, velocity, 
and variety. Volume refers to the amount of data currently present in an enterprise; many expert s 
assert that “90% of the data (stored)” has been created only over the last eight years (Sherman 4). 
Velocity measures the time sensitivity of data; reporting and analysis need to be immediate and","How is AI being used in healthcare to enhance clinical decisions, according to the text?"
10,"Merekar 6 
 
there is greater pressure to bridge the gap betwee n when the data is acquired and when it is 
analyzed. Finally, variety is the idea that data is now collected from many different sources. It 
can be structured (e.g., tabular) or unstructured (e.g., emails, documents, PowerPoints). In this 
work, these oppor tunities are taken advantage of, and findings from prior studies are considered .",How does the concept of velocity relate to the pressure to bridge the gap between data acquisition and analysis?
11,"Merekar 7 
 
Chapter 3: Related Work  
 As mentioned previously, this project builds  on prior studies in the field. Machine 
Learning and analytic methods have been explored not only for hear t transplantation, but in other 
realms of clinical decision making as well. For example, a study was conducted that analyzed the 
risk of acute kidney injury (AKI), which is associated with chronic kidney disease and poses a 
high risk of mortality (Parreco and Chatoor 725). Another study used machine learning 
techniques to predict remission outcomes of Type 2 Diabetes following bariatric surgery 
(Johnston et al. 580).  
 Although these studies are not directly related to heart transplantation, they do have 
aspects in common. Those studies, along with this one, are Classification problems where the 
response variable is binary, the methodology is relatively similar, and the same evaluation 
method of Area Under the Curve (AUC) is utilized.  Additionally, a common theme in Machine 
Learning studies in healthcare is the Logistic Regression model serves as a baseline for model 
comparison.  
 Moreover, directly related studies have been conducted with  the same  data in which 
Machine Learning was use d to predict one-year mortality. One example of a similar study 
include d a report from the Journal of Cardiac Failure, which compared results from traditional 
statistical techniques with more advanced techniques . This study employed six  variables: age of 
recipient, creatinine, body mass index, liver function tests, aspartate transaminase, and 
hemodynamics. With the given variables, models were created with traditional statistical 
techniques and machine learning algorithms; these were all evaluated  by the me tric AUC. The 
implementation of Deep Learning  models yielded the best AUC, which was roughly 0.66.  At the 
end, it was deemed that the implementation of more advanced techniques failed to yield a n",What are some examples of studies mentioned in the related work section that used machine learning techniques in healthcare decision making?
12,"Merekar 8 
 
improved result when compared to traditional approaches, as there was a modest discrepancy 
between the two (Miller et al. 3 -4). One possible drawback to this study was that the researchers 
only used the recipient data and did not emphasize the donor’s data. The study was also only 
limited to univaria te variables.  
In another  similar study , clinicians and data scientists utilized existing models to predict 
one-year mortality outcomes . The two models were IMPACT (Index for Mortality Prediction 
After Cardiac Transplantation) and IH TSA (International Heart  Transplantation Survival 
Algorithm) which both implemented Machine Learning and Deep Learning techniques. The 
IMPACT model yielded an AUC of 0.608 with 18 variables being used, while the IH TSA model 
yielded an AUC of 0.64 with 32 variables  (Medved et al.  3-6). One drawback to this study is that 
it was ambiguous as to which variables were used when each model was initially created .  
Another similar study was carried out where analytic methods were leveraged  to predict 
mortality outcomes, however, this study addressed a slightly different problem and diff ered in 
overall methodology. Various analytic techniques were still carried out, but this paper considered 
one, five, and nine -year mortality instead. To impute missing values for bias removal, the 
researchers used Synthetic Minority Oversampling TEchnique  (SMOTE) which is used to 
“improve random oversampling” (Blagus and Lusa 2). This technique is well suited for lower 
dimensional data but is not ideal in higher dimensional settings. The best performance this study 
was able to uncover were AUC’s of 0.624, 0.676, and 0.838 for one, five, and nine -year 
mortality, respectively (Dag et al. 47 -49).  
 In this study however, Donor and Recipient data will be analyzed through advanced 
algorithms to improve  overall predictive power in heart transplantation.  Additiona lly, the data 
will be split in various ways to potentially yield further insights and variables are to be","What were the two models used in a study to predict one-year mortality outcomes in heart transplantation, and what were their respective AUC values?"
13,"Merekar 9 
 
eliminated from the full dataset for interpretability. This is further explained in following 
chapters.",Why was Merekar 9 eliminated from the full dataset for interpretability?
14,"Merekar 10 
 
Chapter 4: Data  & Methodology  
 Data mi ning projects such as these have flexibility to be creative and dynamic. There is, 
however, an accepted framework known as CRISP -DM ( CRoss Industry Standard Process for 
Data Mining) which is independent in industry and technology used. This framework aims to 
organize the steps in a data mining project. CRISP -DM has been seen as beneficial for 
organizations conducting large data mining projects in terms of cost, reliability, and 
manageability. This is the overarching methodology used for this project to keep  a rigid structure 
and reduce errors. The relevant five elements of this framework are Business Understanding, 
Data Understanding, Data Preparation, Model Building, and Evaluation.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 1. Phases of the CRISP -DM Model for Data Mining/Machine Learning. CRISP -DM – a 
Standard Methodology to Ensure a Goo d Outcome - Data Science Central . 
https://www.datasciencecentral.com/profiles/blogs/crisp -dm-a-standard -methodology -to-ensure -a-
good -outcome .",Explain the significance of the CRISP-DM framework in data mining projects and how it helps organizations.
15,"Merekar 11 
 
Business Understanding  
 The initial phase of the framework involves defining the problem at hand solely from the 
perspective of the business. Following this, the problem is converted into one that can be solved 
by data mining (Wirth and Hipp 5). In this case, the project lies in the domain of 
healthcare/medicine, so that was kept in mind throughout the process. The clearly defined 
problem is to understand and predict one -year mortality after a heart transplant.  
Data Understanding  
 The next phase of the framework is understanding t he data. This begins with obtaining 
data and conducting elementary observations to pick up on any immediate trends. Here, data 
quality issues are also identified (Wirth and Hipp 5). The corresponding query from the UNOS 
database yields a 32018 by 558 data frame. Each row represents an individual patient, and each 
patient has descriptive variables. It was apparent that the quality of the data was not quite up to 
par; missing values were present which made conducting initial analyses difficult . The data types  
of some variables also were not correct.  
Data Preparation  
 The data preparation phase spans all activities used to create the final data set; the one(s) 
that will be used in the model building process. This phase may be repeated many times in a 
given project. Some tasks include feature selection, data cleaning, and additional data 
transformation. In this project, the data had to be “cleaned” (e .g., adjusting data types, removing 
features, dealing with missing values) (Wirth and Hipp 5). One example of an adjusted data type 
was the response  variable, “One_year_mortality_retransplant”. This had to be changed to a 
“factor” data type. According to Python’s pandas documentation, these data types take on a 
“limited, and usually fixed, number of possible value s.” Missing values were dealt with",Explain the initial phase of the framework in the context of the healthcare/medicine domain. How is the problem defined and converted for data mining?
16,"Merekar 12 
 
differently depending  on the algorithm used to build each model. Cleaning was validated with 
clinicians, as they provided more insight for understanding the variables. In this project, the 
target variable had categories “ Died” or “Has not died” signified by the binary 1 and 0, 
respectively. In terms of feature selection, initially, many variables were omitted due to lack of 
variability cutting down the count to 420. Feature Selection was used again to reduce the number 
of variables to improve interpretability. This is further discussed at the end of the chapter.  
 Additionally, the data was iteratively prepared by splitting on factors such as time period 
of transplant, LVAD support, and Donor/Recipient gender combinations. W ith respect to time 
period, two new datasets were derived based on transplantation dates before and after 2006. For 
gender, each donor/recipient combination was analyzed, yielding four new datasets. The four 
combinations were male donor/male recipient, mal e donor/female recipient, female donor/male 
recipient, and female donor/female recipient. A Left Ventricular Assist Device (LVAD) is used 
when a patient is near heart failure but cannot acquire a transplant immediately. This device is 
installed to assume t he role of providing blood to vital organs (Gedela et al. 19). Two datasets 
were created here to understand how the analyses would change depending on whether the 
recipient had an LVAD or not.  
Model Building  
 In this phase, models are built and tested. Usu ally, there are several techniques that can 
be used to create models (Wirth and Hipp 6). The specific algorithms used for this study were 
Logistic Regression, Decision Trees/Random Forest, Tree Boosting, Neural Networks (Deep 
Learning), and Stacking. Logis tic Regression is the most traditional algorithm used in medicine, 
so for the purposes of this project, it serve d as the baseline for comparison. All models were built 
using the H2O module in the Python programming language powered by Amazon Web Services.",What were the categories of the target variable in this project and how were they represented?
17,"Merekar 13 
 
Discussion of Relevant Algorithms  
1. Logistic Regression  
 Logistic Regression is a widely used algorithm when the desired outcome is a question of 
classification. Rather than modeling a specific response directly, Logistic Regression model s the 
probability that the desired outcome belongs in a particular category. As the algorithm deals with 
probability, the related  sigmoidal  function must only produce outputs between 0 and 1. Below is 
the general logistic function for a multivariate Logist ic Regression (see Fig . B in Appendix for 
visual representation):  
𝑝(𝑋)= 𝑒𝛽0+𝛽1𝑋1+⋯+𝛽𝑝𝑋𝑝
1+𝑒𝛽0+𝛽1𝑋1+⋯+𝛽𝑝𝑋𝑝, 
where  𝑋=(𝑋1,𝑋2,…,𝑋𝑝) are p predictors. In this work, th e model derived from this algorithm 
served as a baseline for the performance of other  “advanced”  models  (James et. al 130 -133). 
With respect to models built with Logistic Regression, missing values were filled by mean 
imputation.  
2. Decision Trees/Random Fores t 
 Decision Tree is a hierarchical structure composed of branches and nodes . These are 
essentially a collection of ‘if/else’ statements that split decisions into binary classifications (in 
this case, “Died”  or “Has not died” ). Formally, this algorithm involves stratifying or segmenting 
the data set into a number of simple regions a nd making predictions (James et. al 306 -312):",Explain the concept of Logistic Regression and its application in classification tasks. How does the sigmoidal function play a role in this algorithm?
18,"Merekar 14 
 
 
 
 
 
 
 
 
Although decision trees are relatively simple to interpret, their downfall lies in the fact 
that they are not robust. A small change in the data can yield quite a large change in the final 
estimated tree. To correct this problem, the Random Forest algorithm may be implemented, 
which is essentially a larg e collection of decision trees ( Amornsamankul et al.  3). This algorithm 
corrects the instability of Decision Trees by bootstrap aggregating and decorrelating trees. This 
leads to stronger predictive power and lessens the risk of overfitting, which is touch ed on later in 
this chapter. With these models, missing values were filled by using the mode of respective 
variables.a. Divide the predictor space (the set of possible values for X1, X2, …, Xp) into J distinct and 
non-overlapping regions, R1, R2,…, R J. 
b. Create each binary partition based on node purity . This can be quantified by the Gini 
Index, where a pure node would produce a value closer to 0 ( 𝑝̂𝑚𝑘 represents the 
proportion of the kth classification in the mth region) : 
𝐺= ∑𝑝̂𝑚𝑘(1−𝑝̂𝑚𝑘) 𝐾
𝑘=1 
c. For every observation that falls into the region Rj, the same prediction is made , which is 
simply the mode  of the response values for the training observations in Rj.  
 
 Figure 2. Decision Tree Algorithm . James, Gareth, et al. An Introduction to Statistical Learning . 
Springer New York, 2013. DOI.org (Crossref) , pp. 306 -312, doi:10.1007/978 -1-4614 -7138 -7.",Explain the concept of node purity in the context of decision trees. How is it quantified using the Gini Index?
19,"Merekar 15 
 
3. Tree Boosting  
 Another way to combat Decision Tree instability is by using Boosting. Tree Boosting 
(e.g., Gradient Boosting Machine, Extreme Gradient Boosting) is widely used in machine 
learning  to achieve optimal performance. It is an ensemble method that sequentially creates new 
members; the newest member is created to account for incorrectly labeled inst ances from 
previous learners to minimize the loss function (direct relationship to error). The results of new 
trees are then applied partially to the entire solution. The algorithm executes M boosting 
iterations to learn a function F(x) that outputs predic tions ŷ = F(x)  while simultaneously 
minimizing a loss function L (y, ŷ). At each iteration, a new estimator f(x) is added to correct the 
prediction of y for each instance in training. This is shown formally below:  
 
 
 
 
 
 
 
 
 
 
 
 
 a. Start with a function which approximate s the true relationship of x and y: 
𝐹𝑚+1(𝑥)= 𝐹𝑚(𝑥)+𝑓(𝑥)=𝑦 
𝑓(𝑥)=𝑦−𝐹𝑚(𝑥) 
b. This fits the model f(x) for the current boosting iteration to the errors above 
(difference of actual and predicted). This can be shown as a gradient descent 
algorithm when the los s function is the squared error:  
𝐿(𝑦,𝐹(𝑥))= 1
2(𝑦−𝐹(𝑥))2 
c. Let the summation of this loss function be denoted as J. The goal is to minimize J by 
adjusting F(xi), the function of a particular instance.",Explain the concept of Tree Boosting and how it helps combat Decision Tree instability in machine learning.
20,"This can be shown as a gradient descent 
algorithm when the los s function is the squared error:  
𝐿(𝑦,𝐹(𝑥))= 1
2(𝑦−𝐹(𝑥))2 
c. Let the summation of this loss function be denoted as J. The goal is to minimize J by 
adjusting F(xi), the function of a particular instance.  
𝐽= ∑𝐿(𝑦𝑖,𝐹(𝑥𝑖))
𝑖 
𝑑𝐽
𝑑𝐹(𝑥𝑖)=𝑑(∑𝐿(𝑦𝑖,𝐹(𝑥𝑖)) 𝑖 )
𝑑𝐹(𝑥𝑖)=𝐹𝑚(𝑥𝑖)− 𝑦𝑖 
d. Thus, errors are equal to the negative gradient of the squared error loss function:  
𝑓(𝑥)=𝑦−𝐹𝑚(𝑥)=−𝑑(∑𝐿(𝑦𝑖,𝐹(𝑥𝑖)) 𝑖 )
𝑑𝐹(𝑥𝑖) 
 
Figure 3 . Tree Boosting Algorithm . Mitchell, Rory, and Eibe Frank. “Accelerating the XGBoost Algorithm Using GPU 
Computing.” PeerJ Computer Science , vol. 3, July 2017, pp. 3-4. DOI.org (Crossref) , doi: 10.7717/peerj -cs.127 .",Explain how the gradient descent algorithm can be applied to minimize the squared error loss function in the context of tree boosting.
21,"Merekar 16 
 
By adding a model that approximates this, the loss function is further minimized (Mitchell and 
Frank 3 -4). 
Often in the building phase, this algorithm will produce the strongest model in all facets 
of evaluation as it is inherently robust. Some of the drawbacks to this is that Tree Boosting tends 
to overfit but can also be corrected by adjusting tree sizes by p runing (Chen and Guestrin 3 -5). 
With these models, missing values were filled by using the mode of respective variables.  
4. Artificial Neural Networks (Deep Learning)  
 Neural Networks have recently been adapted as a viable data analytic method. These 
networks  aim to emulate that of the human brain, as they contain “neurons” (linear or non -linear 
computing elements) interconnected in complex ways and organized in layers.  
 A simple perceptron 2 constructs a linear combination of the inputs called the net input. 
Thereafter, an activation function is linked to produce an output, this maps any real input to a 
bounded range. A functional link network introduces a hidden layer in the network. This uses 
nonlinear activation functions to produce a fully nonlinear model (in parameters). Th e resulting 
model is known as an MLP or multilayer perceptron. These models are flexible, general purpose, 
and non -linear and have the ability to yield multiple outputs from many inputs. Given enough 
data, this can approximate to a desir ed degree of accuracy. During building, numerical values 
were filled by using mean imputation, while categorical ones were omitted. An illustration of a 
sample neural network may be found in the Appendix  (Fig. C) (Sarle 2 -5). 
5. Stacking  
Stacking is a n approach for building up classifier ensembles; this refers to a collection of 
classifiers in which their decisions are put together to classify new instances. The algorithm
 
2. A perceptron is a single -layer Neural Network. An interconnected system of perceptrons (MLP) yields a Neural 
Network.",Explain the concept of Tree Boosting and its advantages in the building phase of model evaluation. How can the issue of overfitting be addressed in Tree Boosting?
22,"Merekar 17 
 
combines multiple classifiers to induce a higher -level classifier with improved p redictive 
performance (Sakkis et al. 1 -2). In this study, two variants are used. “Best of Family” combines 
the best models from each algorithm, and “All Models” merges all models in a given iteration of 
building. Missing values were handled by the base alg orithms of the ensemble.  
Resampling Methods  
 Generally, in Machine Learning projects, the researcher splits a given dataset into a 
training and test set based on a chosen ratio. The training set is used to build models upon, while 
the test set acts as a pr oxy for how the model will perform on future, unseen data. This method, 
known as the Hold -Out Method, introduces additional error.  
An important concept to understand in this domain is the bias-variance tradeoff  
(sometimes also referred to as the tradeoff between data -fit and complexity). The main point of 
model building is to showcase that it can generalize to unseen data. Maybe counter -intuitively, 
predictive performance is not maximized by learning the training data as precisely as possible. 
An extremely  close fit to the training data is typically not ideal because the model will pick up 
random fluctuations in the data (i.e. noise) and miss the “broader regularities” in the dataset 
(Briscoe and Feldman 3 -4). Using just the Hold -Out method can lead to high  bias, low variance, 
or vice -versa. An illustration of the bias -variance tradeoff may be found in the Appendix (Fig. 
D). 
 Alternatively, r esampling methods can be defined as iteratively fitting models on 
randomly drawn samples of given data. One of these methods, perhaps the most popular, is k-
Fold Cross Validation. The dataset is divided into k folds (row -wise), where each k – 1 folds 
becom e training sets while the remaining fold acts as a test set. An error value is calculated for 
the “test” fold, and finally, the average of each test fold becomes the overall error value",Explain the concept of bias-variance tradeoff in the context of model building and predictive performance.
23,"Merekar 18 
 
(Rodriguez et al. 569).  This was the resampling method of choice for t his project, with k = 5. An 
illustration may be found below:  
 
 
 
 
 
 
 
 
 
Compared to the simple Hold -Out method, k-Fold Cross Validation has advantages in 
both bias and variance. Bias is reduced in the sense that there are more observations “seen” by 
the model during training. Variance is also reduced due to the fact that there is smaller overlap 
between each training  set. Given these considerations, it is empirically proven that k = 5 or k = 
10 yield test errors that do not suffer from high bias nor very high variance (James et al. 181 -
184).  
Model Evaluation  
 At this point in the framework, multiple models have been b uilt and are ready to be 
compared. The models are constructed correctly and are assumed to be of the best quality 
possible. Before deployment of a model, it is vital to thoroughly evaluate each model, review 
steps taken to build the model, and justify it u sing the business context (Wirth and Hipp 6).  
 
Figure 4 . K-Fold Cross Validation . Ren, Qiubing, et al. “Tectonic Discrimination of Olivine in 
Basalt Using Data Mining Techniques Based on Major Elements: A Comparative Study from 
Multiple Perspectives.” Big Earth Data , vol. 3, no. 1, Jan. 2019, p. 14. DOI.org (Crossref) , 
doi:10.1080/20964471.2019.1572452 .",Explain the advantages of k-Fold Cross Validation over the simple Hold-Out method in terms of bias and variance.
24,"Merekar 19 
 
 Confusion matrices are used in evaluation of problems of binary classification. It is a 2x2 
table formed by counting the number of the four outcomes of a binary classifier. The two 
“classes” are actual and pr edicted with a positive/negative value for each. The four cells are 
filled with the number of True Positives (TP), False Positives (FP), False Negatives (FN), and 
True Negatives (TN) (Amornsamankul et al. 5).  
In this project, the outcome s of “Died” or “Has not died” are examined. True Positives in 
this case refer to the number of “Died” predicted when the actual value was also “Died.” False 
Positives are the number of “Has not died” predicted when the outcome was actually “Died.” 
False Negatives are the number of predict ed “Died” when the actual was “Has not died.” True 
Negatives are the number of predicted “Has not died” when the actual classification was “Has 
not died”. In this case, False Negatives are more costly than False Positives, so the metric of 
Sensitivity (Rec all) is also relevant in this study.",Explain the concept of a confusion matrix and how it is used in the evaluation of binary classification problems.
25,"5).  
In this project, the outcome s of “Died” or “Has not died” are examined. True Positives in 
this case refer to the number of “Died” predicted when the actual value was also “Died.” False 
Positives are the number of “Has not died” predicted when the outcome was actually “Died.” 
False Negatives are the number of predict ed “Died” when the actual was “Has not died.” True 
Negatives are the number of predicted “Has not died” when the actual classification was “Has 
not died”. In this case, False Negatives are more costly than False Positives, so the metric of 
Sensitivity (Rec all) is also relevant in this study.  
 
 
From this, prediction accuracy, sensitivity (recall), and specificity may be derived:  
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑃+𝑇𝑁
𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 
𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦  (𝑅𝑒𝑐𝑎𝑙𝑙 )= 𝑇𝑃
𝑇𝑃+𝐹𝑁 
𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 =𝑇𝑁
𝑇𝑁+𝐹𝑃   Predicted Class  
  Died  Has not died  
Actual Class  Died  TP FP 
Has not died  FN TN 
Figure 5 . One -year Mortality Confusion Matrix .","Explain the difference between True Positives, False Positives, False Negatives, and True Negatives in the context of predicting outcomes of 'Died' or 'Has not died'."
26,"Merekar 20 
 
Key Metric: Area Under the Receiver Operating Characteristic Curve (AUC)  
 The Receiver Operating Characteristic Curve (ROC Curve) has been identified as a 
viable soluti on of visualizing a classifier’s performance in order to select an optimal decision 
threshold. The term was coined during World War II when it was used in “Signal Detection 
Theory” for radars, but later made its way to diagnostic medicine. It was determine d that an 
“ideal” threshold is almost always a trade -off between sensitivity (True Positives) and specificity 
(True Negatives). Since it may be difficult for a researcher to imagine an ideal “cut -off”, this 
concept was visualized. The Y -Axis represents Sen sitivity while the X -Axis represents 
Specificity. In theory, a researcher would want to achieve both high Sensitivity and Specificity, 
but this is far from practical in application. Hence, there is a trade -off, and either one of the two 
metrics can be opti mized (Bradley 1145).  
 The area under the ROC Curve (referred to as AUC) is widely recognized as the measure 
of a diagnostic test’s discriminatory power. The value of AUC ranges from 0.0 to 1.0, where a 
value of 1.0 implies 100% sensitivity and specificity . A value of 0.5 indicates no discriminative 
value. This entails 50% sensitivity and 50% specificity  (Fan et al. 20) . In terms of this project, 
ROC curves were visualized for each model, and ranked based on AUC. Models with AUC ’s 
that performed better than  the baseline Logistic Regression were deemed the most useful . 
Variable Analysis  
 As there are greater than 400 variables in the full dataset, only significant variables were 
considered in individual variable analyses. Each model produced a list of important variables and 
these were brought to clinicians for validation. Additionally, to  further drill down the number of 
variables, the important variables  from the top model from each dataset was taken. These were 
then counted up to understand the shared important variables throughout all models .",Explain the concept of the Receiver Operating Characteristic Curve (ROC Curve) and its significance in visualizing a classifier's performance. How does the ROC Curve help in selecting an optimal decision threshold?
27,"Merekar 21 
 
Chapter 5: Results  
 After the methodology wa s carried out, initial results were obtained for each model per 
dataset. These contain the top two models from each dataset, the baseline Logistic Regression, 
algorithms, AUC measures, Sensitivity, and dataset dimensions. A detailed table depicting all of 
these may be found below.  
 
Algorithm  AUC  Sensitivity (Recall)  
Full Dataset ( 32,018  Observations , 420  Variables)  
Logistic Regression  0.63683332  0.4190646  
 Stacking, All Models  0.681168  N/A 
Extreme Gradient Boosting  0.680403  0.41853034  
 Patients with no LVAD (10,912  Observations , 420  Variables ) 
Logistic Regression  0.57672026  0.45374164  
 Stacking, All Models  0.65443  N/A 
Extreme Gradient Boosting  0.653004  0.41827255  
 Patients with an LVAD (7,700  Observations , 420  Variables ) 
Logistic Regression  0.58331451  
 0.5547305  
 Stacking, All Models  0.65692  
 N/A 
Extreme Gradient Boosting  0.654315  
 0.43852264  
 Patients Recorded Before 2006 (13,406  Observations , 420  Variables ) 
Logistic Regression  0.61300899  
 0.57854533  
 Stacking with All Models  0.674364  
 N/A 
Extreme Gradient Boosting  0.67345  
 0.41904527  
 Patients Recorded After 2006 (18,612  Observations , 420  Variables ) 
Logistic Regression  0.60626572  
 0.46481952  
 Stacking, All Models  0.668372  
 N/A 
Extreme Gradient Boosting  0.66386  
 0.4589736  
 Table  1. Model Performance by Dataset and Algorithm.","Explain the significance of the AUC measure in evaluating the performance of the models in the given dataset. How does the AUC value differ for Logistic Regression, Stacking with All Models, and Extreme Gradient Boosting across different patient groups?"
28,"Merekar 22 
 
Female Donor Female Recipient (2,713  Observations , 420  Variables ) 
Logistic Regression  0.54148685  
 0.5008384  
 Stacking, Best of Family  0.662545  
 N/A 
Extreme Gradient Boosting  0.677088  
 0.53623605  
 Female Donor Male Recipient (2,730  Observations , 420  Variables ) 
Logistic Regression  0.570355  
 0.47577724  
 Stacking, Best of Family  0.647494  
 N/A 
Extreme Gradient Boosting  0.635834  
 
 0.42375335  
 Male Donor Female Recipient (2,713  Observations , 420  Variables ) 
Logistic Regression  0.5310544  
 0.5507362  
 Stacking, All Models  0.651886  
 N/A 
Extreme Gradient Boosting  0.664648  
 0.47671908  
 Male Donor Male Recipient (11,306  Observations , 420  Variables ) 
Logistic Regression  0.57988709  
 0.45201996  
 Stacking, All Models  0.650807  
 N/A 
Extreme Gradient Boosting  0.647327  
 0.41651163  
  
After the initial analysis, a subset of the full dataset was taken. The data was filtered on the basis 
of important variables from the Extreme Gradient Boosting Algorithm on the full dataset and 
models were ran again. The results are displayed below:  
 
Full Dataset with Significant Variables (32,018, 25)  
Algorithm  AUC  Sensitivity (Recall)  
Logistic Regression  0.6603  
 0.4304224  
 Stacking, All Models  0.671926  
 N/A 
Extreme Gradient Boosting  0.670706  
 0.4328194  
  Table  2. Model Performance: Full Dataset with Significant Variables and Algorithms.",Explain the process of filtering the data based on important variables from the Extreme Gradient Boosting Algorithm. How did this process impact the subsequent analysis?
29,"Merekar 23 
 
Following that, more features were removed to make the model more interpretable. The top 16 
variables were chosen based on important variables  from the top model from each dataset. These 
were the significant variables that were most common throughout all m odels from all datasets. 
The models were run again, and the results are displayed below:  
 
Full Dataset with Clinically Significant Variables (32,018, 16) 
Algorithm  AUC  Sensitivity (Recall)  
Logistic Regression  0.6284  
 0.44540313  
 
 Stacking, Best of Family  0.6500 
 N/A 
Extreme Gradient Boosting  0.6491 
 0.46050477  
  
Clinically Significant  Variables  
 Analysis of variables included validation with clinicians. They were able to provide 
additional context as to whether or not models included variables that made sense from a 
physiological standpoint. The top 16 variables included in the most interpretable model are 
displayed on the next page.  Table  3. Model Performance: Full Dataset with Clinically Significant Variables and Algorithms.",Explain the process of selecting the top 16 variables for the most interpretable model in the study.
30,"Merekar 24 
 
 Top 16 Clinically Significant Variables with Descriptions  
Variable  Description   
Creatinine  of Recipient  Waste product filtered by kidney; a build -up may lead to 
cardiovascular disease.  
 
 Total Bil irubin  of 
Recipient  Associated with liver complications; a strong predictor in 
heart failure.  
 Ischemic Time  The time an organ has spent cooling/warming before 
transplant.  
 
 Most Recent Creatinine 
Measurement  of Recipient  Most recent evaluation of creatinine levels for a recipient 
at the time of listing.  
Age of Donor  Median Age: 32 years old  
Age of Recipient  Median Age: 55 years old  
PVR  of Recipient  Pulmonary vascular resistance for recipient measured at 
listing.  
Right Ventricular Mass of 
Donor  Both estimates of the myocardial mass of that ventricle 
based on echocardiographic measurements.  
Right Ventricular Mass of 
Recipient   
Systolic Pressure of 
Recipient  Pulmonary artery systolic pressure in mm/Hg at 
registration.  
Predictive Heart Mass 
Ratio  Total heart myocardial mass estimated by echocardiogram 
of donor divided by that of the recipient.  
Albumin Measurement of 
Recipient  Albumin measured in the recipient plasma at registration.  
Distance  Distance between the donor and the recipient  in nautical 
miles.  
Cardiac Output  of 
Recipient  Cardiac output of the recipient at registration.  
Recipient Waiting Days  Total days on the UNOS waiting list.  
GPT Measurement of 
Donor  GPT Level measured at time of transplant.  Table  4. Top 16 Clinically Significant Variables with Descriptions.",Explain the significance of the Ischemic Time variable in the context of organ transplantation.
31,"Merekar 25 
 
Chapter 6: Discussion & Insights  
After analyzing all datasets, it was apparent that advanced methods such as Tree Boosting 
(Extreme Gradient Boosting) and Stacking led to higher AUC’s than the baseline  Logistic 
Regression. This is because Tree Based algorithms are usually more complex. Although they 
may be prone to overfitting, pruning and correctly adjusting the number of trees resolves this 
issue. Logistic Regression is simpler to understand a nd less prone to overfitting, however, may 
not always grant optimal predictive performance.  
Running the Stacking algorithm on the full dataset yielded the highest AUC of about 
0.6810. Although this is significant, Stacking is just a collection of models, a nd cannot be truly 
used for interpretation. This is also why there were no Sensitivity metrics for those models. 
Aside from Stacking, Boosting also gave high results. When applied to the full dataset, the model 
created from this algorithm had an AUC of rou ghly 0.6804. In practice however, it would not be 
feasible for a cardiologist to assess 420 parameters (variables) when trying to understand the 
future of a patient. This brings in the idea of real -world use where models must be interpretable . 
When the ful l dataset was cut down by column, 25 variables remained. These were the most 
important variable s deemed by the Boosting model ran on  the full dataset. All algorithms were 
run on that subset of data and the Stacking algorithm yielded an AUC of 0.6719. The B oosting 
algorithm was very similar in performance, with a 0.6707 AUC. After counting recurring 
variables from all model variable importance plots, analysis was reduced to 16 variables. The 
Stacking algorithm gave an AUC of .6500 and the Boosting algorithm gave an AUC of .6491. 
This would be far more interpretable for clinicians as the parameters were brought down to only 
16 variables. Some performance is lost, but the difference is marginal (Fig. 6).",Explain why Tree Boosting and Stacking algorithms led to higher AUCs compared to Logistic Regression in the analysis of the datasets.
32,"Merekar 26 
 
 
 
 
 
 
 
 
 
 
The prior studies discussed in Chapter 3 did not achieve an AUC greater than 0.66 when 
using advanced learning techniques. Through optimal parameter tuning, the Tree Boosting 
algorithms were able to achieve higher performance. In those studies, however, th e trend was 
leaning towards Deep Learning being a long -term solution in this field. In this study, running the 
Deep Learning algorithm on the full dataset only produced an AUC of 0.5706. This could have 
been again, due to data quality issues as neural netw orks do not perform well on rather sparse 
datasets. Even after cleaning, the data was not suitable to implement an effective Deep Learning 
model.  
 From a clinical standpoint, the aforementioned variables are reasonable to build models 
with going forward. C reatinine levels are often greater in patients that are hospitalized with heart 
failure, which validates this variable as a driver of mortality (Smith et al. 14). Liver function 
abnormalities such as high levels of Total Bilirubin are also associated with higher morality (van 
Deursen et al.). Age of Recipient is a good indicator of mortality; older people generally have 
poorer health. Ischemic time and Age of Donor are also reasonable because these represent 
Figure 6. Visual Depiction of Trade -off Betw een Performance and Interpretability for Boosting Models. 
Average AUC on the Y -Axis corresponds to the average of the Top 2 models from the Full Dataset, Full 
Dataset with Significant Variables, and Full Dataset with Clinically Significant Variables.",Explain why the Deep Learning algorithm did not perform well on the full dataset in this study. What could have been the possible reasons for the low AUC of 0.5706?
33,"Merekar 27 
 
worse organ quality. Multiple studies have deemed  that age is an independent risk factor for 
mortality and argued that longer ischemi c times are associated with higher mortality outcomes 
(Kilic et al.).  
Another point to analyze is the comparisons of the row -wise dataset splits. Comparing the 
LVAD support datasets, it was apparent that there was not much of a difference between 
performance of those top models. The same can be said for time period, but models run on 
observations before 2006 yielded a marginally higher AUC. For Donor/Recipient gender 
combinations, it was found that the dataset containing Female Donors and Female Recipients 
gave an AUC of 0.6770. This exceeded the performance of all other Donor/Recipient gender 
combinations.  
Although the models ran on the row -wise splits did not  perform better than models from 
the full dataset with respect to AUC, they are useful in terms of Sensitivity. Albeit not discussed 
as thoroughly as AUC, it is meaningful to analyze this metric  in clinical decision making because 
in this case, False Negat ive Cases would be costly. The Tree Boosting model for the full dataset 
produced a Sensitivity of about .42. After running that algorithm on the Donor/Recipient gender 
combinations datasets, the average Sensitivity of the four was .463. Comparing those two  
metrics, Sensitivity of Donor/Recipient gender combinations saw an increase in Sensitivity by 
8% on average. This supports the claim that Donor/Recipient gender combinations add value in 
clinical decision making (see Fig. H in Appendix).",How does age contribute as an independent risk factor for mortality according to the information provided?
34,"Merekar 28 
 
Chapter 7: Concl usion  
This project aim ed to leverage Machine Learning techniques to predict one -year 
mortality after a heart transplant. Predictive models were constructed to help clinicians better 
understand underlying patterns in data from Donors and Recipients. This wa y, correct procedures 
for treatment may be taken from a medicinal standpoint. This study was continuously validated 
by clinicians, from Data Preparation to Evaluation. The data was split based on clinical factors 
such as LVAD Support, Donor/Recipient Gende r Combinations, and Time Period of Transplant. 
Finally, specific features were selected based on the clinician’s ability to interpret the created 
models.  
As mentioned previously, performance and interpretability vary inversely. This trade -off 
is important to understand in deployment. From the analysis done in this work, the Extreme 
Gradient Boosting model applied to the full dataset with 25 variables is the optimal one. It serves 
as a “middle -ground” between AUC and number of variables. This model performed  better than 
those from prior studies and uses 25 variables. Although it is not the most interpretable, the 
model will perform similarly when exposed to future, unseen data.  
Essentially, there are two schools of thought. The data scientist wants to optimiz e model 
performance, while the clinicians strive for the simplest model to understand. To that end, from a 
clinical standpoint, the model with 16 variables may be the most optimal. There is a modest 
discrepancy in AUC when compared to the model with 25 var iables, and theoretically, the 
clinician would trade marginal model performance for interpretability. The system needs to be 
fed with new data and using 16 parameters rather than 25 is more convenient, easier to 
track/measure, and simpler to enforce data i ntegrity constraints upon.",Explain the trade-off between model performance and interpretability as mentioned in the document. Why is it important to understand this trade-off in deployment?
35,"Merekar 29 
 
Chapter 8: Implications  
The last phase in the CRISP -DM Framework is Deployment  of a selected model. Rarely, 
creation of the model is not the end of the project as it still needs to be rolled out on a system, 
maintained, and updat ed periodically (Wirth and Hipp 7). It is important to continuously validate 
models built throughout the framework. By doing this, data scientists will be able to gain insight 
on whether or not models fit the context of the problem.  
Although there is a ne ed for the AI in healthcare as highlighted previously, there are a 
few pertinent issues when it comes to deployment and industry -wide adoption of these systems.  
Data Privacy  
 Data is essential for AI and model training, but some patients may be unwilling t o give 
other entities access to private records. Besides for training, a large data supply is needed for 
validation and improvement of these models. For widespread deployment, this sensitive data 
must be shared among numerous institutions. To combat this, these EHR’s must be anonymized 
and patients would need to be fairly informed. The shift to value -based care will support this, and 
further incentivize organizations to collect and ethically maintain this data for analysis (He et al. 
31). 
Data Standardizati on 
 From a data science standpoint, this is an important aspect to consider. Data 
standardization refers to the process of transforming data into a common format to be used for 
analysis. This way, it can be understood regardless of tools and methodologies (He et al. 33). In 
practice, data is collected in many different ways. It is stored in a variety of formats, databases, 
and information systems. Although this data may be formatted a certain way in one organization, 
if it is shared, another organization ma y not be able to properly interpret this for analysis. With",Explain the importance of continuously validating models built throughout the CRISP-DM Framework. How does this help data scientists gain insight into the context of the problem?
36,"Merekar 30 
 
the complexity and volume of healthcare data in particular, this should occur in the initial phases 
of model development, even before the CRISP -DM Framework is carried out.  
Existing Workforce  
 There has been significant concern throughout multiple industries of Artificial 
Intelligence eliminating the need for human workers. Although some jobs have potential to be 
automated, this will likely limit overall job loss. In healthcare, c osts of automation  technologies 
and regulatory and social acceptance are some reasons as to why this may be curbed (Davenport 
and Kalakota 96).  
To overcome these aforementioned challenges, the workforce itself must understand that 
AI is not here to replace it. Inst ead, the workforce will be able to leverage it to augment existing 
workflows and decision making. For an industry -wide implementation, healthcare professionals 
must develop trust for these systems. Similarly, the onus is on patients to trust institutions t o 
handle their data ethically in hopes of improving their own outcomes .",How can the healthcare industry overcome the challenges of AI potentially replacing human workers?
37,"Merekar 31 
 
Chapter 9: Limitations & Future Directions  
 There are a few limitations this research has faced, the first being the time period of the 
queried data. The UNOS Data spans from 1990 -2016 and does not take into account records after 
that. With the increased importance of data collection in the space, i t is reasonable to assume that 
future records will follow specific constraints. This entails that the data will contain less errors, 
yielding a more thorough analysis. The second limitation would be the interpretation of the 
variables in the full dataset. These are generally subjective since there are over 400, so another 
set of clinicians could have understood these variables from a different perspective than in this 
study. Finally, although discussed thoroughly, it was impossible to complete the CRISP -DM 
Framework. The optimal model was limited to simulation and could not  be deployed in a real use 
case.  
 For future studies, researchers will be able to leverage “cleaner” data from UNOS as data 
standards begin to conform. Aside from that, some questions are posed to future data 
scientist/clinician teams that have not been explored in this study:  
Can mortality be predicted independent of time period?  
 This is particularly significant as this project was limited to one -year mortality. If this 
limitation was rem oved, clinicians would be able to understand what may contribute to 
shorter/longer mortality periods.  
How robust will these models be? How will they scale?  
 Models must adapt to rapid change as new data is collected. Another point to examine 
would be identifying how these models would scale. This is useful from a data science 
perspective, and with the emergence of enterprise -wide data and cloud computing, analytic 
solutions (e.g. Random Forest, Tree Boosting ) have potential to scale rather well .",Explain the limitations faced by the research in terms of the time period of the queried data and the interpretation of variables in the full dataset.
38,"Merekar 32 
 
Appendix  
 
 
 
 
 
 
 
 
 
 
Figure A. Detailed Overview of the CRISP -DM Framework. Wirth, Rüdiger, and Jochen Hipp. 
CRISP -DM: Towards a Standard Process Model for Data Mining . p. 6.",What is the title of the document where the CRISP-DM framework is detailed?
39,"Merekar 33 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure B. Sigmoidal Function in Logistic Regression. Note, the minimum value is 0 and the 
maximum value is 1. Logistic Regression Theory for Practitioners - Towards Data Science . 
https://towardsdatascience.com/the -data-scientists -field-guide -to-logistic -regression -part-1-
intuition -97084b11bd68 . Accessed 16 Apr. 2020.  
 
 
 
 
 
 
 
Figure C. Sample Artificial Neural Network. Management AI: Types Of Machine Learning Systems . 
https://www.forbes.com/sites /davidteich/2018/07/06/management -ai-types -of-machine -learning -
systems/#390b5ba832fb . Accessed 16 Apr. 2020.",Explain the concept of logistic regression and how it is represented in Figure B.
40,"Merekar 34 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure D. Bias -Variance Tradeoff. Bias and Variance in Machine Learning - Data Driven Investor - 
Medium. https://medium.com/datadriveninvestor/bias -and-variance -in-machine -learning -51fdd38d1f86 . 
Accessed 16 Apr. 2020.  
 
 
 
 
 
 
 
 
F",Explain the concept of bias-variance tradeoff in machine learning and how it impacts model performance.
41,"Merekar 35 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure E. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset . 
 
 
 
 
 
 
 
 
F 
Figure F. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset  with Significant Variables.  
 
 
 
 
 
 
 
 
F 
Figure G. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset  with Clinically Significant Variables.",What is the purpose of using an Extreme Gradient Boosting model in this study?
42,"Merekar 36 
 
 
0.4633050280.42567487
0.4 0.41 0.42 0.43 0.44 0.45 0.46 0.47XGBoost Recall Averages Donor/Recipient
Gender CombinationsXGBoost Recall Averages for Full Datasets
(No Row Splits)
SENSITIVITY (RECALL)SENSITIVITY MEASURES FOR TREE 
BOOSTING
Figure H. Sensitivity Comparisons: Full Datasets and Donor/Recipient Gender Combinations.  
 
 
 
 
 
 
 
F",Explain the significance of sensitivity (recall) in the context of tree boosting algorithms.
43,"Merekar 37  
References  
Amornsamankul, Somkid , et al. “A Comparison of Machine Learning Algorithms and Their 
Applications.” International Journal of Simulation: Systems, Science & Technology , Aug. 2019, 
pp. 1 –17. DOI.org (Crossref) , doi: 10.501 3/IJSSST.a.20.04.08 . 
 
Beam, Andrew L., and Isaac S. Kohane. “Big Data and Machine Learning in Health Care.” JAMA , 
vol. 319, no. 13, Apr. 2018, pp. 1317 –18. DOI.org (Crossref) , doi: 10.1001/jama.2017.18391 . 
 
Blagus, Rok, and Lara Lusa. “SMOTE for High -Dimensional Class -Imbalanced Data.” BMC 
Bioinformatics , vol. 14, no. 1, Dec. 2013, pp. 1 –16. DOI.org (Crossref) , doi: 10.1186/1471 -2105 -
14-106. 
 
Bradley, Andrew P. “The Use of the Area under the ROC Curve in the Evaluation of Machine 
Learning Algorithms.” Pattern Recognition , vol. 30, no. 7, July 1997, pp. 1145 –59. DOI.org 
(Crossref) , doi: 10.1016/S0031 -3203(96)00142 -2. 
 
Briscoe, Erica, and Jacob Feldman. “Conceptual Complexity and the Bias/Variance Tradeoff.” 
Cognition , vol. 118, no. 1, Jan. 2011, pp. 2 –16. DOI.org (Crossref) , 
doi:10.1016/j.cognition.2010.10.004 . 
 
Burrill, Steve. “Health Care Outlook for 2019: Five Trends That Could Impact Health Plans, 
Hospitals, and Patients.” Deloitte United States .","Explain the significance of the Area under the ROC Curve in evaluating machine learning algorithms, as discussed in Bradley's 1997 paper."
44,"Bradley, Andrew P. “The Use of the Area under the ROC Curve in the Evaluation of Machine 
Learning Algorithms.” Pattern Recognition , vol. 30, no. 7, July 1997, pp. 1145 –59. DOI.org 
(Crossref) , doi: 10.1016/S0031 -3203(96)00142 -2. 
 
Briscoe, Erica, and Jacob Feldman. “Conceptual Complexity and the Bias/Variance Tradeoff.” 
Cognition , vol. 118, no. 1, Jan. 2011, pp. 2 –16. DOI.org (Crossref) , 
doi:10.1016/j.cognition.2010.10.004 . 
 
Burrill, Steve. “Health Care Outlook for 2019: Five Trends That Could Impact Health Plans, 
Hospitals, and Patients.” Deloitte United States . www2.deloitte.com , 
https://www2.deloitte.com/us/en/pages/life -sciences -and-health -care/articles/health -care-current -
december4 -2018.html . Accessed 2 Mar. 2020.  
 
Chen, Tianqi, and Carlo s Guestrin. “XGBoost: A Scalable Tree Boosting System.” Proceedings of the 
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - 
KDD ’16 , 2016, pp. 785 –94. arXiv.org , doi: 10.1145/2939672.2939785 . 
 
Collier, Matt, et al. Artificial Intelligence: Healthcare’s New Nervous System . Industry Outlook 
Report, Accenture, pp. 1 –8. 
 
D, Shashank, et al. “The Internet of Medical Things (IoMT).” Journal of Pharmaceutical Research , 
vol. 16, no. 4, Dec. 2017, p. 290.  
 
Dag, Ali, et al. “Predicting Heart Transplantation Outcomes through Data Analytics.” Decision 
Support Systems , vol. 94, Nov. 2016, pp. 42 –52.",Explain the significance of the Area under the ROC Curve in evaluating machine learning algorithms based on the information provided.
45,"785 –94. arXiv.org , doi: 10.1145/2939672.2939785 . 
 
Collier, Matt, et al. Artificial Intelligence: Healthcare’s New Nervous System . Industry Outlook 
Report, Accenture, pp. 1 –8. 
 
D, Shashank, et al. “The Internet of Medical Things (IoMT).” Journal of Pharmaceutical Research , 
vol. 16, no. 4, Dec. 2017, p. 290.  
 
Dag, Ali, et al. “Predicting Heart Transplantation Outcomes through Data Analytics.” Decision 
Support Systems , vol. 94, Nov. 2016, pp. 42 –52. DOI.org (Crossref) , 
doi:10.1016/j.dss.2016.10.005 . 
 
Davenport, Thomas, and Ravi Kalakota. “The Potential for Artificial Intelligence in Healthcare.” 
Future Healthcare Journal , vol. 6, 2019, pp. 94 –98. 
 
Fan, Jerome, et al. “Understanding Receiver Opera ting Characteristic (ROC) Curves.” CJEM , vol. 8, 
no. 01, Jan. 2006, pp. 19 –20. DOI.org (Crossref) , doi: 10.1017/S1481803500013336 .","Explain the concept of the Internet of Medical Things (IoMT) and its significance in the healthcare industry based on the article by D, Shashank, et al."
46,"Merekar 38  
Gedela, Maheedhar, et al. A Brief Review of Left Ventricular Assist Devices and Their Management . 
pp. 19 -26. 
 
He, Jianxing, et al. “The Practical Implementation of Artificial Intelligence Technologies in 
Medicine.” Nature Medicine , vol. 25, no. 1, Jan. 2019, pp. 30 –36. DOI.org (Crossref) , 
doi:10.1038/s41591 -018-0307 -0. 
 
Heart Failure | National Heart, Lung, and Blood Institute (NHLBI) . 
https ://www.nhlbi.nih.gov/health -topics/heart -failure. Accessed 1 Mar. 2020 . 
 
Heart Transplant | National Heart, Lung, and Blood Institute (NHLBI) . 
https://www.nhlbi.nih.gov/health -topics/ heart -transplant . Accessed 1 Mar. 2020.  
 
Hong, Kimberly N., et al. “Who Is the High -Risk Recipient? Predicting Mortality After Heart 
Transplant Using Pretransplant Donor and Recipient Risk Factors.” The Annals of Thoracic 
Surgery , vol. 92, no. 2, Aug. 2011 , pp. 520 –27. DOI.org (Crossref) , 
doi:10.1016/j.athoracsur.2011.02.086 . 
 
Iyer, Arjun, et al. “Primary Graft Failure after Heart Transplantation.” Journal of Transplantation , 
vol. 2011, 2011, pp. 1 –9. DOI.org (Crossref) , doi: 10.1155/2011/175768 . 
 
James, Gareth, et al. An Introduction to Statistical Learning . Springer New York, 2013. DOI.org 
(Crossref) , doi: 10.1007/978 -1-4614 -7138 -7. 
 
Jiang, Fei, et al. “Artificial Intelligence in Healthcare: Past, Present and Future.” Stroke and Vascular 
Neurology , vol. 2, no. 4, Dec.","Explain the practical implementation of artificial intelligence technologies in medicine as discussed in the article by He, Jianxing, et al."
47,"DOI.org (Crossref) , 
doi:10.1016/j.athoracsur.2011.02.086 . 
 
Iyer, Arjun, et al. “Primary Graft Failure after Heart Transplantation.” Journal of Transplantation , 
vol. 2011, 2011, pp. 1 –9. DOI.org (Crossref) , doi: 10.1155/2011/175768 . 
 
James, Gareth, et al. An Introduction to Statistical Learning . Springer New York, 2013. DOI.org 
(Crossref) , doi: 10.1007/978 -1-4614 -7138 -7. 
 
Jiang, Fei, et al. “Artificial Intelligence in Healthcare: Past, Present and Future.” Stroke and Vascular 
Neurology , vol. 2, no. 4, Dec. 2017, pp. 230 –43. DOI.org (Crossref) , doi: 10.1136/svn -2017 -
000101 . 
 
Johnston, Stephen S., et al. “Using Machine Learning Applied to Real -World Healthcare Data for 
Predictive Analytics: An Applied Example in Bariatric Surgery.” Value in Health , vol. 22, no. 5, 
May 2019, pp. 580 –86. DOI.org (Crossref) , doi: 10.1016/j.jval.2019.01.011 . 
 
Kilic, Ahmet, et al. “Donor Selection in Heart Transplantation.” Journal of Thoracic Disease , vol. 6, 
no. 8, 2014, pp. 1097 –104. 
 
Medved, Dennis, et al. “Improving Prediction of Heart Transplantation Outcome Using Deep 
Learning Techniques.” Scientific Reports , vol. 8, no. 1, Feb. 2018, pp. 1 –9. DOI.org (Crossref) , 
doi:10.1038/s41598 -018-21417 -7. 
 
Miller, P. Elliott, et al.",Explain the concept of primary graft failure after heart transplantation as discussed in the article by Iyer et al. What are some factors that may contribute to this phenomenon?
48,"22, no. 5, 
May 2019, pp. 580 –86. DOI.org (Crossref) , doi: 10.1016/j.jval.2019.01.011 . 
 
Kilic, Ahmet, et al. “Donor Selection in Heart Transplantation.” Journal of Thoracic Disease , vol. 6, 
no. 8, 2014, pp. 1097 –104. 
 
Medved, Dennis, et al. “Improving Prediction of Heart Transplantation Outcome Using Deep 
Learning Techniques.” Scientific Reports , vol. 8, no. 1, Feb. 2018, pp. 1 –9. DOI.org (Crossref) , 
doi:10.1038/s41598 -018-21417 -7. 
 
Miller, P. Elliott, et al. “Predictive Abilities of Machine Learning Techniques May Be Limited by 
Dataset Characteristics: Insights From the UNOS Database.” Journal of Cardiac Failure , vol. 
25, no. 6, June 2019 , pp. 479 –83. DOI.org (Crossref) , doi: 10.1016/j.cardfail.2019.01.018 . 
 
Mitchell, Rory, and Eibe Frank. “Accelerating the XGBoost  Algorithm Using GPU Computing.” 
PeerJ Computer Science , vol. 3, July 2017, pp. 1 –28. DOI.org (Crossref) , doi: 10.7717/peerj -
cs.127 .",What is the main focus of the study 'Improving Prediction of Heart Transplantation Outcome Using Deep Learning Techniques' by Medved et al.?
49,"Merekar 39  
NEJM Catalyst. “What Is Value -Based Healthcare?” Catalyst Carryover , vol. 3, no. 1, Massachusetts 
Medical Society, Jan. 2017. catalyst.nejm.org (Atypon) , doi: 10.1056/CAT.17.0558 . 
 
Parreco, Joshua, and Matthew Chatoor. Comparing Machine Learning Algorithms for Predicting 
Acute Kidney Injury . no. 7, July 2019, pp. 725 –29. 
 
Rodriguez, J. D., et al. “Sensitivity Analysis of K -Fold Cross Validation in Prediction Error 
Estimation.” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 32, no. 3, 
Mar. 2010, pp. 569 –75. DOI.org (Crossref) , doi: 10.1109/TPAMI.2009.187 . 
 
Roski, Joachim, et al. “Creating Value In Health Care Through Big Data: Opportunities And Policy 
Implications.” Health Affairs , vol. 33, no. 7, July 2 014, pp. 1115 –22. DOI.org (Crossref) , 
doi:10.1377/hlthaff.2014.0147 . 
 
Sakkis, Georgios, et al. Stacking Classifiers for Anti -Spam Filtering of e -Mail . 2001, pp. 1 –7. 
 
Sarle, Warren S. Neural Network s and Statistical Models . Apr. 1994, pp. 1 –12. 
 
Sherman, Rick. Business Intelligence Guidebook: From Data Integration to Analytics . Elsevier, 
Morgan Kaufmann, 2015.  
 
Smith, Grace L., et al. “Worsening Renal Function: What Is a Clinically Meaningful Change in 
Creatinine during Hospitalization with Heart Failure?” Journal of Cardiac Failure , vol. 9, no. 1, 
Feb. 2003, pp. 13 –25. DOI.org (Crossref) , doi: 10.1054/jcaf.2003.3 .",Explain the concept of value-based healthcare as discussed in the NEJM Catalyst article. How does it relate to improving healthcare outcomes?
50,"Sakkis, Georgios, et al. Stacking Classifiers for Anti -Spam Filtering of e -Mail . 2001, pp. 1 –7. 
 
Sarle, Warren S. Neural Network s and Statistical Models . Apr. 1994, pp. 1 –12. 
 
Sherman, Rick. Business Intelligence Guidebook: From Data Integration to Analytics . Elsevier, 
Morgan Kaufmann, 2015.  
 
Smith, Grace L., et al. “Worsening Renal Function: What Is a Clinically Meaningful Change in 
Creatinine during Hospitalization with Heart Failure?” Journal of Cardiac Failure , vol. 9, no. 1, 
Feb. 2003, pp. 13 –25. DOI.org (Crossref) , doi: 10.1054/jcaf.2003.3 . 
 
Srivastava, Siddharth, et al. “Deep Learning for Health Informatics: Recent Trends and Future 
Directions.” 2017 International Conference on Advances in Computing, Communications and 
Informatics (ICACCI ), IEEE, 2017, pp. 1665 –70. DOI.org (Crossref) , 
doi:10.1109/ICACCI.2017.8126082 . 
 
United Network for Organ Sharing | UNOS | US Organ Transplantation . https://unos .org/. Accessed 
1 Mar. 2020.  
 
van Deursen, V. M., et al. “Abnormal Liver Function in Relation to Hemodynamic Profile in Heart 
Failure Patients.” Journal of Cardiac Failure , vol. 16, no. 1, Jan. 2010, pp. 84 –90. DOI.org 
(Crossref) , doi: 10.1016/j.cardfail.2009.08.002 . 
 
Wirth, Rüdiger, and Jochen Hipp. CRISP -DM: Towards a Standard Process Model for Data Mining . 
pp. 1 –10.","Based on the study by Smith et al., what is considered a clinically meaningful change in creatinine levels during hospitalization with heart failure?"
51,"City Univ ersity of New Y ork (CUN Y) City Univ ersity of New Y ork (CUN Y) 
CUN Y Academic W orks CUN Y Academic W orks 
Student Theses Baruch College 
Spring 5-18-2020 
Emer ging T echnologies in Healthcar e: Analysis of UNOS Data Emer ging T echnologies in Healthcar e: Analysis of UNOS Data 
Through Machine Learning Through Machine Learning 
Reyhan Mer ekar 
CUN Y Bernar d M Baruch College 
How does access t o this work benefit y ou? Let us know! 
More information about this work at: https:/ /academicworks.cuny .edu/bb_etds/100 
Disco ver additional works at: https:/ /academicworks.cuny .edu 
This work is made publicly a vailable b y the City Univ ersity of New Y ork (CUN Y). 
Contact: AcademicW orks@cuny .edu",How can individuals benefit from accessing the work mentioned in the document?
52,"Emerging Technologies in Healthcare : Analysis of UNOS Data Through Machine Learning  
 
 
by 
 
 
Reyhan Merekar  
 
 
Submitted to the Committee on Undergraduate Honors at Baruch College of the City University 
of New York in partial fulfillment of the requirements for the degree of Bachelor of Business 
Administration in Computer Information Systems with Honors  
 
 
April 27 th, 2020",What degree is Reyhan Merekar pursuing at Baruch College of the City University of New York?
53,"Table of Contents  
 
Acknowledgements  ................................ ................................ ................................ ........................  i 
Abstract  ................................ ................................ ................................ ................................ ..........  ii 
Chapter 1: Introduction  ................................ ................................ ................................ ...............  1 
Chapter 2: Background & Additional Context  ................................ ................................ ..........  3 
Moving to Performance Based Medicine  ................................ ................................ ..............................  3 
Virtual Visits and Wearables  ................................ ................................ ................................ .................  4 
Leveraging Artificial Intelligence  ................................ ................................ ................................ ..........  4 
Emergence of Big Data in Healthcare  ................................ ................................ ................................ ... 5 
Chapter 3: Related Work  ................................ ................................ ................................ .............  7 
Chapter 4: Data & Methodology  ................................ ................................ ...............................  10 
Busi ness Understanding  ................................ ................................ ................................ ........................  11 
Data Understanding  ................................ ................................ ................................ ..............................  11 
Data Preparation  ................................ ................................ ................................ ................................ ... 11 
Model Building  ................................ ................................ ................................ ................................ ...... 12 
Discussion of Rele vant Algorithms  ................................ ................................ ................................ ....................  13 
Resampling Methods  ................................ ................................ ................................ ................................ ..........  17 
Model Evaluation  ................................ ................................ ................................ ................................ .. 18 
Key Metric: Area  Under the Receiver Operating Characteristic Curve (AUC)  ................................ .................  20 
Variable Analysis  ................................ ................................ ................................ ................................ ...............  20 
Chapter 5: Results ................................ ................................ ................................ .......................  21 
Clinically Validated Variables  ................................ ................................ ................................ .............  23 
Chapter 6: Discussion & Insights  ................................ ................................ ..............................  25 
Chapter 7: Conclusion  ................................ ................................ ................................ ................  28 
Chapter 8: Implications  ................................ ................................ ................................ ..............  29 
Data Priva cy ................................ ................................ ................................ ................................ ...........  29 
Data Standardization  ................................ ................................ ................................ ............................  29 
Existing Workforce  ................................ ................................ ................................ ...............................  30 
Chapter 9: Lim itations & Future Directions ................................ ................................ ............  31 
Can mortality be predicted independent of time period?  ................................ ................................ .. 31 
How robust will these models be? How will they scale?  ................................ ................................ .... 31 
Appendix  ................................ ................................ ................................ ................................ ...... 32 
References  ................................ ................................ ................................ ................................ .... 37",Discuss the resampling methods mentioned in the document and how they are used in model evaluation.
54,"Merekar i  
 
Acknowledgements  
 First and foremost, I would like to thank my Faculty Advisor, Professor Arturo 
Castellanos, for providing feedback and guidance throughout this project and most of my 
undergraduate career. We have worked on a few projects together, and it has been a privil ege to 
learn from him throughout these four years. Additionally, I would like to thank my two Faculty 
Readers, Professors Kevin Craig and Zeda Li, for taking time to read my work and provide 
important feedback.  
 Special thanks to Claudio A . Bravo, Miguel A lvarez, and Mahek Shah for devoting time 
out of their busy schedules to provide valuable insight and context throughout this project.  
 Of course, I would like to thank my family for continuously supporting me through my 
undergraduate studies. All of this w ould not be possible without their belief in me.",Which two Faculty Readers are acknowledged in the document and what contribution did they make to the project?
55,"Merekar ii  
Abstract  
 The healthcare industry is primed for a massive transformation in the coming decades 
due to emerging technologies such as Artificial Intelligence (AI) and Machine Learning. Wi th a 
practical application to the UNOS (United Network of Organ Sharing) database, this Thesis 
seeks to investigate how Machine Learning and analytic methods may be used to predict one-
year heart transplantation outcomes. This study also sought to improve on predictive 
performances from prior studies by analyzing  both Donor and Recipient data. Models built wi th 
algorithms such as Stacking and Tree Boosting gave the highest performance, with AUC’s of 
0.681 0 and 0.68 04, respectively. In this work, a roadmap w as created that justifies the need for 
these technologies in healthcare. In application, the data was prepared, models were built using 
advanced algorithms, and important variables were selected. These steps were continuously done 
with validation from expe rienced clinicians. To yield greater insights in this study, the dataset 
was split row -wise by factors such as LVAD Support, Donor/Recipient Gender Combinations, 
and Time Period; this rendered 8 new datasets for analysis. This work explores the trade -off 
between interpretability and performance in applying analytic methods in a real -world problem 
in this domain. Finally, forward looking industry implications are discussed.","What were the highest performing algorithms used in building models for predicting heart transplantation outcomes, and what were their corresponding AUC values?"
56,"Merekar 1 
 
Chapter 1: Introduction  
Heart transplantation has been carried out since the 1970s, but still remains one of the 
riskiest procedures today. Formally, a heart transplant is defined as the surgical replacement of 
the heart of a diseased individual with that of a healthy donor  (National Heart, Lung, and Blood 
Institute , “Heart Transplant” ). Typically, p atients who have end -stage heart failure, where the 
heart is severely damaged or weakened, undergo this procedure. Heart failure is caused by 
conditions such as coronary heart disease, hereditary conditions, and/or viral infections (National 
Heart, Lung, and Blood Institute , “Heart Transplant” ). A patient in need of a heart transplant can 
locate donor organs through the United Network for Organ Sharing. This private, non -profit 
organization manages the United States’  organ transplant system and provides a computerized 
national waiting list which assures equal access and fair distribution of organs as they become 
available  (United Network for Organ Sharing, “About UNOS”) . 
Several risk factors are associated with heart transplantation . The first is primary graft 
dysfunction (PGF), which occurs when the donor heart fails and is unable to function (National 
Heart, Lung, and Blood Institute , “Risk Factors” ). This is an immediate issue and usually leads 
to a quick time of de ath for the patient. It is also a major contributor to mortality and 
additionally, may lead other complications (Iyer et al. 1 -2). The patient’s immune system may 
also reject the newly transplanted heart within the first six months of transplantation. To c ombat 
this, the patient must take additional medicine to suppress the immune system. Long term side 
effects associated with this medicine include diabetes, osteoporosis, and kidney damage 
(National Heart, Lung, and Blood Institute , “Risk Factors”).",What are some risk factors associated with heart transplantation mentioned in the context?
57,"Merekar 2 
 
The p resence of technology in healthcare, particularly data science, has begun to emerge 
within  the past few decades. Professionals are beginning to explore the implications of using data 
to provide reliable solutions. H eart transplantation  procedures  are prime d to increase in the 
coming years , which will be driven by the aging population in the United States. According to 
Primers in Medicine, the number of people older than 65 will “double  by 2060 ” (Gedela et al. 
19). Another key trend is heavy investment in AI  and Machine Learning. As reported by 
Accenture, AI investment by healthcare firms will increase to $6B by 2021  (Collier et al. 2).  As 
more people are at risk for end -stage heart failur e and other diseases, AI  and Machine Learning  
can be leveraged to incr ease predictive power  for heart transplantation outcomes and disease 
detection.  
This project aims to  use Machine Learning 1 techniques to predict one -year heart 
transplantation outcomes using queried data from the UNOS registry from 1990 -2016. It also 
strives to build on prior studies in this domain. Predictive models are constructed to help 
clinicians better understand underl ying patterns in Donor and Recipient data. Since the 
underlying models will be able to predict the likelihood of a patient’s survival after one year, 
they will allow the clinician (s) to take the appropriate course of action for treatment .
 
1. In this work, the terms Machine Learning and Data Mining are used interchangeably.","What key trends are emerging in healthcare, according to the information provided? How are AI and Machine Learning being leveraged in this context?"
58,"Merekar 3 
 
Chapter 2: Backgr ound & Additional Context  
 The imminence of technology within healthcare has grown rapidly and has been a driver 
of major industry shifts throughout the past decade. One example of such advancements is the 
emergence of AI, specifically, Machine Learning. F ormally defined,  it is the practice of using 
algorithms to build models that learn from any n-number of observations and try to emulate the 
underlying pattern of the phenomena (Beam and Kohane 1317). This concept allows the 
computer to autonomously make de cisions without instructions from the researcher and is better 
suited for higher dimensional datasets where relationships may not necessarily be linear (Beam 
and Kohane 1317). The following trends emphasize why AI will be prevalent and contribute to 
the ev olving landscape of healthcare/medicine.  
Moving to Performance Based Medicine  
The healthcare system in the United States is now moving toward a financial model based 
on value rather than volume. The onus is now on delivering excellent population health thr ough 
treating patients like members. Rather than accounting for revenue due to patient volume, this 
value -based model shows each visit as an expense rather than a source of revenue (Burrill, 
“Health Care Outlook for 2019: Five Trends That Could Impact Heal th Plans, Hospitals, and 
Patients ”). 
This shift will take time, as the current transition has not been entirely smooth. In the 
short term, healthcare firms may see financial hits before longer -term costs decrease. Despite 
this, the value -based model has been embraced as the best method in lowering healthcare costs 
while increasing the quality of care. Since patients are seeking the best care possible, Machine 
Learning can be a catalyst in  providing that, helping people live healthier lives (NEJM Catalyst, 
“What is Value Based Healthcare?”) .",Discuss the shift towards a value-based model in the healthcare system of the United States and its implications for healthcare firms and patients.
59,"Merekar 4 
 
Virtual Visits and Wearables  
 The general population dreads visiting the doctor, and often, waits until a later date when 
the condition has become more  severe to visit one. This mentality drives up costs for the patient. 
Virtual visits and telehealth serve as a basis to interact with a caregiver without attending the 
office. According to Steve Burrill of Deloitte, this technology helps them “see more pat ients, 
deal with rising clinical complexity, and support patients as they take a greater role in their own 
care.” There is much room to leverage this practice, as currently, only 14% of caregivers are 
utilizing this (Burrill, “ Health Care Outlook for 2019:  Five Trends That Could Impact Health 
Plans, Hospitals,  and Patients”).  
 As the popularity of wearables (e.g., Apple Watch, FitBit) grow, so does the data they 
transmit. The Internet of Medical Things (IoMT) is the health spin -off of the Internet of Thing s. 
This phenomenon can be explained as the “collection of medical, drug delivery devices and 
applications that connect to healthcare IT systems through online computer networks” (D et al. 
290). Medical devices equipped with Wi -Fi allow for machine -to-machi ne interactions. Such a 
phenomenon can help clinicians/healthcare professionals collect data points that may be used for 
disease prediction, patient status checks, and drug developments.  
Leveraging Artificial Intelligence  
 Artificial Intelligence lies at the center of all of these trends. For example, in a virtual 
visit, software can be used to track a person’s mood. Rather than meeting with someone, data 
from a patient’s Electronic Health Record (EHR) can help manage illnesses. Data from 
wearables  and tracking will be used to predict what a diagnosis may be, what drugs can be 
developed to help that will help the patient, and realistic timelines of treatment.","Explain the concept of the Internet of Medical Things (IoMT) and how it can benefit healthcare professionals in collecting data for disease prediction, patient status checks, and drug developments."
60,"Merekar 5 
 
 AI aims to mimic human cognition. Rather than fully replace doctors, it must be used in a 
way where professionals are working with AI to enhance clinical decisions. Now, with various 
analytic methods and collections of EHR’s, this is becoming a reality. For example, it is already 
making waves in radiology, oncology, disease prediction/preventio n, and outcome prediction 
with AI system IBM Watson. The system includes underlying Machine Learning models but is 
also a pioneer in the field. According to Jiang et al., “99% of the treatment recommendations 
from Watson are coherent with the physician dec isions” (241). Several reviews have appeared in 
literature referencing similar analytic methods in healthcare; these have covered techniques, 
algorithms, and dataset evaluations. Additionally, research in this space is growing, as the 
number of published p apers has increased by nearly 300% from 2008 to 2015 (Srivastava et al. 
1665). This trend also contributes to the motivation for this study.  
Emergence of Big Data in Healthcare  
 Big data has an incredible potential to yield significant value in healthcar e. This will be 
driven by decreasing costs of data storage, access to powerful but remote cloud computing, 
proliferation of “smart” devices, and the increase in electronic communication. Take for example 
healthcare titan Kaiser Permanente, which consists o f approximately nine million members. The 
firm has the capability to manage up to 44 petabytes of data through its EHR. This is “4,400 
times the equivalent of the data stored in the Library of Congress” (Roski et al. 1115). This 
implies that there is a vas t amount of data readily available for analysis in healthcare.  
 Big data is typically understood as a combination of three concepts – volume, velocity, 
and variety. Volume refers to the amount of data currently present in an enterprise; many expert s 
assert that “90% of the data (stored)” has been created only over the last eight years (Sherman 4). 
Velocity measures the time sensitivity of data; reporting and analysis need to be immediate and","What are the three concepts that make up big data in healthcare, as mentioned in the document?"
61,"Merekar 6 
 
there is greater pressure to bridge the gap betwee n when the data is acquired and when it is 
analyzed. Finally, variety is the idea that data is now collected from many different sources. It 
can be structured (e.g., tabular) or unstructured (e.g., emails, documents, PowerPoints). In this 
work, these oppor tunities are taken advantage of, and findings from prior studies are considered .","Explain the significance of collecting data from various sources, including structured and unstructured data, in the context of data analysis."
62,"Merekar 7 
 
Chapter 3: Related Work  
 As mentioned previously, this project builds  on prior studies in the field. Machine 
Learning and analytic methods have been explored not only for hear t transplantation, but in other 
realms of clinical decision making as well. For example, a study was conducted that analyzed the 
risk of acute kidney injury (AKI), which is associated with chronic kidney disease and poses a 
high risk of mortality (Parreco and Chatoor 725). Another study used machine learning 
techniques to predict remission outcomes of Type 2 Diabetes following bariatric surgery 
(Johnston et al. 580).  
 Although these studies are not directly related to heart transplantation, they do have 
aspects in common. Those studies, along with this one, are Classification problems where the 
response variable is binary, the methodology is relatively similar, and the same evaluation 
method of Area Under the Curve (AUC) is utilized.  Additionally, a common theme in Machine 
Learning studies in healthcare is the Logistic Regression model serves as a baseline for model 
comparison.  
 Moreover, directly related studies have been conducted with  the same  data in which 
Machine Learning was use d to predict one-year mortality. One example of a similar study 
include d a report from the Journal of Cardiac Failure, which compared results from traditional 
statistical techniques with more advanced techniques . This study employed six  variables: age of 
recipient, creatinine, body mass index, liver function tests, aspartate transaminase, and 
hemodynamics. With the given variables, models were created with traditional statistical 
techniques and machine learning algorithms; these were all evaluated  by the me tric AUC. The 
implementation of Deep Learning  models yielded the best AUC, which was roughly 0.66.  At the 
end, it was deemed that the implementation of more advanced techniques failed to yield a n",What common aspects do the studies mentioned in the related work section and the current project share?
63,"Merekar 8 
 
improved result when compared to traditional approaches, as there was a modest discrepancy 
between the two (Miller et al. 3 -4). One possible drawback to this study was that the researchers 
only used the recipient data and did not emphasize the donor’s data. The study was also only 
limited to univaria te variables.  
In another  similar study , clinicians and data scientists utilized existing models to predict 
one-year mortality outcomes . The two models were IMPACT (Index for Mortality Prediction 
After Cardiac Transplantation) and IH TSA (International Heart  Transplantation Survival 
Algorithm) which both implemented Machine Learning and Deep Learning techniques. The 
IMPACT model yielded an AUC of 0.608 with 18 variables being used, while the IH TSA model 
yielded an AUC of 0.64 with 32 variables  (Medved et al.  3-6). One drawback to this study is that 
it was ambiguous as to which variables were used when each model was initially created .  
Another similar study was carried out where analytic methods were leveraged  to predict 
mortality outcomes, however, this study addressed a slightly different problem and diff ered in 
overall methodology. Various analytic techniques were still carried out, but this paper considered 
one, five, and nine -year mortality instead. To impute missing values for bias removal, the 
researchers used Synthetic Minority Oversampling TEchnique  (SMOTE) which is used to 
“improve random oversampling” (Blagus and Lusa 2). This technique is well suited for lower 
dimensional data but is not ideal in higher dimensional settings. The best performance this study 
was able to uncover were AUC’s of 0.624, 0.676, and 0.838 for one, five, and nine -year 
mortality, respectively (Dag et al. 47 -49).  
 In this study however, Donor and Recipient data will be analyzed through advanced 
algorithms to improve  overall predictive power in heart transplantation.  Additiona lly, the data 
will be split in various ways to potentially yield further insights and variables are to be","How did researchers address the issue of missing values for bias removal in a study predicting mortality outcomes, and what technique did they use?"
64,"Merekar 9 
 
eliminated from the full dataset for interpretability. This is further explained in following 
chapters.",How is the elimination of Merekar 9 further explained in the following chapters?
65,"Merekar 10 
 
Chapter 4: Data  & Methodology  
 Data mi ning projects such as these have flexibility to be creative and dynamic. There is, 
however, an accepted framework known as CRISP -DM ( CRoss Industry Standard Process for 
Data Mining) which is independent in industry and technology used. This framework aims to 
organize the steps in a data mining project. CRISP -DM has been seen as beneficial for 
organizations conducting large data mining projects in terms of cost, reliability, and 
manageability. This is the overarching methodology used for this project to keep  a rigid structure 
and reduce errors. The relevant five elements of this framework are Business Understanding, 
Data Understanding, Data Preparation, Model Building, and Evaluation.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 1. Phases of the CRISP -DM Model for Data Mining/Machine Learning. CRISP -DM – a 
Standard Methodology to Ensure a Goo d Outcome - Data Science Central . 
https://www.datasciencecentral.com/profiles/blogs/crisp -dm-a-standard -methodology -to-ensure -a-
good -outcome .",Discuss the five elements of the CRISP-DM framework and their importance in the data mining process.
66,"Merekar 11 
 
Business Understanding  
 The initial phase of the framework involves defining the problem at hand solely from the 
perspective of the business. Following this, the problem is converted into one that can be solved 
by data mining (Wirth and Hipp 5). In this case, the project lies in the domain of 
healthcare/medicine, so that was kept in mind throughout the process. The clearly defined 
problem is to understand and predict one -year mortality after a heart transplant.  
Data Understanding  
 The next phase of the framework is understanding t he data. This begins with obtaining 
data and conducting elementary observations to pick up on any immediate trends. Here, data 
quality issues are also identified (Wirth and Hipp 5). The corresponding query from the UNOS 
database yields a 32018 by 558 data frame. Each row represents an individual patient, and each 
patient has descriptive variables. It was apparent that the quality of the data was not quite up to 
par; missing values were present which made conducting initial analyses difficult . The data types  
of some variables also were not correct.  
Data Preparation  
 The data preparation phase spans all activities used to create the final data set; the one(s) 
that will be used in the model building process. This phase may be repeated many times in a 
given project. Some tasks include feature selection, data cleaning, and additional data 
transformation. In this project, the data had to be “cleaned” (e .g., adjusting data types, removing 
features, dealing with missing values) (Wirth and Hipp 5). One example of an adjusted data type 
was the response  variable, “One_year_mortality_retransplant”. This had to be changed to a 
“factor” data type. According to Python’s pandas documentation, these data types take on a 
“limited, and usually fixed, number of possible value s.” Missing values were dealt with",What were some data quality issues identified during the data understanding phase in the project? How did these issues impact the initial analyses?
67,"Merekar 12 
 
differently depending  on the algorithm used to build each model. Cleaning was validated with 
clinicians, as they provided more insight for understanding the variables. In this project, the 
target variable had categories “ Died” or “Has not died” signified by the binary 1 and 0, 
respectively. In terms of feature selection, initially, many variables were omitted due to lack of 
variability cutting down the count to 420. Feature Selection was used again to reduce the number 
of variables to improve interpretability. This is further discussed at the end of the chapter.  
 Additionally, the data was iteratively prepared by splitting on factors such as time period 
of transplant, LVAD support, and Donor/Recipient gender combinations. W ith respect to time 
period, two new datasets were derived based on transplantation dates before and after 2006. For 
gender, each donor/recipient combination was analyzed, yielding four new datasets. The four 
combinations were male donor/male recipient, mal e donor/female recipient, female donor/male 
recipient, and female donor/female recipient. A Left Ventricular Assist Device (LVAD) is used 
when a patient is near heart failure but cannot acquire a transplant immediately. This device is 
installed to assume t he role of providing blood to vital organs (Gedela et al. 19). Two datasets 
were created here to understand how the analyses would change depending on whether the 
recipient had an LVAD or not.  
Model Building  
 In this phase, models are built and tested. Usu ally, there are several techniques that can 
be used to create models (Wirth and Hipp 6). The specific algorithms used for this study were 
Logistic Regression, Decision Trees/Random Forest, Tree Boosting, Neural Networks (Deep 
Learning), and Stacking. Logis tic Regression is the most traditional algorithm used in medicine, 
so for the purposes of this project, it serve d as the baseline for comparison. All models were built 
using the H2O module in the Python programming language powered by Amazon Web Services.",How was feature selection used in this project and why was it important?
68,"Merekar 13 
 
Discussion of Relevant Algorithms  
1. Logistic Regression  
 Logistic Regression is a widely used algorithm when the desired outcome is a question of 
classification. Rather than modeling a specific response directly, Logistic Regression model s the 
probability that the desired outcome belongs in a particular category. As the algorithm deals with 
probability, the related  sigmoidal  function must only produce outputs between 0 and 1. Below is 
the general logistic function for a multivariate Logist ic Regression (see Fig . B in Appendix for 
visual representation):  
𝑝(𝑋)= 𝑒𝛽0+𝛽1𝑋1+⋯+𝛽𝑝𝑋𝑝
1+𝑒𝛽0+𝛽1𝑋1+⋯+𝛽𝑝𝑋𝑝, 
where  𝑋=(𝑋1,𝑋2,…,𝑋𝑝) are p predictors. In this work, th e model derived from this algorithm 
served as a baseline for the performance of other  “advanced”  models  (James et. al 130 -133). 
With respect to models built with Logistic Regression, missing values were filled by mean 
imputation.  
2. Decision Trees/Random Fores t 
 Decision Tree is a hierarchical structure composed of branches and nodes . These are 
essentially a collection of ‘if/else’ statements that split decisions into binary classifications (in 
this case, “Died”  or “Has not died” ). Formally, this algorithm involves stratifying or segmenting 
the data set into a number of simple regions a nd making predictions (James et. al 306 -312):",Describe the structure of a Decision Tree and how it is used for making predictions in binary classifications. How does it differ from Logistic Regression in terms of modeling?
69,"Merekar 14 
 
 
 
 
 
 
 
 
Although decision trees are relatively simple to interpret, their downfall lies in the fact 
that they are not robust. A small change in the data can yield quite a large change in the final 
estimated tree. To correct this problem, the Random Forest algorithm may be implemented, 
which is essentially a larg e collection of decision trees ( Amornsamankul et al.  3). This algorithm 
corrects the instability of Decision Trees by bootstrap aggregating and decorrelating trees. This 
leads to stronger predictive power and lessens the risk of overfitting, which is touch ed on later in 
this chapter. With these models, missing values were filled by using the mode of respective 
variables.a. Divide the predictor space (the set of possible values for X1, X2, …, Xp) into J distinct and 
non-overlapping regions, R1, R2,…, R J. 
b. Create each binary partition based on node purity . This can be quantified by the Gini 
Index, where a pure node would produce a value closer to 0 ( 𝑝̂𝑚𝑘 represents the 
proportion of the kth classification in the mth region) : 
𝐺= ∑𝑝̂𝑚𝑘(1−𝑝̂𝑚𝑘) 𝐾
𝑘=1 
c. For every observation that falls into the region Rj, the same prediction is made , which is 
simply the mode  of the response values for the training observations in Rj.  
 
 Figure 2. Decision Tree Algorithm . James, Gareth, et al. An Introduction to Statistical Learning . 
Springer New York, 2013. DOI.org (Crossref) , pp. 306 -312, doi:10.1007/978 -1-4614 -7138 -7.",What is the main drawback of decision trees in terms of robustness? How does the Random Forest algorithm address this issue?
70,"Merekar 15 
 
3. Tree Boosting  
 Another way to combat Decision Tree instability is by using Boosting. Tree Boosting 
(e.g., Gradient Boosting Machine, Extreme Gradient Boosting) is widely used in machine 
learning  to achieve optimal performance. It is an ensemble method that sequentially creates new 
members; the newest member is created to account for incorrectly labeled inst ances from 
previous learners to minimize the loss function (direct relationship to error). The results of new 
trees are then applied partially to the entire solution. The algorithm executes M boosting 
iterations to learn a function F(x) that outputs predic tions ŷ = F(x)  while simultaneously 
minimizing a loss function L (y, ŷ). At each iteration, a new estimator f(x) is added to correct the 
prediction of y for each instance in training. This is shown formally below:  
 
 
 
 
 
 
 
 
 
 
 
 
 a. Start with a function which approximate s the true relationship of x and y: 
𝐹𝑚+1(𝑥)= 𝐹𝑚(𝑥)+𝑓(𝑥)=𝑦 
𝑓(𝑥)=𝑦−𝐹𝑚(𝑥) 
b. This fits the model f(x) for the current boosting iteration to the errors above 
(difference of actual and predicted). This can be shown as a gradient descent 
algorithm when the los s function is the squared error:  
𝐿(𝑦,𝐹(𝑥))= 1
2(𝑦−𝐹(𝑥))2 
c. Let the summation of this loss function be denoted as J. The goal is to minimize J by 
adjusting F(xi), the function of a particular instance.",Describe the process of Gradient Boosting Machine in the context of creating new members to minimize the loss function. How does it differ from traditional Decision Trees?
71,"This can be shown as a gradient descent 
algorithm when the los s function is the squared error:  
𝐿(𝑦,𝐹(𝑥))= 1
2(𝑦−𝐹(𝑥))2 
c. Let the summation of this loss function be denoted as J. The goal is to minimize J by 
adjusting F(xi), the function of a particular instance.  
𝐽= ∑𝐿(𝑦𝑖,𝐹(𝑥𝑖))
𝑖 
𝑑𝐽
𝑑𝐹(𝑥𝑖)=𝑑(∑𝐿(𝑦𝑖,𝐹(𝑥𝑖)) 𝑖 )
𝑑𝐹(𝑥𝑖)=𝐹𝑚(𝑥𝑖)− 𝑦𝑖 
d. Thus, errors are equal to the negative gradient of the squared error loss function:  
𝑓(𝑥)=𝑦−𝐹𝑚(𝑥)=−𝑑(∑𝐿(𝑦𝑖,𝐹(𝑥𝑖)) 𝑖 )
𝑑𝐹(𝑥𝑖) 
 
Figure 3 . Tree Boosting Algorithm . Mitchell, Rory, and Eibe Frank. “Accelerating the XGBoost Algorithm Using GPU 
Computing.” PeerJ Computer Science , vol. 3, July 2017, pp. 3-4. DOI.org (Crossref) , doi: 10.7717/peerj -cs.127 .",What is the goal of adjusting the function F(xi) in the context of minimizing the summation of the loss function J?
72,"Merekar 16 
 
By adding a model that approximates this, the loss function is further minimized (Mitchell and 
Frank 3 -4). 
Often in the building phase, this algorithm will produce the strongest model in all facets 
of evaluation as it is inherently robust. Some of the drawbacks to this is that Tree Boosting tends 
to overfit but can also be corrected by adjusting tree sizes by p runing (Chen and Guestrin 3 -5). 
With these models, missing values were filled by using the mode of respective variables.  
4. Artificial Neural Networks (Deep Learning)  
 Neural Networks have recently been adapted as a viable data analytic method. These 
networks  aim to emulate that of the human brain, as they contain “neurons” (linear or non -linear 
computing elements) interconnected in complex ways and organized in layers.  
 A simple perceptron 2 constructs a linear combination of the inputs called the net input. 
Thereafter, an activation function is linked to produce an output, this maps any real input to a 
bounded range. A functional link network introduces a hidden layer in the network. This uses 
nonlinear activation functions to produce a fully nonlinear model (in parameters). Th e resulting 
model is known as an MLP or multilayer perceptron. These models are flexible, general purpose, 
and non -linear and have the ability to yield multiple outputs from many inputs. Given enough 
data, this can approximate to a desir ed degree of accuracy. During building, numerical values 
were filled by using mean imputation, while categorical ones were omitted. An illustration of a 
sample neural network may be found in the Appendix  (Fig. C) (Sarle 2 -5). 
5. Stacking  
Stacking is a n approach for building up classifier ensembles; this refers to a collection of 
classifiers in which their decisions are put together to classify new instances. The algorithm
 
2. A perceptron is a single -layer Neural Network. An interconnected system of perceptrons (MLP) yields a Neural 
Network.","Describe the structure and functioning of Artificial Neural Networks, specifically focusing on perceptrons and multilayer perceptrons. How are missing numerical values handled during the building phase of neural network models?"
73,"Merekar 17 
 
combines multiple classifiers to induce a higher -level classifier with improved p redictive 
performance (Sakkis et al. 1 -2). In this study, two variants are used. “Best of Family” combines 
the best models from each algorithm, and “All Models” merges all models in a given iteration of 
building. Missing values were handled by the base alg orithms of the ensemble.  
Resampling Methods  
 Generally, in Machine Learning projects, the researcher splits a given dataset into a 
training and test set based on a chosen ratio. The training set is used to build models upon, while 
the test set acts as a pr oxy for how the model will perform on future, unseen data. This method, 
known as the Hold -Out Method, introduces additional error.  
An important concept to understand in this domain is the bias-variance tradeoff  
(sometimes also referred to as the tradeoff between data -fit and complexity). The main point of 
model building is to showcase that it can generalize to unseen data. Maybe counter -intuitively, 
predictive performance is not maximized by learning the training data as precisely as possible. 
An extremely  close fit to the training data is typically not ideal because the model will pick up 
random fluctuations in the data (i.e. noise) and miss the “broader regularities” in the dataset 
(Briscoe and Feldman 3 -4). Using just the Hold -Out method can lead to high  bias, low variance, 
or vice -versa. An illustration of the bias -variance tradeoff may be found in the Appendix (Fig. 
D). 
 Alternatively, r esampling methods can be defined as iteratively fitting models on 
randomly drawn samples of given data. One of these methods, perhaps the most popular, is k-
Fold Cross Validation. The dataset is divided into k folds (row -wise), where each k – 1 folds 
becom e training sets while the remaining fold acts as a test set. An error value is calculated for 
the “test” fold, and finally, the average of each test fold becomes the overall error value",Compare and contrast the Hold-Out Method with k-Fold Cross Validation in terms of their approach to splitting data for model training and testing.
74,"Merekar 18 
 
(Rodriguez et al. 569).  This was the resampling method of choice for t his project, with k = 5. An 
illustration may be found below:  
 
 
 
 
 
 
 
 
 
Compared to the simple Hold -Out method, k-Fold Cross Validation has advantages in 
both bias and variance. Bias is reduced in the sense that there are more observations “seen” by 
the model during training. Variance is also reduced due to the fact that there is smaller overlap 
between each training  set. Given these considerations, it is empirically proven that k = 5 or k = 
10 yield test errors that do not suffer from high bias nor very high variance (James et al. 181 -
184).  
Model Evaluation  
 At this point in the framework, multiple models have been b uilt and are ready to be 
compared. The models are constructed correctly and are assumed to be of the best quality 
possible. Before deployment of a model, it is vital to thoroughly evaluate each model, review 
steps taken to build the model, and justify it u sing the business context (Wirth and Hipp 6).  
 
Figure 4 . K-Fold Cross Validation . Ren, Qiubing, et al. “Tectonic Discrimination of Olivine in 
Basalt Using Data Mining Techniques Based on Major Elements: A Comparative Study from 
Multiple Perspectives.” Big Earth Data , vol. 3, no. 1, Jan. 2019, p. 14. DOI.org (Crossref) , 
doi:10.1080/20964471.2019.1572452 .","Why is it important to thoroughly evaluate each model before deployment, according to the framework mentioned in the context?"
75,"Merekar 19 
 
 Confusion matrices are used in evaluation of problems of binary classification. It is a 2x2 
table formed by counting the number of the four outcomes of a binary classifier. The two 
“classes” are actual and pr edicted with a positive/negative value for each. The four cells are 
filled with the number of True Positives (TP), False Positives (FP), False Negatives (FN), and 
True Negatives (TN) (Amornsamankul et al. 5).  
In this project, the outcome s of “Died” or “Has not died” are examined. True Positives in 
this case refer to the number of “Died” predicted when the actual value was also “Died.” False 
Positives are the number of “Has not died” predicted when the outcome was actually “Died.” 
False Negatives are the number of predict ed “Died” when the actual was “Has not died.” True 
Negatives are the number of predicted “Has not died” when the actual classification was “Has 
not died”. In this case, False Negatives are more costly than False Positives, so the metric of 
Sensitivity (Rec all) is also relevant in this study.","Define True Positives, False Positives, False Negatives, and True Negatives in the context of binary classification. Provide an example to illustrate each."
76,"5).  
In this project, the outcome s of “Died” or “Has not died” are examined. True Positives in 
this case refer to the number of “Died” predicted when the actual value was also “Died.” False 
Positives are the number of “Has not died” predicted when the outcome was actually “Died.” 
False Negatives are the number of predict ed “Died” when the actual was “Has not died.” True 
Negatives are the number of predicted “Has not died” when the actual classification was “Has 
not died”. In this case, False Negatives are more costly than False Positives, so the metric of 
Sensitivity (Rec all) is also relevant in this study.  
 
 
From this, prediction accuracy, sensitivity (recall), and specificity may be derived:  
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑃+𝑇𝑁
𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 
𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦  (𝑅𝑒𝑐𝑎𝑙𝑙 )= 𝑇𝑃
𝑇𝑃+𝐹𝑁 
𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 =𝑇𝑁
𝑇𝑁+𝐹𝑃   Predicted Class  
  Died  Has not died  
Actual Class  Died  TP FP 
Has not died  FN TN 
Figure 5 . One -year Mortality Confusion Matrix .",Why are False Negatives considered more costly than False Positives in this study? How does this impact the metric of Sensitivity (Recall)?
77,"Merekar 20 
 
Key Metric: Area Under the Receiver Operating Characteristic Curve (AUC)  
 The Receiver Operating Characteristic Curve (ROC Curve) has been identified as a 
viable soluti on of visualizing a classifier’s performance in order to select an optimal decision 
threshold. The term was coined during World War II when it was used in “Signal Detection 
Theory” for radars, but later made its way to diagnostic medicine. It was determine d that an 
“ideal” threshold is almost always a trade -off between sensitivity (True Positives) and specificity 
(True Negatives). Since it may be difficult for a researcher to imagine an ideal “cut -off”, this 
concept was visualized. The Y -Axis represents Sen sitivity while the X -Axis represents 
Specificity. In theory, a researcher would want to achieve both high Sensitivity and Specificity, 
but this is far from practical in application. Hence, there is a trade -off, and either one of the two 
metrics can be opti mized (Bradley 1145).  
 The area under the ROC Curve (referred to as AUC) is widely recognized as the measure 
of a diagnostic test’s discriminatory power. The value of AUC ranges from 0.0 to 1.0, where a 
value of 1.0 implies 100% sensitivity and specificity . A value of 0.5 indicates no discriminative 
value. This entails 50% sensitivity and 50% specificity  (Fan et al. 20) . In terms of this project, 
ROC curves were visualized for each model, and ranked based on AUC. Models with AUC ’s 
that performed better than  the baseline Logistic Regression were deemed the most useful . 
Variable Analysis  
 As there are greater than 400 variables in the full dataset, only significant variables were 
considered in individual variable analyses. Each model produced a list of important variables and 
these were brought to clinicians for validation. Additionally, to  further drill down the number of 
variables, the important variables  from the top model from each dataset was taken. These were 
then counted up to understand the shared important variables throughout all models .",What is the importance of the Area Under the Receiver Operating Characteristic Curve (AUC) in measuring a diagnostic test's discriminatory power? How is the AUC value interpreted in terms of sensitivity and specificity?
78,"Merekar 21 
 
Chapter 5: Results  
 After the methodology wa s carried out, initial results were obtained for each model per 
dataset. These contain the top two models from each dataset, the baseline Logistic Regression, 
algorithms, AUC measures, Sensitivity, and dataset dimensions. A detailed table depicting all of 
these may be found below.  
 
Algorithm  AUC  Sensitivity (Recall)  
Full Dataset ( 32,018  Observations , 420  Variables)  
Logistic Regression  0.63683332  0.4190646  
 Stacking, All Models  0.681168  N/A 
Extreme Gradient Boosting  0.680403  0.41853034  
 Patients with no LVAD (10,912  Observations , 420  Variables ) 
Logistic Regression  0.57672026  0.45374164  
 Stacking, All Models  0.65443  N/A 
Extreme Gradient Boosting  0.653004  0.41827255  
 Patients with an LVAD (7,700  Observations , 420  Variables ) 
Logistic Regression  0.58331451  
 0.5547305  
 Stacking, All Models  0.65692  
 N/A 
Extreme Gradient Boosting  0.654315  
 0.43852264  
 Patients Recorded Before 2006 (13,406  Observations , 420  Variables ) 
Logistic Regression  0.61300899  
 0.57854533  
 Stacking with All Models  0.674364  
 N/A 
Extreme Gradient Boosting  0.67345  
 0.41904527  
 Patients Recorded After 2006 (18,612  Observations , 420  Variables ) 
Logistic Regression  0.60626572  
 0.46481952  
 Stacking, All Models  0.668372  
 N/A 
Extreme Gradient Boosting  0.66386  
 0.4589736  
 Table  1. Model Performance by Dataset and Algorithm.","Compare and contrast the Sensitivity (Recall) values for Logistic Regression, Stacking with All Models, and Extreme Gradient Boosting in predicting patients with an LVAD. How do these values vary for patients recorded before and after 2006?"
79,"Merekar 22 
 
Female Donor Female Recipient (2,713  Observations , 420  Variables ) 
Logistic Regression  0.54148685  
 0.5008384  
 Stacking, Best of Family  0.662545  
 N/A 
Extreme Gradient Boosting  0.677088  
 0.53623605  
 Female Donor Male Recipient (2,730  Observations , 420  Variables ) 
Logistic Regression  0.570355  
 0.47577724  
 Stacking, Best of Family  0.647494  
 N/A 
Extreme Gradient Boosting  0.635834  
 
 0.42375335  
 Male Donor Female Recipient (2,713  Observations , 420  Variables ) 
Logistic Regression  0.5310544  
 0.5507362  
 Stacking, All Models  0.651886  
 N/A 
Extreme Gradient Boosting  0.664648  
 0.47671908  
 Male Donor Male Recipient (11,306  Observations , 420  Variables ) 
Logistic Regression  0.57988709  
 0.45201996  
 Stacking, All Models  0.650807  
 N/A 
Extreme Gradient Boosting  0.647327  
 0.41651163  
  
After the initial analysis, a subset of the full dataset was taken. The data was filtered on the basis 
of important variables from the Extreme Gradient Boosting Algorithm on the full dataset and 
models were ran again. The results are displayed below:  
 
Full Dataset with Significant Variables (32,018, 25)  
Algorithm  AUC  Sensitivity (Recall)  
Logistic Regression  0.6603  
 0.4304224  
 Stacking, All Models  0.671926  
 N/A 
Extreme Gradient Boosting  0.670706  
 0.4328194  
  Table  2. Model Performance: Full Dataset with Significant Variables and Algorithms.","Compare the performance of Logistic Regression, Stacking (All Models), and Extreme Gradient Boosting algorithms for Female Donor Male Recipient cases. Which algorithm performed the best and why?"
80,"Merekar 23 
 
Following that, more features were removed to make the model more interpretable. The top 16 
variables were chosen based on important variables  from the top model from each dataset. These 
were the significant variables that were most common throughout all m odels from all datasets. 
The models were run again, and the results are displayed below:  
 
Full Dataset with Clinically Significant Variables (32,018, 16) 
Algorithm  AUC  Sensitivity (Recall)  
Logistic Regression  0.6284  
 0.44540313  
 
 Stacking, Best of Family  0.6500 
 N/A 
Extreme Gradient Boosting  0.6491 
 0.46050477  
  
Clinically Significant  Variables  
 Analysis of variables included validation with clinicians. They were able to provide 
additional context as to whether or not models included variables that made sense from a 
physiological standpoint. The top 16 variables included in the most interpretable model are 
displayed on the next page.  Table  3. Model Performance: Full Dataset with Clinically Significant Variables and Algorithms.",What was the role of clinicians in the analysis of variables in the study? How did they contribute to the selection of variables for the model?
81,"Merekar 24 
 
 Top 16 Clinically Significant Variables with Descriptions  
Variable  Description   
Creatinine  of Recipient  Waste product filtered by kidney; a build -up may lead to 
cardiovascular disease.  
 
 Total Bil irubin  of 
Recipient  Associated with liver complications; a strong predictor in 
heart failure.  
 Ischemic Time  The time an organ has spent cooling/warming before 
transplant.  
 
 Most Recent Creatinine 
Measurement  of Recipient  Most recent evaluation of creatinine levels for a recipient 
at the time of listing.  
Age of Donor  Median Age: 32 years old  
Age of Recipient  Median Age: 55 years old  
PVR  of Recipient  Pulmonary vascular resistance for recipient measured at 
listing.  
Right Ventricular Mass of 
Donor  Both estimates of the myocardial mass of that ventricle 
based on echocardiographic measurements.  
Right Ventricular Mass of 
Recipient   
Systolic Pressure of 
Recipient  Pulmonary artery systolic pressure in mm/Hg at 
registration.  
Predictive Heart Mass 
Ratio  Total heart myocardial mass estimated by echocardiogram 
of donor divided by that of the recipient.  
Albumin Measurement of 
Recipient  Albumin measured in the recipient plasma at registration.  
Distance  Distance between the donor and the recipient  in nautical 
miles.  
Cardiac Output  of 
Recipient  Cardiac output of the recipient at registration.  
Recipient Waiting Days  Total days on the UNOS waiting list.  
GPT Measurement of 
Donor  GPT Level measured at time of transplant.  Table  4. Top 16 Clinically Significant Variables with Descriptions.",How does the Total Bilirubin of Recipient variable relate to heart failure according to the information provided?
82,"Merekar 25 
 
Chapter 6: Discussion & Insights  
After analyzing all datasets, it was apparent that advanced methods such as Tree Boosting 
(Extreme Gradient Boosting) and Stacking led to higher AUC’s than the baseline  Logistic 
Regression. This is because Tree Based algorithms are usually more complex. Although they 
may be prone to overfitting, pruning and correctly adjusting the number of trees resolves this 
issue. Logistic Regression is simpler to understand a nd less prone to overfitting, however, may 
not always grant optimal predictive performance.  
Running the Stacking algorithm on the full dataset yielded the highest AUC of about 
0.6810. Although this is significant, Stacking is just a collection of models, a nd cannot be truly 
used for interpretation. This is also why there were no Sensitivity metrics for those models. 
Aside from Stacking, Boosting also gave high results. When applied to the full dataset, the model 
created from this algorithm had an AUC of rou ghly 0.6804. In practice however, it would not be 
feasible for a cardiologist to assess 420 parameters (variables) when trying to understand the 
future of a patient. This brings in the idea of real -world use where models must be interpretable . 
When the ful l dataset was cut down by column, 25 variables remained. These were the most 
important variable s deemed by the Boosting model ran on  the full dataset. All algorithms were 
run on that subset of data and the Stacking algorithm yielded an AUC of 0.6719. The B oosting 
algorithm was very similar in performance, with a 0.6707 AUC. After counting recurring 
variables from all model variable importance plots, analysis was reduced to 16 variables. The 
Stacking algorithm gave an AUC of .6500 and the Boosting algorithm gave an AUC of .6491. 
This would be far more interpretable for clinicians as the parameters were brought down to only 
16 variables. Some performance is lost, but the difference is marginal (Fig. 6).",Discuss the trade-offs between model complexity and interpretability in the context of using Stacking and Boosting algorithms for predictive modeling.
83,"Merekar 26 
 
 
 
 
 
 
 
 
 
 
The prior studies discussed in Chapter 3 did not achieve an AUC greater than 0.66 when 
using advanced learning techniques. Through optimal parameter tuning, the Tree Boosting 
algorithms were able to achieve higher performance. In those studies, however, th e trend was 
leaning towards Deep Learning being a long -term solution in this field. In this study, running the 
Deep Learning algorithm on the full dataset only produced an AUC of 0.5706. This could have 
been again, due to data quality issues as neural netw orks do not perform well on rather sparse 
datasets. Even after cleaning, the data was not suitable to implement an effective Deep Learning 
model.  
 From a clinical standpoint, the aforementioned variables are reasonable to build models 
with going forward. C reatinine levels are often greater in patients that are hospitalized with heart 
failure, which validates this variable as a driver of mortality (Smith et al. 14). Liver function 
abnormalities such as high levels of Total Bilirubin are also associated with higher morality (van 
Deursen et al.). Age of Recipient is a good indicator of mortality; older people generally have 
poorer health. Ischemic time and Age of Donor are also reasonable because these represent 
Figure 6. Visual Depiction of Trade -off Betw een Performance and Interpretability for Boosting Models. 
Average AUC on the Y -Axis corresponds to the average of the Top 2 models from the Full Dataset, Full 
Dataset with Significant Variables, and Full Dataset with Clinically Significant Variables.","How are creatinine levels, liver function abnormalities, age of recipient, ischemic time, and age of donor identified as reasonable variables to build mortality prediction models in the clinical context? Provide examples from the text to support your answer."
84,"Merekar 27 
 
worse organ quality. Multiple studies have deemed  that age is an independent risk factor for 
mortality and argued that longer ischemi c times are associated with higher mortality outcomes 
(Kilic et al.).  
Another point to analyze is the comparisons of the row -wise dataset splits. Comparing the 
LVAD support datasets, it was apparent that there was not much of a difference between 
performance of those top models. The same can be said for time period, but models run on 
observations before 2006 yielded a marginally higher AUC. For Donor/Recipient gender 
combinations, it was found that the dataset containing Female Donors and Female Recipients 
gave an AUC of 0.6770. This exceeded the performance of all other Donor/Recipient gender 
combinations.  
Although the models ran on the row -wise splits did not  perform better than models from 
the full dataset with respect to AUC, they are useful in terms of Sensitivity. Albeit not discussed 
as thoroughly as AUC, it is meaningful to analyze this metric  in clinical decision making because 
in this case, False Negat ive Cases would be costly. The Tree Boosting model for the full dataset 
produced a Sensitivity of about .42. After running that algorithm on the Donor/Recipient gender 
combinations datasets, the average Sensitivity of the four was .463. Comparing those two  
metrics, Sensitivity of Donor/Recipient gender combinations saw an increase in Sensitivity by 
8% on average. This supports the claim that Donor/Recipient gender combinations add value in 
clinical decision making (see Fig. H in Appendix).",What was the finding regarding the performance of models based on Donor/Recipient gender combinations compared to models from the full dataset?
85,"Merekar 28 
 
Chapter 7: Concl usion  
This project aim ed to leverage Machine Learning techniques to predict one -year 
mortality after a heart transplant. Predictive models were constructed to help clinicians better 
understand underlying patterns in data from Donors and Recipients. This wa y, correct procedures 
for treatment may be taken from a medicinal standpoint. This study was continuously validated 
by clinicians, from Data Preparation to Evaluation. The data was split based on clinical factors 
such as LVAD Support, Donor/Recipient Gende r Combinations, and Time Period of Transplant. 
Finally, specific features were selected based on the clinician’s ability to interpret the created 
models.  
As mentioned previously, performance and interpretability vary inversely. This trade -off 
is important to understand in deployment. From the analysis done in this work, the Extreme 
Gradient Boosting model applied to the full dataset with 25 variables is the optimal one. It serves 
as a “middle -ground” between AUC and number of variables. This model performed  better than 
those from prior studies and uses 25 variables. Although it is not the most interpretable, the 
model will perform similarly when exposed to future, unseen data.  
Essentially, there are two schools of thought. The data scientist wants to optimiz e model 
performance, while the clinicians strive for the simplest model to understand. To that end, from a 
clinical standpoint, the model with 16 variables may be the most optimal. There is a modest 
discrepancy in AUC when compared to the model with 25 var iables, and theoretically, the 
clinician would trade marginal model performance for interpretability. The system needs to be 
fed with new data and using 16 parameters rather than 25 is more convenient, easier to 
track/measure, and simpler to enforce data i ntegrity constraints upon.","Compare and contrast the Extreme Gradient Boosting model with 25 variables and the model with 16 variables in terms of performance and interpretability. From a clinical standpoint, which model would be considered more optimal and why?"
86,"Merekar 29 
 
Chapter 8: Implications  
The last phase in the CRISP -DM Framework is Deployment  of a selected model. Rarely, 
creation of the model is not the end of the project as it still needs to be rolled out on a system, 
maintained, and updat ed periodically (Wirth and Hipp 7). It is important to continuously validate 
models built throughout the framework. By doing this, data scientists will be able to gain insight 
on whether or not models fit the context of the problem.  
Although there is a ne ed for the AI in healthcare as highlighted previously, there are a 
few pertinent issues when it comes to deployment and industry -wide adoption of these systems.  
Data Privacy  
 Data is essential for AI and model training, but some patients may be unwilling t o give 
other entities access to private records. Besides for training, a large data supply is needed for 
validation and improvement of these models. For widespread deployment, this sensitive data 
must be shared among numerous institutions. To combat this, these EHR’s must be anonymized 
and patients would need to be fairly informed. The shift to value -based care will support this, and 
further incentivize organizations to collect and ethically maintain this data for analysis (He et al. 
31). 
Data Standardizati on 
 From a data science standpoint, this is an important aspect to consider. Data 
standardization refers to the process of transforming data into a common format to be used for 
analysis. This way, it can be understood regardless of tools and methodologies (He et al. 33). In 
practice, data is collected in many different ways. It is stored in a variety of formats, databases, 
and information systems. Although this data may be formatted a certain way in one organization, 
if it is shared, another organization ma y not be able to properly interpret this for analysis. With",Discuss the challenges related to data privacy in the deployment and industry-wide adoption of AI systems in healthcare. How can organizations address these challenges while ensuring patient confidentiality?
87,"Merekar 30 
 
the complexity and volume of healthcare data in particular, this should occur in the initial phases 
of model development, even before the CRISP -DM Framework is carried out.  
Existing Workforce  
 There has been significant concern throughout multiple industries of Artificial 
Intelligence eliminating the need for human workers. Although some jobs have potential to be 
automated, this will likely limit overall job loss. In healthcare, c osts of automation  technologies 
and regulatory and social acceptance are some reasons as to why this may be curbed (Davenport 
and Kalakota 96).  
To overcome these aforementioned challenges, the workforce itself must understand that 
AI is not here to replace it. Inst ead, the workforce will be able to leverage it to augment existing 
workflows and decision making. For an industry -wide implementation, healthcare professionals 
must develop trust for these systems. Similarly, the onus is on patients to trust institutions t o 
handle their data ethically in hopes of improving their own outcomes .",What are some reasons why job loss due to automation in healthcare may be limited?
88,"Merekar 31 
 
Chapter 9: Limitations & Future Directions  
 There are a few limitations this research has faced, the first being the time period of the 
queried data. The UNOS Data spans from 1990 -2016 and does not take into account records after 
that. With the increased importance of data collection in the space, i t is reasonable to assume that 
future records will follow specific constraints. This entails that the data will contain less errors, 
yielding a more thorough analysis. The second limitation would be the interpretation of the 
variables in the full dataset. These are generally subjective since there are over 400, so another 
set of clinicians could have understood these variables from a different perspective than in this 
study. Finally, although discussed thoroughly, it was impossible to complete the CRISP -DM 
Framework. The optimal model was limited to simulation and could not  be deployed in a real use 
case.  
 For future studies, researchers will be able to leverage “cleaner” data from UNOS as data 
standards begin to conform. Aside from that, some questions are posed to future data 
scientist/clinician teams that have not been explored in this study:  
Can mortality be predicted independent of time period?  
 This is particularly significant as this project was limited to one -year mortality. If this 
limitation was rem oved, clinicians would be able to understand what may contribute to 
shorter/longer mortality periods.  
How robust will these models be? How will they scale?  
 Models must adapt to rapid change as new data is collected. Another point to examine 
would be identifying how these models would scale. This is useful from a data science 
perspective, and with the emergence of enterprise -wide data and cloud computing, analytic 
solutions (e.g. Random Forest, Tree Boosting ) have potential to scale rather well .","Why is it significant to explore the prediction of mortality independent of time period, and how would this benefit clinicians?"
89,"Merekar 32 
 
Appendix  
 
 
 
 
 
 
 
 
 
 
Figure A. Detailed Overview of the CRISP -DM Framework. Wirth, Rüdiger, and Jochen Hipp. 
CRISP -DM: Towards a Standard Process Model for Data Mining . p. 6.",Who are the authors of the document that introduced the CRISP-DM framework?
90,"Merekar 33 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure B. Sigmoidal Function in Logistic Regression. Note, the minimum value is 0 and the 
maximum value is 1. Logistic Regression Theory for Practitioners - Towards Data Science . 
https://towardsdatascience.com/the -data-scientists -field-guide -to-logistic -regression -part-1-
intuition -97084b11bd68 . Accessed 16 Apr. 2020.  
 
 
 
 
 
 
 
Figure C. Sample Artificial Neural Network. Management AI: Types Of Machine Learning Systems . 
https://www.forbes.com/sites /davidteich/2018/07/06/management -ai-types -of-machine -learning -
systems/#390b5ba832fb . Accessed 16 Apr. 2020.",What is the significance of the minimum and maximum values being 0 and 1 in logistic regression?
91,"Merekar 34 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure D. Bias -Variance Tradeoff. Bias and Variance in Machine Learning - Data Driven Investor - 
Medium. https://medium.com/datadriveninvestor/bias -and-variance -in-machine -learning -51fdd38d1f86 . 
Accessed 16 Apr. 2020.  
 
 
 
 
 
 
 
 
F",What are some strategies that can be employed to address bias and variance in machine learning models?
92,"Merekar 35 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure E. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset . 
 
 
 
 
 
 
 
 
F 
Figure F. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset  with Significant Variables.  
 
 
 
 
 
 
 
 
F 
Figure G. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset  with Clinically Significant Variables.",How does the ROC curve differ when applying the model to the full dataset versus when using only significant variables?
93,"Merekar 36 
 
 
0.4633050280.42567487
0.4 0.41 0.42 0.43 0.44 0.45 0.46 0.47XGBoost Recall Averages Donor/Recipient
Gender CombinationsXGBoost Recall Averages for Full Datasets
(No Row Splits)
SENSITIVITY (RECALL)SENSITIVITY MEASURES FOR TREE 
BOOSTING
Figure H. Sensitivity Comparisons: Full Datasets and Donor/Recipient Gender Combinations.  
 
 
 
 
 
 
 
F",Compare and contrast the XGBoost recall averages for full datasets with the donor/recipient gender combinations. How do they differ?
94,"Merekar 37  
References  
Amornsamankul, Somkid , et al. “A Comparison of Machine Learning Algorithms and Their 
Applications.” International Journal of Simulation: Systems, Science & Technology , Aug. 2019, 
pp. 1 –17. DOI.org (Crossref) , doi: 10.501 3/IJSSST.a.20.04.08 . 
 
Beam, Andrew L., and Isaac S. Kohane. “Big Data and Machine Learning in Health Care.” JAMA , 
vol. 319, no. 13, Apr. 2018, pp. 1317 –18. DOI.org (Crossref) , doi: 10.1001/jama.2017.18391 . 
 
Blagus, Rok, and Lara Lusa. “SMOTE for High -Dimensional Class -Imbalanced Data.” BMC 
Bioinformatics , vol. 14, no. 1, Dec. 2013, pp. 1 –16. DOI.org (Crossref) , doi: 10.1186/1471 -2105 -
14-106. 
 
Bradley, Andrew P. “The Use of the Area under the ROC Curve in the Evaluation of Machine 
Learning Algorithms.” Pattern Recognition , vol. 30, no. 7, July 1997, pp. 1145 –59. DOI.org 
(Crossref) , doi: 10.1016/S0031 -3203(96)00142 -2. 
 
Briscoe, Erica, and Jacob Feldman. “Conceptual Complexity and the Bias/Variance Tradeoff.” 
Cognition , vol. 118, no. 1, Jan. 2011, pp. 2 –16. DOI.org (Crossref) , 
doi:10.1016/j.cognition.2010.10.004 . 
 
Burrill, Steve. “Health Care Outlook for 2019: Five Trends That Could Impact Health Plans, 
Hospitals, and Patients.” Deloitte United States .","Discuss the application of SMOTE for handling high-dimensional class-imbalanced data, as presented in Blagus and Lusa's 2013 study."
95,"Bradley, Andrew P. “The Use of the Area under the ROC Curve in the Evaluation of Machine 
Learning Algorithms.” Pattern Recognition , vol. 30, no. 7, July 1997, pp. 1145 –59. DOI.org 
(Crossref) , doi: 10.1016/S0031 -3203(96)00142 -2. 
 
Briscoe, Erica, and Jacob Feldman. “Conceptual Complexity and the Bias/Variance Tradeoff.” 
Cognition , vol. 118, no. 1, Jan. 2011, pp. 2 –16. DOI.org (Crossref) , 
doi:10.1016/j.cognition.2010.10.004 . 
 
Burrill, Steve. “Health Care Outlook for 2019: Five Trends That Could Impact Health Plans, 
Hospitals, and Patients.” Deloitte United States . www2.deloitte.com , 
https://www2.deloitte.com/us/en/pages/life -sciences -and-health -care/articles/health -care-current -
december4 -2018.html . Accessed 2 Mar. 2020.  
 
Chen, Tianqi, and Carlo s Guestrin. “XGBoost: A Scalable Tree Boosting System.” Proceedings of the 
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - 
KDD ’16 , 2016, pp. 785 –94. arXiv.org , doi: 10.1145/2939672.2939785 . 
 
Collier, Matt, et al. Artificial Intelligence: Healthcare’s New Nervous System . Industry Outlook 
Report, Accenture, pp. 1 –8. 
 
D, Shashank, et al. “The Internet of Medical Things (IoMT).” Journal of Pharmaceutical Research , 
vol. 16, no. 4, Dec. 2017, p. 290.  
 
Dag, Ali, et al. “Predicting Heart Transplantation Outcomes through Data Analytics.” Decision 
Support Systems , vol. 94, Nov. 2016, pp. 42 –52.","How does the bias/variance tradeoff relate to conceptual complexity, as discussed in the article by Briscoe and Feldman?"
96,"785 –94. arXiv.org , doi: 10.1145/2939672.2939785 . 
 
Collier, Matt, et al. Artificial Intelligence: Healthcare’s New Nervous System . Industry Outlook 
Report, Accenture, pp. 1 –8. 
 
D, Shashank, et al. “The Internet of Medical Things (IoMT).” Journal of Pharmaceutical Research , 
vol. 16, no. 4, Dec. 2017, p. 290.  
 
Dag, Ali, et al. “Predicting Heart Transplantation Outcomes through Data Analytics.” Decision 
Support Systems , vol. 94, Nov. 2016, pp. 42 –52. DOI.org (Crossref) , 
doi:10.1016/j.dss.2016.10.005 . 
 
Davenport, Thomas, and Ravi Kalakota. “The Potential for Artificial Intelligence in Healthcare.” 
Future Healthcare Journal , vol. 6, 2019, pp. 94 –98. 
 
Fan, Jerome, et al. “Understanding Receiver Opera ting Characteristic (ROC) Curves.” CJEM , vol. 8, 
no. 01, Jan. 2006, pp. 19 –20. DOI.org (Crossref) , doi: 10.1017/S1481803500013336 .","Discuss the potential applications of artificial intelligence in healthcare as outlined by Davenport, Thomas, and Ravi Kalakota in their article."
97,"Merekar 38  
Gedela, Maheedhar, et al. A Brief Review of Left Ventricular Assist Devices and Their Management . 
pp. 19 -26. 
 
He, Jianxing, et al. “The Practical Implementation of Artificial Intelligence Technologies in 
Medicine.” Nature Medicine , vol. 25, no. 1, Jan. 2019, pp. 30 –36. DOI.org (Crossref) , 
doi:10.1038/s41591 -018-0307 -0. 
 
Heart Failure | National Heart, Lung, and Blood Institute (NHLBI) . 
https ://www.nhlbi.nih.gov/health -topics/heart -failure. Accessed 1 Mar. 2020 . 
 
Heart Transplant | National Heart, Lung, and Blood Institute (NHLBI) . 
https://www.nhlbi.nih.gov/health -topics/ heart -transplant . Accessed 1 Mar. 2020.  
 
Hong, Kimberly N., et al. “Who Is the High -Risk Recipient? Predicting Mortality After Heart 
Transplant Using Pretransplant Donor and Recipient Risk Factors.” The Annals of Thoracic 
Surgery , vol. 92, no. 2, Aug. 2011 , pp. 520 –27. DOI.org (Crossref) , 
doi:10.1016/j.athoracsur.2011.02.086 . 
 
Iyer, Arjun, et al. “Primary Graft Failure after Heart Transplantation.” Journal of Transplantation , 
vol. 2011, 2011, pp. 1 –9. DOI.org (Crossref) , doi: 10.1155/2011/175768 . 
 
James, Gareth, et al. An Introduction to Statistical Learning . Springer New York, 2013. DOI.org 
(Crossref) , doi: 10.1007/978 -1-4614 -7138 -7. 
 
Jiang, Fei, et al. “Artificial Intelligence in Healthcare: Past, Present and Future.” Stroke and Vascular 
Neurology , vol. 2, no. 4, Dec.","What are the key risk factors that can predict mortality after heart transplant, as mentioned in the study by Hong, Kimberly N., et al.?"
98,"DOI.org (Crossref) , 
doi:10.1016/j.athoracsur.2011.02.086 . 
 
Iyer, Arjun, et al. “Primary Graft Failure after Heart Transplantation.” Journal of Transplantation , 
vol. 2011, 2011, pp. 1 –9. DOI.org (Crossref) , doi: 10.1155/2011/175768 . 
 
James, Gareth, et al. An Introduction to Statistical Learning . Springer New York, 2013. DOI.org 
(Crossref) , doi: 10.1007/978 -1-4614 -7138 -7. 
 
Jiang, Fei, et al. “Artificial Intelligence in Healthcare: Past, Present and Future.” Stroke and Vascular 
Neurology , vol. 2, no. 4, Dec. 2017, pp. 230 –43. DOI.org (Crossref) , doi: 10.1136/svn -2017 -
000101 . 
 
Johnston, Stephen S., et al. “Using Machine Learning Applied to Real -World Healthcare Data for 
Predictive Analytics: An Applied Example in Bariatric Surgery.” Value in Health , vol. 22, no. 5, 
May 2019, pp. 580 –86. DOI.org (Crossref) , doi: 10.1016/j.jval.2019.01.011 . 
 
Kilic, Ahmet, et al. “Donor Selection in Heart Transplantation.” Journal of Thoracic Disease , vol. 6, 
no. 8, 2014, pp. 1097 –104. 
 
Medved, Dennis, et al. “Improving Prediction of Heart Transplantation Outcome Using Deep 
Learning Techniques.” Scientific Reports , vol. 8, no. 1, Feb. 2018, pp. 1 –9. DOI.org (Crossref) , 
doi:10.1038/s41598 -018-21417 -7. 
 
Miller, P. Elliott, et al.","How is artificial intelligence being utilized in healthcare, according to the article by Jiang et al.? Provide examples of past, present, and future applications mentioned in the text."
99,"22, no. 5, 
May 2019, pp. 580 –86. DOI.org (Crossref) , doi: 10.1016/j.jval.2019.01.011 . 
 
Kilic, Ahmet, et al. “Donor Selection in Heart Transplantation.” Journal of Thoracic Disease , vol. 6, 
no. 8, 2014, pp. 1097 –104. 
 
Medved, Dennis, et al. “Improving Prediction of Heart Transplantation Outcome Using Deep 
Learning Techniques.” Scientific Reports , vol. 8, no. 1, Feb. 2018, pp. 1 –9. DOI.org (Crossref) , 
doi:10.1038/s41598 -018-21417 -7. 
 
Miller, P. Elliott, et al. “Predictive Abilities of Machine Learning Techniques May Be Limited by 
Dataset Characteristics: Insights From the UNOS Database.” Journal of Cardiac Failure , vol. 
25, no. 6, June 2019 , pp. 479 –83. DOI.org (Crossref) , doi: 10.1016/j.cardfail.2019.01.018 . 
 
Mitchell, Rory, and Eibe Frank. “Accelerating the XGBoost  Algorithm Using GPU Computing.” 
PeerJ Computer Science , vol. 3, July 2017, pp. 1 –28. DOI.org (Crossref) , doi: 10.7717/peerj -
cs.127 .","How do dataset characteristics impact the predictive abilities of machine learning techniques, according to the study by Miller et al.?"
100,"Merekar 39  
NEJM Catalyst. “What Is Value -Based Healthcare?” Catalyst Carryover , vol. 3, no. 1, Massachusetts 
Medical Society, Jan. 2017. catalyst.nejm.org (Atypon) , doi: 10.1056/CAT.17.0558 . 
 
Parreco, Joshua, and Matthew Chatoor. Comparing Machine Learning Algorithms for Predicting 
Acute Kidney Injury . no. 7, July 2019, pp. 725 –29. 
 
Rodriguez, J. D., et al. “Sensitivity Analysis of K -Fold Cross Validation in Prediction Error 
Estimation.” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 32, no. 3, 
Mar. 2010, pp. 569 –75. DOI.org (Crossref) , doi: 10.1109/TPAMI.2009.187 . 
 
Roski, Joachim, et al. “Creating Value In Health Care Through Big Data: Opportunities And Policy 
Implications.” Health Affairs , vol. 33, no. 7, July 2 014, pp. 1115 –22. DOI.org (Crossref) , 
doi:10.1377/hlthaff.2014.0147 . 
 
Sakkis, Georgios, et al. Stacking Classifiers for Anti -Spam Filtering of e -Mail . 2001, pp. 1 –7. 
 
Sarle, Warren S. Neural Network s and Statistical Models . Apr. 1994, pp. 1 –12. 
 
Sherman, Rick. Business Intelligence Guidebook: From Data Integration to Analytics . Elsevier, 
Morgan Kaufmann, 2015.  
 
Smith, Grace L., et al. “Worsening Renal Function: What Is a Clinically Meaningful Change in 
Creatinine during Hospitalization with Heart Failure?” Journal of Cardiac Failure , vol. 9, no. 1, 
Feb. 2003, pp. 13 –25. DOI.org (Crossref) , doi: 10.1054/jcaf.2003.3 .",Compare and contrast the machine learning algorithms used for predicting Acute Kidney Injury in the study by Parreco and Chatoor. What were the key findings of this comparison?
101,"Sakkis, Georgios, et al. Stacking Classifiers for Anti -Spam Filtering of e -Mail . 2001, pp. 1 –7. 
 
Sarle, Warren S. Neural Network s and Statistical Models . Apr. 1994, pp. 1 –12. 
 
Sherman, Rick. Business Intelligence Guidebook: From Data Integration to Analytics . Elsevier, 
Morgan Kaufmann, 2015.  
 
Smith, Grace L., et al. “Worsening Renal Function: What Is a Clinically Meaningful Change in 
Creatinine during Hospitalization with Heart Failure?” Journal of Cardiac Failure , vol. 9, no. 1, 
Feb. 2003, pp. 13 –25. DOI.org (Crossref) , doi: 10.1054/jcaf.2003.3 . 
 
Srivastava, Siddharth, et al. “Deep Learning for Health Informatics: Recent Trends and Future 
Directions.” 2017 International Conference on Advances in Computing, Communications and 
Informatics (ICACCI ), IEEE, 2017, pp. 1665 –70. DOI.org (Crossref) , 
doi:10.1109/ICACCI.2017.8126082 . 
 
United Network for Organ Sharing | UNOS | US Organ Transplantation . https://unos .org/. Accessed 
1 Mar. 2020.  
 
van Deursen, V. M., et al. “Abnormal Liver Function in Relation to Hemodynamic Profile in Heart 
Failure Patients.” Journal of Cardiac Failure , vol. 16, no. 1, Jan. 2010, pp. 84 –90. DOI.org 
(Crossref) , doi: 10.1016/j.cardfail.2009.08.002 . 
 
Wirth, Rüdiger, and Jochen Hipp. CRISP -DM: Towards a Standard Process Model for Data Mining . 
pp. 1 –10.","According to Srivastava et al., what are some recent trends and future directions of deep learning for health informatics?"
102,"City Univ ersity of New Y ork (CUN Y) City Univ ersity of New Y ork (CUN Y) 
CUN Y Academic W orks CUN Y Academic W orks 
Student Theses Baruch College 
Spring 5-18-2020 
Emer ging T echnologies in Healthcar e: Analysis of UNOS Data Emer ging T echnologies in Healthcar e: Analysis of UNOS Data 
Through Machine Learning Through Machine Learning 
Reyhan Mer ekar 
CUN Y Bernar d M Baruch College 
How does access t o this work benefit y ou? Let us know! 
More information about this work at: https:/ /academicworks.cuny .edu/bb_etds/100 
Disco ver additional works at: https:/ /academicworks.cuny .edu 
This work is made publicly a vailable b y the City Univ ersity of New Y ork (CUN Y). 
Contact: AcademicW orks@cuny .edu",Who can be contacted for more information about the work made publicly available by CUNY?
103,"Emerging Technologies in Healthcare : Analysis of UNOS Data Through Machine Learning  
 
 
by 
 
 
Reyhan Merekar  
 
 
Submitted to the Committee on Undergraduate Honors at Baruch College of the City University 
of New York in partial fulfillment of the requirements for the degree of Bachelor of Business 
Administration in Computer Information Systems with Honors  
 
 
April 27 th, 2020",When was Reyhan Merekar's research project submitted to the Committee on Undergraduate Honors?
104,"Table of Contents  
 
Acknowledgements  ................................ ................................ ................................ ........................  i 
Abstract  ................................ ................................ ................................ ................................ ..........  ii 
Chapter 1: Introduction  ................................ ................................ ................................ ...............  1 
Chapter 2: Background & Additional Context  ................................ ................................ ..........  3 
Moving to Performance Based Medicine  ................................ ................................ ..............................  3 
Virtual Visits and Wearables  ................................ ................................ ................................ .................  4 
Leveraging Artificial Intelligence  ................................ ................................ ................................ ..........  4 
Emergence of Big Data in Healthcare  ................................ ................................ ................................ ... 5 
Chapter 3: Related Work  ................................ ................................ ................................ .............  7 
Chapter 4: Data & Methodology  ................................ ................................ ...............................  10 
Busi ness Understanding  ................................ ................................ ................................ ........................  11 
Data Understanding  ................................ ................................ ................................ ..............................  11 
Data Preparation  ................................ ................................ ................................ ................................ ... 11 
Model Building  ................................ ................................ ................................ ................................ ...... 12 
Discussion of Rele vant Algorithms  ................................ ................................ ................................ ....................  13 
Resampling Methods  ................................ ................................ ................................ ................................ ..........  17 
Model Evaluation  ................................ ................................ ................................ ................................ .. 18 
Key Metric: Area  Under the Receiver Operating Characteristic Curve (AUC)  ................................ .................  20 
Variable Analysis  ................................ ................................ ................................ ................................ ...............  20 
Chapter 5: Results ................................ ................................ ................................ .......................  21 
Clinically Validated Variables  ................................ ................................ ................................ .............  23 
Chapter 6: Discussion & Insights  ................................ ................................ ..............................  25 
Chapter 7: Conclusion  ................................ ................................ ................................ ................  28 
Chapter 8: Implications  ................................ ................................ ................................ ..............  29 
Data Priva cy ................................ ................................ ................................ ................................ ...........  29 
Data Standardization  ................................ ................................ ................................ ............................  29 
Existing Workforce  ................................ ................................ ................................ ...............................  30 
Chapter 9: Lim itations & Future Directions ................................ ................................ ............  31 
Can mortality be predicted independent of time period?  ................................ ................................ .. 31 
How robust will these models be? How will they scale?  ................................ ................................ .... 31 
Appendix  ................................ ................................ ................................ ................................ ...... 32 
References  ................................ ................................ ................................ ................................ .... 37","What are the key implications of data privacy, data standardization, and the existing workforce in the healthcare industry as outlined in Chapter 8?"
105,"Merekar i  
 
Acknowledgements  
 First and foremost, I would like to thank my Faculty Advisor, Professor Arturo 
Castellanos, for providing feedback and guidance throughout this project and most of my 
undergraduate career. We have worked on a few projects together, and it has been a privil ege to 
learn from him throughout these four years. Additionally, I would like to thank my two Faculty 
Readers, Professors Kevin Craig and Zeda Li, for taking time to read my work and provide 
important feedback.  
 Special thanks to Claudio A . Bravo, Miguel A lvarez, and Mahek Shah for devoting time 
out of their busy schedules to provide valuable insight and context throughout this project.  
 Of course, I would like to thank my family for continuously supporting me through my 
undergraduate studies. All of this w ould not be possible without their belief in me.","Why are Claudio A. Bravo, Miguel Alvarez, and Mahek Shah thanked in the acknowledgements section and what was their role in the project?"
106,"Merekar ii  
Abstract  
 The healthcare industry is primed for a massive transformation in the coming decades 
due to emerging technologies such as Artificial Intelligence (AI) and Machine Learning. Wi th a 
practical application to the UNOS (United Network of Organ Sharing) database, this Thesis 
seeks to investigate how Machine Learning and analytic methods may be used to predict one-
year heart transplantation outcomes. This study also sought to improve on predictive 
performances from prior studies by analyzing  both Donor and Recipient data. Models built wi th 
algorithms such as Stacking and Tree Boosting gave the highest performance, with AUC’s of 
0.681 0 and 0.68 04, respectively. In this work, a roadmap w as created that justifies the need for 
these technologies in healthcare. In application, the data was prepared, models were built using 
advanced algorithms, and important variables were selected. These steps were continuously done 
with validation from expe rienced clinicians. To yield greater insights in this study, the dataset 
was split row -wise by factors such as LVAD Support, Donor/Recipient Gender Combinations, 
and Time Period; this rendered 8 new datasets for analysis. This work explores the trade -off 
between interpretability and performance in applying analytic methods in a real -world problem 
in this domain. Finally, forward looking industry implications are discussed.","How did the researchers split the dataset for analysis in order to yield greater insights, and what trade-off did they explore in applying analytic methods to the real-world problem?"
107,"Merekar 1 
 
Chapter 1: Introduction  
Heart transplantation has been carried out since the 1970s, but still remains one of the 
riskiest procedures today. Formally, a heart transplant is defined as the surgical replacement of 
the heart of a diseased individual with that of a healthy donor  (National Heart, Lung, and Blood 
Institute , “Heart Transplant” ). Typically, p atients who have end -stage heart failure, where the 
heart is severely damaged or weakened, undergo this procedure. Heart failure is caused by 
conditions such as coronary heart disease, hereditary conditions, and/or viral infections (National 
Heart, Lung, and Blood Institute , “Heart Transplant” ). A patient in need of a heart transplant can 
locate donor organs through the United Network for Organ Sharing. This private, non -profit 
organization manages the United States’  organ transplant system and provides a computerized 
national waiting list which assures equal access and fair distribution of organs as they become 
available  (United Network for Organ Sharing, “About UNOS”) . 
Several risk factors are associated with heart transplantation . The first is primary graft 
dysfunction (PGF), which occurs when the donor heart fails and is unable to function (National 
Heart, Lung, and Blood Institute , “Risk Factors” ). This is an immediate issue and usually leads 
to a quick time of de ath for the patient. It is also a major contributor to mortality and 
additionally, may lead other complications (Iyer et al. 1 -2). The patient’s immune system may 
also reject the newly transplanted heart within the first six months of transplantation. To c ombat 
this, the patient must take additional medicine to suppress the immune system. Long term side 
effects associated with this medicine include diabetes, osteoporosis, and kidney damage 
(National Heart, Lung, and Blood Institute , “Risk Factors”).",How does the United Network for Organ Sharing assist patients in need of a heart transplant?
108,"Merekar 2 
 
The p resence of technology in healthcare, particularly data science, has begun to emerge 
within  the past few decades. Professionals are beginning to explore the implications of using data 
to provide reliable solutions. H eart transplantation  procedures  are prime d to increase in the 
coming years , which will be driven by the aging population in the United States. According to 
Primers in Medicine, the number of people older than 65 will “double  by 2060 ” (Gedela et al. 
19). Another key trend is heavy investment in AI  and Machine Learning. As reported by 
Accenture, AI investment by healthcare firms will increase to $6B by 2021  (Collier et al. 2).  As 
more people are at risk for end -stage heart failur e and other diseases, AI  and Machine Learning  
can be leveraged to incr ease predictive power  for heart transplantation outcomes and disease 
detection.  
This project aims to  use Machine Learning 1 techniques to predict one -year heart 
transplantation outcomes using queried data from the UNOS registry from 1990 -2016. It also 
strives to build on prior studies in this domain. Predictive models are constructed to help 
clinicians better understand underl ying patterns in Donor and Recipient data. Since the 
underlying models will be able to predict the likelihood of a patient’s survival after one year, 
they will allow the clinician (s) to take the appropriate course of action for treatment .
 
1. In this work, the terms Machine Learning and Data Mining are used interchangeably.",What is the main objective of the project mentioned in the context? How does it aim to utilize Machine Learning techniques in predicting heart transplantation outcomes?
109,"Merekar 3 
 
Chapter 2: Backgr ound & Additional Context  
 The imminence of technology within healthcare has grown rapidly and has been a driver 
of major industry shifts throughout the past decade. One example of such advancements is the 
emergence of AI, specifically, Machine Learning. F ormally defined,  it is the practice of using 
algorithms to build models that learn from any n-number of observations and try to emulate the 
underlying pattern of the phenomena (Beam and Kohane 1317). This concept allows the 
computer to autonomously make de cisions without instructions from the researcher and is better 
suited for higher dimensional datasets where relationships may not necessarily be linear (Beam 
and Kohane 1317). The following trends emphasize why AI will be prevalent and contribute to 
the ev olving landscape of healthcare/medicine.  
Moving to Performance Based Medicine  
The healthcare system in the United States is now moving toward a financial model based 
on value rather than volume. The onus is now on delivering excellent population health thr ough 
treating patients like members. Rather than accounting for revenue due to patient volume, this 
value -based model shows each visit as an expense rather than a source of revenue (Burrill, 
“Health Care Outlook for 2019: Five Trends That Could Impact Heal th Plans, Hospitals, and 
Patients ”). 
This shift will take time, as the current transition has not been entirely smooth. In the 
short term, healthcare firms may see financial hits before longer -term costs decrease. Despite 
this, the value -based model has been embraced as the best method in lowering healthcare costs 
while increasing the quality of care. Since patients are seeking the best care possible, Machine 
Learning can be a catalyst in  providing that, helping people live healthier lives (NEJM Catalyst, 
“What is Value Based Healthcare?”) .",How can Machine Learning be a catalyst in providing better care and helping people live healthier lives in the evolving landscape of healthcare/medicine?
110,"Merekar 4 
 
Virtual Visits and Wearables  
 The general population dreads visiting the doctor, and often, waits until a later date when 
the condition has become more  severe to visit one. This mentality drives up costs for the patient. 
Virtual visits and telehealth serve as a basis to interact with a caregiver without attending the 
office. According to Steve Burrill of Deloitte, this technology helps them “see more pat ients, 
deal with rising clinical complexity, and support patients as they take a greater role in their own 
care.” There is much room to leverage this practice, as currently, only 14% of caregivers are 
utilizing this (Burrill, “ Health Care Outlook for 2019:  Five Trends That Could Impact Health 
Plans, Hospitals,  and Patients”).  
 As the popularity of wearables (e.g., Apple Watch, FitBit) grow, so does the data they 
transmit. The Internet of Medical Things (IoMT) is the health spin -off of the Internet of Thing s. 
This phenomenon can be explained as the “collection of medical, drug delivery devices and 
applications that connect to healthcare IT systems through online computer networks” (D et al. 
290). Medical devices equipped with Wi -Fi allow for machine -to-machi ne interactions. Such a 
phenomenon can help clinicians/healthcare professionals collect data points that may be used for 
disease prediction, patient status checks, and drug developments.  
Leveraging Artificial Intelligence  
 Artificial Intelligence lies at the center of all of these trends. For example, in a virtual 
visit, software can be used to track a person’s mood. Rather than meeting with someone, data 
from a patient’s Electronic Health Record (EHR) can help manage illnesses. Data from 
wearables  and tracking will be used to predict what a diagnosis may be, what drugs can be 
developed to help that will help the patient, and realistic timelines of treatment.","How can Artificial Intelligence be leveraged in virtual visits, wearables, and tracking data to manage illnesses, predict diagnoses, develop drugs, and determine treatment timelines?"
111,"Merekar 5 
 
 AI aims to mimic human cognition. Rather than fully replace doctors, it must be used in a 
way where professionals are working with AI to enhance clinical decisions. Now, with various 
analytic methods and collections of EHR’s, this is becoming a reality. For example, it is already 
making waves in radiology, oncology, disease prediction/preventio n, and outcome prediction 
with AI system IBM Watson. The system includes underlying Machine Learning models but is 
also a pioneer in the field. According to Jiang et al., “99% of the treatment recommendations 
from Watson are coherent with the physician dec isions” (241). Several reviews have appeared in 
literature referencing similar analytic methods in healthcare; these have covered techniques, 
algorithms, and dataset evaluations. Additionally, research in this space is growing, as the 
number of published p apers has increased by nearly 300% from 2008 to 2015 (Srivastava et al. 
1665). This trend also contributes to the motivation for this study.  
Emergence of Big Data in Healthcare  
 Big data has an incredible potential to yield significant value in healthcar e. This will be 
driven by decreasing costs of data storage, access to powerful but remote cloud computing, 
proliferation of “smart” devices, and the increase in electronic communication. Take for example 
healthcare titan Kaiser Permanente, which consists o f approximately nine million members. The 
firm has the capability to manage up to 44 petabytes of data through its EHR. This is “4,400 
times the equivalent of the data stored in the Library of Congress” (Roski et al. 1115). This 
implies that there is a vas t amount of data readily available for analysis in healthcare.  
 Big data is typically understood as a combination of three concepts – volume, velocity, 
and variety. Volume refers to the amount of data currently present in an enterprise; many expert s 
assert that “90% of the data (stored)” has been created only over the last eight years (Sherman 4). 
Velocity measures the time sensitivity of data; reporting and analysis need to be immediate and",Provide an example of a healthcare organization mentioned in the text that is utilizing big data for analysis.
112,"Merekar 6 
 
there is greater pressure to bridge the gap betwee n when the data is acquired and when it is 
analyzed. Finally, variety is the idea that data is now collected from many different sources. It 
can be structured (e.g., tabular) or unstructured (e.g., emails, documents, PowerPoints). In this 
work, these oppor tunities are taken advantage of, and findings from prior studies are considered .",How can researchers leverage findings from prior studies to take advantage of the opportunities presented by the variety of data sources available today?
113,"Merekar 7 
 
Chapter 3: Related Work  
 As mentioned previously, this project builds  on prior studies in the field. Machine 
Learning and analytic methods have been explored not only for hear t transplantation, but in other 
realms of clinical decision making as well. For example, a study was conducted that analyzed the 
risk of acute kidney injury (AKI), which is associated with chronic kidney disease and poses a 
high risk of mortality (Parreco and Chatoor 725). Another study used machine learning 
techniques to predict remission outcomes of Type 2 Diabetes following bariatric surgery 
(Johnston et al. 580).  
 Although these studies are not directly related to heart transplantation, they do have 
aspects in common. Those studies, along with this one, are Classification problems where the 
response variable is binary, the methodology is relatively similar, and the same evaluation 
method of Area Under the Curve (AUC) is utilized.  Additionally, a common theme in Machine 
Learning studies in healthcare is the Logistic Regression model serves as a baseline for model 
comparison.  
 Moreover, directly related studies have been conducted with  the same  data in which 
Machine Learning was use d to predict one-year mortality. One example of a similar study 
include d a report from the Journal of Cardiac Failure, which compared results from traditional 
statistical techniques with more advanced techniques . This study employed six  variables: age of 
recipient, creatinine, body mass index, liver function tests, aspartate transaminase, and 
hemodynamics. With the given variables, models were created with traditional statistical 
techniques and machine learning algorithms; these were all evaluated  by the me tric AUC. The 
implementation of Deep Learning  models yielded the best AUC, which was roughly 0.66.  At the 
end, it was deemed that the implementation of more advanced techniques failed to yield a n","In the study mentioned from the Journal of Cardiac Failure, what variables were used to create models for predicting one-year mortality, and what was the best AUC achieved with the implementation of Deep Learning models?"
114,"Merekar 8 
 
improved result when compared to traditional approaches, as there was a modest discrepancy 
between the two (Miller et al. 3 -4). One possible drawback to this study was that the researchers 
only used the recipient data and did not emphasize the donor’s data. The study was also only 
limited to univaria te variables.  
In another  similar study , clinicians and data scientists utilized existing models to predict 
one-year mortality outcomes . The two models were IMPACT (Index for Mortality Prediction 
After Cardiac Transplantation) and IH TSA (International Heart  Transplantation Survival 
Algorithm) which both implemented Machine Learning and Deep Learning techniques. The 
IMPACT model yielded an AUC of 0.608 with 18 variables being used, while the IH TSA model 
yielded an AUC of 0.64 with 32 variables  (Medved et al.  3-6). One drawback to this study is that 
it was ambiguous as to which variables were used when each model was initially created .  
Another similar study was carried out where analytic methods were leveraged  to predict 
mortality outcomes, however, this study addressed a slightly different problem and diff ered in 
overall methodology. Various analytic techniques were still carried out, but this paper considered 
one, five, and nine -year mortality instead. To impute missing values for bias removal, the 
researchers used Synthetic Minority Oversampling TEchnique  (SMOTE) which is used to 
“improve random oversampling” (Blagus and Lusa 2). This technique is well suited for lower 
dimensional data but is not ideal in higher dimensional settings. The best performance this study 
was able to uncover were AUC’s of 0.624, 0.676, and 0.838 for one, five, and nine -year 
mortality, respectively (Dag et al. 47 -49).  
 In this study however, Donor and Recipient data will be analyzed through advanced 
algorithms to improve  overall predictive power in heart transplantation.  Additiona lly, the data 
will be split in various ways to potentially yield further insights and variables are to be","In the upcoming study on heart transplantation, what is the focus of the analysis in terms of donor and recipient data, and how do researchers plan to improve the overall predictive power?"
115,"Merekar 9 
 
eliminated from the full dataset for interpretability. This is further explained in following 
chapters.","As a data analyst, how would you ensure interpretability when dealing with datasets that require elimination of certain data points?"
116,"Merekar 10 
 
Chapter 4: Data  & Methodology  
 Data mi ning projects such as these have flexibility to be creative and dynamic. There is, 
however, an accepted framework known as CRISP -DM ( CRoss Industry Standard Process for 
Data Mining) which is independent in industry and technology used. This framework aims to 
organize the steps in a data mining project. CRISP -DM has been seen as beneficial for 
organizations conducting large data mining projects in terms of cost, reliability, and 
manageability. This is the overarching methodology used for this project to keep  a rigid structure 
and reduce errors. The relevant five elements of this framework are Business Understanding, 
Data Understanding, Data Preparation, Model Building, and Evaluation.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 1. Phases of the CRISP -DM Model for Data Mining/Machine Learning. CRISP -DM – a 
Standard Methodology to Ensure a Goo d Outcome - Data Science Central . 
https://www.datasciencecentral.com/profiles/blogs/crisp -dm-a-standard -methodology -to-ensure -a-
good -outcome .",How does the CRISP-DM model ensure a good outcome in data mining and machine learning projects? Provide examples to support your answer.
117,"Merekar 11 
 
Business Understanding  
 The initial phase of the framework involves defining the problem at hand solely from the 
perspective of the business. Following this, the problem is converted into one that can be solved 
by data mining (Wirth and Hipp 5). In this case, the project lies in the domain of 
healthcare/medicine, so that was kept in mind throughout the process. The clearly defined 
problem is to understand and predict one -year mortality after a heart transplant.  
Data Understanding  
 The next phase of the framework is understanding t he data. This begins with obtaining 
data and conducting elementary observations to pick up on any immediate trends. Here, data 
quality issues are also identified (Wirth and Hipp 5). The corresponding query from the UNOS 
database yields a 32018 by 558 data frame. Each row represents an individual patient, and each 
patient has descriptive variables. It was apparent that the quality of the data was not quite up to 
par; missing values were present which made conducting initial analyses difficult . The data types  
of some variables also were not correct.  
Data Preparation  
 The data preparation phase spans all activities used to create the final data set; the one(s) 
that will be used in the model building process. This phase may be repeated many times in a 
given project. Some tasks include feature selection, data cleaning, and additional data 
transformation. In this project, the data had to be “cleaned” (e .g., adjusting data types, removing 
features, dealing with missing values) (Wirth and Hipp 5). One example of an adjusted data type 
was the response  variable, “One_year_mortality_retransplant”. This had to be changed to a 
“factor” data type. According to Python’s pandas documentation, these data types take on a 
“limited, and usually fixed, number of possible value s.” Missing values were dealt with","Describe the data preparation phase in the project. What tasks were involved in preparing the data for the model building process, and how was the 'One_year_mortality_retransplant' variable adjusted for the analysis?"
118,"Merekar 12 
 
differently depending  on the algorithm used to build each model. Cleaning was validated with 
clinicians, as they provided more insight for understanding the variables. In this project, the 
target variable had categories “ Died” or “Has not died” signified by the binary 1 and 0, 
respectively. In terms of feature selection, initially, many variables were omitted due to lack of 
variability cutting down the count to 420. Feature Selection was used again to reduce the number 
of variables to improve interpretability. This is further discussed at the end of the chapter.  
 Additionally, the data was iteratively prepared by splitting on factors such as time period 
of transplant, LVAD support, and Donor/Recipient gender combinations. W ith respect to time 
period, two new datasets were derived based on transplantation dates before and after 2006. For 
gender, each donor/recipient combination was analyzed, yielding four new datasets. The four 
combinations were male donor/male recipient, mal e donor/female recipient, female donor/male 
recipient, and female donor/female recipient. A Left Ventricular Assist Device (LVAD) is used 
when a patient is near heart failure but cannot acquire a transplant immediately. This device is 
installed to assume t he role of providing blood to vital organs (Gedela et al. 19). Two datasets 
were created here to understand how the analyses would change depending on whether the 
recipient had an LVAD or not.  
Model Building  
 In this phase, models are built and tested. Usu ally, there are several techniques that can 
be used to create models (Wirth and Hipp 6). The specific algorithms used for this study were 
Logistic Regression, Decision Trees/Random Forest, Tree Boosting, Neural Networks (Deep 
Learning), and Stacking. Logis tic Regression is the most traditional algorithm used in medicine, 
so for the purposes of this project, it serve d as the baseline for comparison. All models were built 
using the H2O module in the Python programming language powered by Amazon Web Services.",Which algorithms were used for model building in this study and why was Logistic Regression chosen as the baseline for comparison?
119,"Merekar 13 
 
Discussion of Relevant Algorithms  
1. Logistic Regression  
 Logistic Regression is a widely used algorithm when the desired outcome is a question of 
classification. Rather than modeling a specific response directly, Logistic Regression model s the 
probability that the desired outcome belongs in a particular category. As the algorithm deals with 
probability, the related  sigmoidal  function must only produce outputs between 0 and 1. Below is 
the general logistic function for a multivariate Logist ic Regression (see Fig . B in Appendix for 
visual representation):  
𝑝(𝑋)= 𝑒𝛽0+𝛽1𝑋1+⋯+𝛽𝑝𝑋𝑝
1+𝑒𝛽0+𝛽1𝑋1+⋯+𝛽𝑝𝑋𝑝, 
where  𝑋=(𝑋1,𝑋2,…,𝑋𝑝) are p predictors. In this work, th e model derived from this algorithm 
served as a baseline for the performance of other  “advanced”  models  (James et. al 130 -133). 
With respect to models built with Logistic Regression, missing values were filled by mean 
imputation.  
2. Decision Trees/Random Fores t 
 Decision Tree is a hierarchical structure composed of branches and nodes . These are 
essentially a collection of ‘if/else’ statements that split decisions into binary classifications (in 
this case, “Died”  or “Has not died” ). Formally, this algorithm involves stratifying or segmenting 
the data set into a number of simple regions a nd making predictions (James et. al 306 -312):",What is mean imputation and how is it used in models built with Logistic Regression? How does it help in handling missing values in the dataset?
120,"Merekar 14 
 
 
 
 
 
 
 
 
Although decision trees are relatively simple to interpret, their downfall lies in the fact 
that they are not robust. A small change in the data can yield quite a large change in the final 
estimated tree. To correct this problem, the Random Forest algorithm may be implemented, 
which is essentially a larg e collection of decision trees ( Amornsamankul et al.  3). This algorithm 
corrects the instability of Decision Trees by bootstrap aggregating and decorrelating trees. This 
leads to stronger predictive power and lessens the risk of overfitting, which is touch ed on later in 
this chapter. With these models, missing values were filled by using the mode of respective 
variables.a. Divide the predictor space (the set of possible values for X1, X2, …, Xp) into J distinct and 
non-overlapping regions, R1, R2,…, R J. 
b. Create each binary partition based on node purity . This can be quantified by the Gini 
Index, where a pure node would produce a value closer to 0 ( 𝑝̂𝑚𝑘 represents the 
proportion of the kth classification in the mth region) : 
𝐺= ∑𝑝̂𝑚𝑘(1−𝑝̂𝑚𝑘) 𝐾
𝑘=1 
c. For every observation that falls into the region Rj, the same prediction is made , which is 
simply the mode  of the response values for the training observations in Rj.  
 
 Figure 2. Decision Tree Algorithm . James, Gareth, et al. An Introduction to Statistical Learning . 
Springer New York, 2013. DOI.org (Crossref) , pp. 306 -312, doi:10.1007/978 -1-4614 -7138 -7.",How are missing values handled in the models discussed in the document? Provide an example of how missing values are filled in the predictor space.
121,"Merekar 15 
 
3. Tree Boosting  
 Another way to combat Decision Tree instability is by using Boosting. Tree Boosting 
(e.g., Gradient Boosting Machine, Extreme Gradient Boosting) is widely used in machine 
learning  to achieve optimal performance. It is an ensemble method that sequentially creates new 
members; the newest member is created to account for incorrectly labeled inst ances from 
previous learners to minimize the loss function (direct relationship to error). The results of new 
trees are then applied partially to the entire solution. The algorithm executes M boosting 
iterations to learn a function F(x) that outputs predic tions ŷ = F(x)  while simultaneously 
minimizing a loss function L (y, ŷ). At each iteration, a new estimator f(x) is added to correct the 
prediction of y for each instance in training. This is shown formally below:  
 
 
 
 
 
 
 
 
 
 
 
 
 a. Start with a function which approximate s the true relationship of x and y: 
𝐹𝑚+1(𝑥)= 𝐹𝑚(𝑥)+𝑓(𝑥)=𝑦 
𝑓(𝑥)=𝑦−𝐹𝑚(𝑥) 
b. This fits the model f(x) for the current boosting iteration to the errors above 
(difference of actual and predicted). This can be shown as a gradient descent 
algorithm when the los s function is the squared error:  
𝐿(𝑦,𝐹(𝑥))= 1
2(𝑦−𝐹(𝑥))2 
c. Let the summation of this loss function be denoted as J. The goal is to minimize J by 
adjusting F(xi), the function of a particular instance.",Discuss the role of the loss function in the Extreme Gradient Boosting algorithm and how it is minimized through adjusting the function of a particular instance. Provide an example to illustrate this concept.
122,"This can be shown as a gradient descent 
algorithm when the los s function is the squared error:  
𝐿(𝑦,𝐹(𝑥))= 1
2(𝑦−𝐹(𝑥))2 
c. Let the summation of this loss function be denoted as J. The goal is to minimize J by 
adjusting F(xi), the function of a particular instance.  
𝐽= ∑𝐿(𝑦𝑖,𝐹(𝑥𝑖))
𝑖 
𝑑𝐽
𝑑𝐹(𝑥𝑖)=𝑑(∑𝐿(𝑦𝑖,𝐹(𝑥𝑖)) 𝑖 )
𝑑𝐹(𝑥𝑖)=𝐹𝑚(𝑥𝑖)− 𝑦𝑖 
d. Thus, errors are equal to the negative gradient of the squared error loss function:  
𝑓(𝑥)=𝑦−𝐹𝑚(𝑥)=−𝑑(∑𝐿(𝑦𝑖,𝐹(𝑥𝑖)) 𝑖 )
𝑑𝐹(𝑥𝑖) 
 
Figure 3 . Tree Boosting Algorithm . Mitchell, Rory, and Eibe Frank. “Accelerating the XGBoost Algorithm Using GPU 
Computing.” PeerJ Computer Science , vol. 3, July 2017, pp. 3-4. DOI.org (Crossref) , doi: 10.7717/peerj -cs.127 .",How are errors related to the negative gradient of the squared error loss function in the context of the tree boosting algorithm?
123,"Merekar 16 
 
By adding a model that approximates this, the loss function is further minimized (Mitchell and 
Frank 3 -4). 
Often in the building phase, this algorithm will produce the strongest model in all facets 
of evaluation as it is inherently robust. Some of the drawbacks to this is that Tree Boosting tends 
to overfit but can also be corrected by adjusting tree sizes by p runing (Chen and Guestrin 3 -5). 
With these models, missing values were filled by using the mode of respective variables.  
4. Artificial Neural Networks (Deep Learning)  
 Neural Networks have recently been adapted as a viable data analytic method. These 
networks  aim to emulate that of the human brain, as they contain “neurons” (linear or non -linear 
computing elements) interconnected in complex ways and organized in layers.  
 A simple perceptron 2 constructs a linear combination of the inputs called the net input. 
Thereafter, an activation function is linked to produce an output, this maps any real input to a 
bounded range. A functional link network introduces a hidden layer in the network. This uses 
nonlinear activation functions to produce a fully nonlinear model (in parameters). Th e resulting 
model is known as an MLP or multilayer perceptron. These models are flexible, general purpose, 
and non -linear and have the ability to yield multiple outputs from many inputs. Given enough 
data, this can approximate to a desir ed degree of accuracy. During building, numerical values 
were filled by using mean imputation, while categorical ones were omitted. An illustration of a 
sample neural network may be found in the Appendix  (Fig. C) (Sarle 2 -5). 
5. Stacking  
Stacking is a n approach for building up classifier ensembles; this refers to a collection of 
classifiers in which their decisions are put together to classify new instances. The algorithm
 
2. A perceptron is a single -layer Neural Network. An interconnected system of perceptrons (MLP) yields a Neural 
Network.",What is Stacking in the context of classifier ensembles? How does it differ from other approaches to building classifier ensembles? Provide an example of how stacking works in practice.
124,"Merekar 17 
 
combines multiple classifiers to induce a higher -level classifier with improved p redictive 
performance (Sakkis et al. 1 -2). In this study, two variants are used. “Best of Family” combines 
the best models from each algorithm, and “All Models” merges all models in a given iteration of 
building. Missing values were handled by the base alg orithms of the ensemble.  
Resampling Methods  
 Generally, in Machine Learning projects, the researcher splits a given dataset into a 
training and test set based on a chosen ratio. The training set is used to build models upon, while 
the test set acts as a pr oxy for how the model will perform on future, unseen data. This method, 
known as the Hold -Out Method, introduces additional error.  
An important concept to understand in this domain is the bias-variance tradeoff  
(sometimes also referred to as the tradeoff between data -fit and complexity). The main point of 
model building is to showcase that it can generalize to unseen data. Maybe counter -intuitively, 
predictive performance is not maximized by learning the training data as precisely as possible. 
An extremely  close fit to the training data is typically not ideal because the model will pick up 
random fluctuations in the data (i.e. noise) and miss the “broader regularities” in the dataset 
(Briscoe and Feldman 3 -4). Using just the Hold -Out method can lead to high  bias, low variance, 
or vice -versa. An illustration of the bias -variance tradeoff may be found in the Appendix (Fig. 
D). 
 Alternatively, r esampling methods can be defined as iteratively fitting models on 
randomly drawn samples of given data. One of these methods, perhaps the most popular, is k-
Fold Cross Validation. The dataset is divided into k folds (row -wise), where each k – 1 folds 
becom e training sets while the remaining fold acts as a test set. An error value is calculated for 
the “test” fold, and finally, the average of each test fold becomes the overall error value","Discuss the advantages and disadvantages of using resampling methods, such as k-Fold Cross Validation, compared to the Hold-Out Method in Machine Learning projects."
125,"Merekar 18 
 
(Rodriguez et al. 569).  This was the resampling method of choice for t his project, with k = 5. An 
illustration may be found below:  
 
 
 
 
 
 
 
 
 
Compared to the simple Hold -Out method, k-Fold Cross Validation has advantages in 
both bias and variance. Bias is reduced in the sense that there are more observations “seen” by 
the model during training. Variance is also reduced due to the fact that there is smaller overlap 
between each training  set. Given these considerations, it is empirically proven that k = 5 or k = 
10 yield test errors that do not suffer from high bias nor very high variance (James et al. 181 -
184).  
Model Evaluation  
 At this point in the framework, multiple models have been b uilt and are ready to be 
compared. The models are constructed correctly and are assumed to be of the best quality 
possible. Before deployment of a model, it is vital to thoroughly evaluate each model, review 
steps taken to build the model, and justify it u sing the business context (Wirth and Hipp 6).  
 
Figure 4 . K-Fold Cross Validation . Ren, Qiubing, et al. “Tectonic Discrimination of Olivine in 
Basalt Using Data Mining Techniques Based on Major Elements: A Comparative Study from 
Multiple Perspectives.” Big Earth Data , vol. 3, no. 1, Jan. 2019, p. 14. DOI.org (Crossref) , 
doi:10.1080/20964471.2019.1572452 .","Based on the information provided, what are the recommended values for k in k-Fold Cross Validation to yield test errors that do not suffer from high bias nor very high variance?"
126,"Merekar 19 
 
 Confusion matrices are used in evaluation of problems of binary classification. It is a 2x2 
table formed by counting the number of the four outcomes of a binary classifier. The two 
“classes” are actual and pr edicted with a positive/negative value for each. The four cells are 
filled with the number of True Positives (TP), False Positives (FP), False Negatives (FN), and 
True Negatives (TN) (Amornsamankul et al. 5).  
In this project, the outcome s of “Died” or “Has not died” are examined. True Positives in 
this case refer to the number of “Died” predicted when the actual value was also “Died.” False 
Positives are the number of “Has not died” predicted when the outcome was actually “Died.” 
False Negatives are the number of predict ed “Died” when the actual was “Has not died.” True 
Negatives are the number of predicted “Has not died” when the actual classification was “Has 
not died”. In this case, False Negatives are more costly than False Positives, so the metric of 
Sensitivity (Rec all) is also relevant in this study.",Why is the metric of Sensitivity (Recall) important in the study mentioned in the context information? How does it relate to the cost of False Negatives versus False Positives?
127,"5).  
In this project, the outcome s of “Died” or “Has not died” are examined. True Positives in 
this case refer to the number of “Died” predicted when the actual value was also “Died.” False 
Positives are the number of “Has not died” predicted when the outcome was actually “Died.” 
False Negatives are the number of predict ed “Died” when the actual was “Has not died.” True 
Negatives are the number of predicted “Has not died” when the actual classification was “Has 
not died”. In this case, False Negatives are more costly than False Positives, so the metric of 
Sensitivity (Rec all) is also relevant in this study.  
 
 
From this, prediction accuracy, sensitivity (recall), and specificity may be derived:  
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑃+𝑇𝑁
𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 
𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦  (𝑅𝑒𝑐𝑎𝑙𝑙 )= 𝑇𝑃
𝑇𝑃+𝐹𝑁 
𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 =𝑇𝑁
𝑇𝑁+𝐹𝑃   Predicted Class  
  Died  Has not died  
Actual Class  Died  TP FP 
Has not died  FN TN 
Figure 5 . One -year Mortality Confusion Matrix .","Using the provided formulas, calculate the prediction accuracy, sensitivity (recall), and specificity based on the values in the 'One-year Mortality Confusion Matrix'."
128,"Merekar 20 
 
Key Metric: Area Under the Receiver Operating Characteristic Curve (AUC)  
 The Receiver Operating Characteristic Curve (ROC Curve) has been identified as a 
viable soluti on of visualizing a classifier’s performance in order to select an optimal decision 
threshold. The term was coined during World War II when it was used in “Signal Detection 
Theory” for radars, but later made its way to diagnostic medicine. It was determine d that an 
“ideal” threshold is almost always a trade -off between sensitivity (True Positives) and specificity 
(True Negatives). Since it may be difficult for a researcher to imagine an ideal “cut -off”, this 
concept was visualized. The Y -Axis represents Sen sitivity while the X -Axis represents 
Specificity. In theory, a researcher would want to achieve both high Sensitivity and Specificity, 
but this is far from practical in application. Hence, there is a trade -off, and either one of the two 
metrics can be opti mized (Bradley 1145).  
 The area under the ROC Curve (referred to as AUC) is widely recognized as the measure 
of a diagnostic test’s discriminatory power. The value of AUC ranges from 0.0 to 1.0, where a 
value of 1.0 implies 100% sensitivity and specificity . A value of 0.5 indicates no discriminative 
value. This entails 50% sensitivity and 50% specificity  (Fan et al. 20) . In terms of this project, 
ROC curves were visualized for each model, and ranked based on AUC. Models with AUC ’s 
that performed better than  the baseline Logistic Regression were deemed the most useful . 
Variable Analysis  
 As there are greater than 400 variables in the full dataset, only significant variables were 
considered in individual variable analyses. Each model produced a list of important variables and 
these were brought to clinicians for validation. Additionally, to  further drill down the number of 
variables, the important variables  from the top model from each dataset was taken. These were 
then counted up to understand the shared important variables throughout all models .",Describe the process of variable analysis in the context of this project. How were significant variables identified and validated by clinicians? What was the significance of identifying shared important variables across all models?
129,"Merekar 21 
 
Chapter 5: Results  
 After the methodology wa s carried out, initial results were obtained for each model per 
dataset. These contain the top two models from each dataset, the baseline Logistic Regression, 
algorithms, AUC measures, Sensitivity, and dataset dimensions. A detailed table depicting all of 
these may be found below.  
 
Algorithm  AUC  Sensitivity (Recall)  
Full Dataset ( 32,018  Observations , 420  Variables)  
Logistic Regression  0.63683332  0.4190646  
 Stacking, All Models  0.681168  N/A 
Extreme Gradient Boosting  0.680403  0.41853034  
 Patients with no LVAD (10,912  Observations , 420  Variables ) 
Logistic Regression  0.57672026  0.45374164  
 Stacking, All Models  0.65443  N/A 
Extreme Gradient Boosting  0.653004  0.41827255  
 Patients with an LVAD (7,700  Observations , 420  Variables ) 
Logistic Regression  0.58331451  
 0.5547305  
 Stacking, All Models  0.65692  
 N/A 
Extreme Gradient Boosting  0.654315  
 0.43852264  
 Patients Recorded Before 2006 (13,406  Observations , 420  Variables ) 
Logistic Regression  0.61300899  
 0.57854533  
 Stacking with All Models  0.674364  
 N/A 
Extreme Gradient Boosting  0.67345  
 0.41904527  
 Patients Recorded After 2006 (18,612  Observations , 420  Variables ) 
Logistic Regression  0.60626572  
 0.46481952  
 Stacking, All Models  0.668372  
 N/A 
Extreme Gradient Boosting  0.66386  
 0.4589736  
 Table  1. Model Performance by Dataset and Algorithm.","Discuss the impact of dataset dimensions on the performance of the models. How does the number of observations and variables in each dataset influence the AUC and Sensitivity values for Logistic Regression, Stacking with All Models, and Extreme Gradient Boosting?"
130,"Merekar 22 
 
Female Donor Female Recipient (2,713  Observations , 420  Variables ) 
Logistic Regression  0.54148685  
 0.5008384  
 Stacking, Best of Family  0.662545  
 N/A 
Extreme Gradient Boosting  0.677088  
 0.53623605  
 Female Donor Male Recipient (2,730  Observations , 420  Variables ) 
Logistic Regression  0.570355  
 0.47577724  
 Stacking, Best of Family  0.647494  
 N/A 
Extreme Gradient Boosting  0.635834  
 
 0.42375335  
 Male Donor Female Recipient (2,713  Observations , 420  Variables ) 
Logistic Regression  0.5310544  
 0.5507362  
 Stacking, All Models  0.651886  
 N/A 
Extreme Gradient Boosting  0.664648  
 0.47671908  
 Male Donor Male Recipient (11,306  Observations , 420  Variables ) 
Logistic Regression  0.57988709  
 0.45201996  
 Stacking, All Models  0.650807  
 N/A 
Extreme Gradient Boosting  0.647327  
 0.41651163  
  
After the initial analysis, a subset of the full dataset was taken. The data was filtered on the basis 
of important variables from the Extreme Gradient Boosting Algorithm on the full dataset and 
models were ran again. The results are displayed below:  
 
Full Dataset with Significant Variables (32,018, 25)  
Algorithm  AUC  Sensitivity (Recall)  
Logistic Regression  0.6603  
 0.4304224  
 Stacking, All Models  0.671926  
 N/A 
Extreme Gradient Boosting  0.670706  
 0.4328194  
  Table  2. Model Performance: Full Dataset with Significant Variables and Algorithms.",Discuss the significance of the AUC and Sensitivity (Recall) values in evaluating the performance of the models on the Full Dataset with Significant Variables. How do these metrics help in assessing the predictive power of the models?
131,"Merekar 23 
 
Following that, more features were removed to make the model more interpretable. The top 16 
variables were chosen based on important variables  from the top model from each dataset. These 
were the significant variables that were most common throughout all m odels from all datasets. 
The models were run again, and the results are displayed below:  
 
Full Dataset with Clinically Significant Variables (32,018, 16) 
Algorithm  AUC  Sensitivity (Recall)  
Logistic Regression  0.6284  
 0.44540313  
 
 Stacking, Best of Family  0.6500 
 N/A 
Extreme Gradient Boosting  0.6491 
 0.46050477  
  
Clinically Significant  Variables  
 Analysis of variables included validation with clinicians. They were able to provide 
additional context as to whether or not models included variables that made sense from a 
physiological standpoint. The top 16 variables included in the most interpretable model are 
displayed on the next page.  Table  3. Model Performance: Full Dataset with Clinically Significant Variables and Algorithms.","Compare the performance of Logistic Regression, Stacking (Best of Family), and Extreme Gradient Boosting algorithms on the full dataset with clinically significant variables. Which algorithm performed the best in terms of AUC and sensitivity?"
132,"Merekar 24 
 
 Top 16 Clinically Significant Variables with Descriptions  
Variable  Description   
Creatinine  of Recipient  Waste product filtered by kidney; a build -up may lead to 
cardiovascular disease.  
 
 Total Bil irubin  of 
Recipient  Associated with liver complications; a strong predictor in 
heart failure.  
 Ischemic Time  The time an organ has spent cooling/warming before 
transplant.  
 
 Most Recent Creatinine 
Measurement  of Recipient  Most recent evaluation of creatinine levels for a recipient 
at the time of listing.  
Age of Donor  Median Age: 32 years old  
Age of Recipient  Median Age: 55 years old  
PVR  of Recipient  Pulmonary vascular resistance for recipient measured at 
listing.  
Right Ventricular Mass of 
Donor  Both estimates of the myocardial mass of that ventricle 
based on echocardiographic measurements.  
Right Ventricular Mass of 
Recipient   
Systolic Pressure of 
Recipient  Pulmonary artery systolic pressure in mm/Hg at 
registration.  
Predictive Heart Mass 
Ratio  Total heart myocardial mass estimated by echocardiogram 
of donor divided by that of the recipient.  
Albumin Measurement of 
Recipient  Albumin measured in the recipient plasma at registration.  
Distance  Distance between the donor and the recipient  in nautical 
miles.  
Cardiac Output  of 
Recipient  Cardiac output of the recipient at registration.  
Recipient Waiting Days  Total days on the UNOS waiting list.  
GPT Measurement of 
Donor  GPT Level measured at time of transplant.  Table  4. Top 16 Clinically Significant Variables with Descriptions.",Discuss the importance of the Predictive Heart Mass Ratio variable in the evaluation of potential heart transplant recipients.
133,"Merekar 25 
 
Chapter 6: Discussion & Insights  
After analyzing all datasets, it was apparent that advanced methods such as Tree Boosting 
(Extreme Gradient Boosting) and Stacking led to higher AUC’s than the baseline  Logistic 
Regression. This is because Tree Based algorithms are usually more complex. Although they 
may be prone to overfitting, pruning and correctly adjusting the number of trees resolves this 
issue. Logistic Regression is simpler to understand a nd less prone to overfitting, however, may 
not always grant optimal predictive performance.  
Running the Stacking algorithm on the full dataset yielded the highest AUC of about 
0.6810. Although this is significant, Stacking is just a collection of models, a nd cannot be truly 
used for interpretation. This is also why there were no Sensitivity metrics for those models. 
Aside from Stacking, Boosting also gave high results. When applied to the full dataset, the model 
created from this algorithm had an AUC of rou ghly 0.6804. In practice however, it would not be 
feasible for a cardiologist to assess 420 parameters (variables) when trying to understand the 
future of a patient. This brings in the idea of real -world use where models must be interpretable . 
When the ful l dataset was cut down by column, 25 variables remained. These were the most 
important variable s deemed by the Boosting model ran on  the full dataset. All algorithms were 
run on that subset of data and the Stacking algorithm yielded an AUC of 0.6719. The B oosting 
algorithm was very similar in performance, with a 0.6707 AUC. After counting recurring 
variables from all model variable importance plots, analysis was reduced to 16 variables. The 
Stacking algorithm gave an AUC of .6500 and the Boosting algorithm gave an AUC of .6491. 
This would be far more interpretable for clinicians as the parameters were brought down to only 
16 variables. Some performance is lost, but the difference is marginal (Fig. 6).","How did the reduction of variables from 420 to 16 impact the interpretability of the models for clinicians, and what was the trade-off in terms of performance?"
134,"Merekar 26 
 
 
 
 
 
 
 
 
 
 
The prior studies discussed in Chapter 3 did not achieve an AUC greater than 0.66 when 
using advanced learning techniques. Through optimal parameter tuning, the Tree Boosting 
algorithms were able to achieve higher performance. In those studies, however, th e trend was 
leaning towards Deep Learning being a long -term solution in this field. In this study, running the 
Deep Learning algorithm on the full dataset only produced an AUC of 0.5706. This could have 
been again, due to data quality issues as neural netw orks do not perform well on rather sparse 
datasets. Even after cleaning, the data was not suitable to implement an effective Deep Learning 
model.  
 From a clinical standpoint, the aforementioned variables are reasonable to build models 
with going forward. C reatinine levels are often greater in patients that are hospitalized with heart 
failure, which validates this variable as a driver of mortality (Smith et al. 14). Liver function 
abnormalities such as high levels of Total Bilirubin are also associated with higher morality (van 
Deursen et al.). Age of Recipient is a good indicator of mortality; older people generally have 
poorer health. Ischemic time and Age of Donor are also reasonable because these represent 
Figure 6. Visual Depiction of Trade -off Betw een Performance and Interpretability for Boosting Models. 
Average AUC on the Y -Axis corresponds to the average of the Top 2 models from the Full Dataset, Full 
Dataset with Significant Variables, and Full Dataset with Clinically Significant Variables.","Based on the visual depiction of the trade-off between performance and interpretability for boosting models in Figure 6, discuss the implications of choosing models with higher interpretability but lower performance versus models with higher performance but lower interpretability. How can this trade-off impact the practical application of predictive models in healthcare settings?"
135,"Merekar 27 
 
worse organ quality. Multiple studies have deemed  that age is an independent risk factor for 
mortality and argued that longer ischemi c times are associated with higher mortality outcomes 
(Kilic et al.).  
Another point to analyze is the comparisons of the row -wise dataset splits. Comparing the 
LVAD support datasets, it was apparent that there was not much of a difference between 
performance of those top models. The same can be said for time period, but models run on 
observations before 2006 yielded a marginally higher AUC. For Donor/Recipient gender 
combinations, it was found that the dataset containing Female Donors and Female Recipients 
gave an AUC of 0.6770. This exceeded the performance of all other Donor/Recipient gender 
combinations.  
Although the models ran on the row -wise splits did not  perform better than models from 
the full dataset with respect to AUC, they are useful in terms of Sensitivity. Albeit not discussed 
as thoroughly as AUC, it is meaningful to analyze this metric  in clinical decision making because 
in this case, False Negat ive Cases would be costly. The Tree Boosting model for the full dataset 
produced a Sensitivity of about .42. After running that algorithm on the Donor/Recipient gender 
combinations datasets, the average Sensitivity of the four was .463. Comparing those two  
metrics, Sensitivity of Donor/Recipient gender combinations saw an increase in Sensitivity by 
8% on average. This supports the claim that Donor/Recipient gender combinations add value in 
clinical decision making (see Fig. H in Appendix).","Why is Sensitivity considered important in clinical decision making, and how did the Sensitivity of Donor/Recipient gender combinations compare to the Tree Boosting model for the full dataset?"
136,"Merekar 28 
 
Chapter 7: Concl usion  
This project aim ed to leverage Machine Learning techniques to predict one -year 
mortality after a heart transplant. Predictive models were constructed to help clinicians better 
understand underlying patterns in data from Donors and Recipients. This wa y, correct procedures 
for treatment may be taken from a medicinal standpoint. This study was continuously validated 
by clinicians, from Data Preparation to Evaluation. The data was split based on clinical factors 
such as LVAD Support, Donor/Recipient Gende r Combinations, and Time Period of Transplant. 
Finally, specific features were selected based on the clinician’s ability to interpret the created 
models.  
As mentioned previously, performance and interpretability vary inversely. This trade -off 
is important to understand in deployment. From the analysis done in this work, the Extreme 
Gradient Boosting model applied to the full dataset with 25 variables is the optimal one. It serves 
as a “middle -ground” between AUC and number of variables. This model performed  better than 
those from prior studies and uses 25 variables. Although it is not the most interpretable, the 
model will perform similarly when exposed to future, unseen data.  
Essentially, there are two schools of thought. The data scientist wants to optimiz e model 
performance, while the clinicians strive for the simplest model to understand. To that end, from a 
clinical standpoint, the model with 16 variables may be the most optimal. There is a modest 
discrepancy in AUC when compared to the model with 25 var iables, and theoretically, the 
clinician would trade marginal model performance for interpretability. The system needs to be 
fed with new data and using 16 parameters rather than 25 is more convenient, easier to 
track/measure, and simpler to enforce data i ntegrity constraints upon.","How was the data split based on clinical factors in this project, and why was it done in that manner? Discuss the significance of selecting specific features based on the clinician's ability to interpret the created models."
137,"Merekar 29 
 
Chapter 8: Implications  
The last phase in the CRISP -DM Framework is Deployment  of a selected model. Rarely, 
creation of the model is not the end of the project as it still needs to be rolled out on a system, 
maintained, and updat ed periodically (Wirth and Hipp 7). It is important to continuously validate 
models built throughout the framework. By doing this, data scientists will be able to gain insight 
on whether or not models fit the context of the problem.  
Although there is a ne ed for the AI in healthcare as highlighted previously, there are a 
few pertinent issues when it comes to deployment and industry -wide adoption of these systems.  
Data Privacy  
 Data is essential for AI and model training, but some patients may be unwilling t o give 
other entities access to private records. Besides for training, a large data supply is needed for 
validation and improvement of these models. For widespread deployment, this sensitive data 
must be shared among numerous institutions. To combat this, these EHR’s must be anonymized 
and patients would need to be fairly informed. The shift to value -based care will support this, and 
further incentivize organizations to collect and ethically maintain this data for analysis (He et al. 
31). 
Data Standardizati on 
 From a data science standpoint, this is an important aspect to consider. Data 
standardization refers to the process of transforming data into a common format to be used for 
analysis. This way, it can be understood regardless of tools and methodologies (He et al. 33). In 
practice, data is collected in many different ways. It is stored in a variety of formats, databases, 
and information systems. Although this data may be formatted a certain way in one organization, 
if it is shared, another organization ma y not be able to properly interpret this for analysis. With",Why is data standardization important in the field of data science? How does data standardization help in ensuring that data can be understood regardless of tools and methodologies used for analysis?
138,"Merekar 30 
 
the complexity and volume of healthcare data in particular, this should occur in the initial phases 
of model development, even before the CRISP -DM Framework is carried out.  
Existing Workforce  
 There has been significant concern throughout multiple industries of Artificial 
Intelligence eliminating the need for human workers. Although some jobs have potential to be 
automated, this will likely limit overall job loss. In healthcare, c osts of automation  technologies 
and regulatory and social acceptance are some reasons as to why this may be curbed (Davenport 
and Kalakota 96).  
To overcome these aforementioned challenges, the workforce itself must understand that 
AI is not here to replace it. Inst ead, the workforce will be able to leverage it to augment existing 
workflows and decision making. For an industry -wide implementation, healthcare professionals 
must develop trust for these systems. Similarly, the onus is on patients to trust institutions t o 
handle their data ethically in hopes of improving their own outcomes .",Why is it important for healthcare professionals to develop trust in AI systems and for patients to trust institutions handling their data ethically?
139,"Merekar 31 
 
Chapter 9: Limitations & Future Directions  
 There are a few limitations this research has faced, the first being the time period of the 
queried data. The UNOS Data spans from 1990 -2016 and does not take into account records after 
that. With the increased importance of data collection in the space, i t is reasonable to assume that 
future records will follow specific constraints. This entails that the data will contain less errors, 
yielding a more thorough analysis. The second limitation would be the interpretation of the 
variables in the full dataset. These are generally subjective since there are over 400, so another 
set of clinicians could have understood these variables from a different perspective than in this 
study. Finally, although discussed thoroughly, it was impossible to complete the CRISP -DM 
Framework. The optimal model was limited to simulation and could not  be deployed in a real use 
case.  
 For future studies, researchers will be able to leverage “cleaner” data from UNOS as data 
standards begin to conform. Aside from that, some questions are posed to future data 
scientist/clinician teams that have not been explored in this study:  
Can mortality be predicted independent of time period?  
 This is particularly significant as this project was limited to one -year mortality. If this 
limitation was rem oved, clinicians would be able to understand what may contribute to 
shorter/longer mortality periods.  
How robust will these models be? How will they scale?  
 Models must adapt to rapid change as new data is collected. Another point to examine 
would be identifying how these models would scale. This is useful from a data science 
perspective, and with the emergence of enterprise -wide data and cloud computing, analytic 
solutions (e.g. Random Forest, Tree Boosting ) have potential to scale rather well .","Discuss the importance of examining the robustness and scalability of models in future studies, especially in the context of rapid data collection and emerging technologies like cloud computing."
140,"Merekar 32 
 
Appendix  
 
 
 
 
 
 
 
 
 
 
Figure A. Detailed Overview of the CRISP -DM Framework. Wirth, Rüdiger, and Jochen Hipp. 
CRISP -DM: Towards a Standard Process Model for Data Mining . p. 6.","According to the document, what is the purpose of the CRISP-DM framework?"
141,"Merekar 33 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure B. Sigmoidal Function in Logistic Regression. Note, the minimum value is 0 and the 
maximum value is 1. Logistic Regression Theory for Practitioners - Towards Data Science . 
https://towardsdatascience.com/the -data-scientists -field-guide -to-logistic -regression -part-1-
intuition -97084b11bd68 . Accessed 16 Apr. 2020.  
 
 
 
 
 
 
 
Figure C. Sample Artificial Neural Network. Management AI: Types Of Machine Learning Systems . 
https://www.forbes.com/sites /davidteich/2018/07/06/management -ai-types -of-machine -learning -
systems/#390b5ba832fb . Accessed 16 Apr. 2020.",Describe the sample artificial neural network shown in Figure C and its relevance in the field of management AI.
142,"Merekar 34 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure D. Bias -Variance Tradeoff. Bias and Variance in Machine Learning - Data Driven Investor - 
Medium. https://medium.com/datadriveninvestor/bias -and-variance -in-machine -learning -51fdd38d1f86 . 
Accessed 16 Apr. 2020.  
 
 
 
 
 
 
 
 
F",How does understanding the bias-variance tradeoff help in improving the overall performance of machine learning algorithms?
143,"Merekar 35 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure E. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset . 
 
 
 
 
 
 
 
 
F 
Figure F. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset  with Significant Variables.  
 
 
 
 
 
 
 
 
F 
Figure G. ROC Curve from Extreme Gradient Boosting Model Applied to Full Dataset  with Clinically Significant Variables.","What is the significance of including clinically significant variables in the Extreme Gradient Boosting model, as shown in Figure G?"
144,"Merekar 36 
 
 
0.4633050280.42567487
0.4 0.41 0.42 0.43 0.44 0.45 0.46 0.47XGBoost Recall Averages Donor/Recipient
Gender CombinationsXGBoost Recall Averages for Full Datasets
(No Row Splits)
SENSITIVITY (RECALL)SENSITIVITY MEASURES FOR TREE 
BOOSTING
Figure H. Sensitivity Comparisons: Full Datasets and Donor/Recipient Gender Combinations.  
 
 
 
 
 
 
 
F","What insights can be gained from the sensitivity comparisons shown in Figure H, regarding the performance of the XGBoost model on different datasets and gender combinations?"
145,"Merekar 37  
References  
Amornsamankul, Somkid , et al. “A Comparison of Machine Learning Algorithms and Their 
Applications.” International Journal of Simulation: Systems, Science & Technology , Aug. 2019, 
pp. 1 –17. DOI.org (Crossref) , doi: 10.501 3/IJSSST.a.20.04.08 . 
 
Beam, Andrew L., and Isaac S. Kohane. “Big Data and Machine Learning in Health Care.” JAMA , 
vol. 319, no. 13, Apr. 2018, pp. 1317 –18. DOI.org (Crossref) , doi: 10.1001/jama.2017.18391 . 
 
Blagus, Rok, and Lara Lusa. “SMOTE for High -Dimensional Class -Imbalanced Data.” BMC 
Bioinformatics , vol. 14, no. 1, Dec. 2013, pp. 1 –16. DOI.org (Crossref) , doi: 10.1186/1471 -2105 -
14-106. 
 
Bradley, Andrew P. “The Use of the Area under the ROC Curve in the Evaluation of Machine 
Learning Algorithms.” Pattern Recognition , vol. 30, no. 7, July 1997, pp. 1145 –59. DOI.org 
(Crossref) , doi: 10.1016/S0031 -3203(96)00142 -2. 
 
Briscoe, Erica, and Jacob Feldman. “Conceptual Complexity and the Bias/Variance Tradeoff.” 
Cognition , vol. 118, no. 1, Jan. 2011, pp. 2 –16. DOI.org (Crossref) , 
doi:10.1016/j.cognition.2010.10.004 . 
 
Burrill, Steve. “Health Care Outlook for 2019: Five Trends That Could Impact Health Plans, 
Hospitals, and Patients.” Deloitte United States .","How do big data and machine learning intersect in the healthcare industry, according to Beam and Kohane's 2018 article in JAMA?"
146,"Bradley, Andrew P. “The Use of the Area under the ROC Curve in the Evaluation of Machine 
Learning Algorithms.” Pattern Recognition , vol. 30, no. 7, July 1997, pp. 1145 –59. DOI.org 
(Crossref) , doi: 10.1016/S0031 -3203(96)00142 -2. 
 
Briscoe, Erica, and Jacob Feldman. “Conceptual Complexity and the Bias/Variance Tradeoff.” 
Cognition , vol. 118, no. 1, Jan. 2011, pp. 2 –16. DOI.org (Crossref) , 
doi:10.1016/j.cognition.2010.10.004 . 
 
Burrill, Steve. “Health Care Outlook for 2019: Five Trends That Could Impact Health Plans, 
Hospitals, and Patients.” Deloitte United States . www2.deloitte.com , 
https://www2.deloitte.com/us/en/pages/life -sciences -and-health -care/articles/health -care-current -
december4 -2018.html . Accessed 2 Mar. 2020.  
 
Chen, Tianqi, and Carlo s Guestrin. “XGBoost: A Scalable Tree Boosting System.” Proceedings of the 
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - 
KDD ’16 , 2016, pp. 785 –94. arXiv.org , doi: 10.1145/2939672.2939785 . 
 
Collier, Matt, et al. Artificial Intelligence: Healthcare’s New Nervous System . Industry Outlook 
Report, Accenture, pp. 1 –8. 
 
D, Shashank, et al. “The Internet of Medical Things (IoMT).” Journal of Pharmaceutical Research , 
vol. 16, no. 4, Dec. 2017, p. 290.  
 
Dag, Ali, et al. “Predicting Heart Transplantation Outcomes through Data Analytics.” Decision 
Support Systems , vol. 94, Nov. 2016, pp. 42 –52.","Discuss the potential impact of the five trends outlined in Steve Burrill's article on health plans, hospitals, and patients in 2019."
147,"785 –94. arXiv.org , doi: 10.1145/2939672.2939785 . 
 
Collier, Matt, et al. Artificial Intelligence: Healthcare’s New Nervous System . Industry Outlook 
Report, Accenture, pp. 1 –8. 
 
D, Shashank, et al. “The Internet of Medical Things (IoMT).” Journal of Pharmaceutical Research , 
vol. 16, no. 4, Dec. 2017, p. 290.  
 
Dag, Ali, et al. “Predicting Heart Transplantation Outcomes through Data Analytics.” Decision 
Support Systems , vol. 94, Nov. 2016, pp. 42 –52. DOI.org (Crossref) , 
doi:10.1016/j.dss.2016.10.005 . 
 
Davenport, Thomas, and Ravi Kalakota. “The Potential for Artificial Intelligence in Healthcare.” 
Future Healthcare Journal , vol. 6, 2019, pp. 94 –98. 
 
Fan, Jerome, et al. “Understanding Receiver Opera ting Characteristic (ROC) Curves.” CJEM , vol. 8, 
no. 01, Jan. 2006, pp. 19 –20. DOI.org (Crossref) , doi: 10.1017/S1481803500013336 .","How can data analytics be used to predict heart transplantation outcomes, as described in the article by Dag, Ali, et al.? Provide examples to support your answer."
148,"Merekar 38  
Gedela, Maheedhar, et al. A Brief Review of Left Ventricular Assist Devices and Their Management . 
pp. 19 -26. 
 
He, Jianxing, et al. “The Practical Implementation of Artificial Intelligence Technologies in 
Medicine.” Nature Medicine , vol. 25, no. 1, Jan. 2019, pp. 30 –36. DOI.org (Crossref) , 
doi:10.1038/s41591 -018-0307 -0. 
 
Heart Failure | National Heart, Lung, and Blood Institute (NHLBI) . 
https ://www.nhlbi.nih.gov/health -topics/heart -failure. Accessed 1 Mar. 2020 . 
 
Heart Transplant | National Heart, Lung, and Blood Institute (NHLBI) . 
https://www.nhlbi.nih.gov/health -topics/ heart -transplant . Accessed 1 Mar. 2020.  
 
Hong, Kimberly N., et al. “Who Is the High -Risk Recipient? Predicting Mortality After Heart 
Transplant Using Pretransplant Donor and Recipient Risk Factors.” The Annals of Thoracic 
Surgery , vol. 92, no. 2, Aug. 2011 , pp. 520 –27. DOI.org (Crossref) , 
doi:10.1016/j.athoracsur.2011.02.086 . 
 
Iyer, Arjun, et al. “Primary Graft Failure after Heart Transplantation.” Journal of Transplantation , 
vol. 2011, 2011, pp. 1 –9. DOI.org (Crossref) , doi: 10.1155/2011/175768 . 
 
James, Gareth, et al. An Introduction to Statistical Learning . Springer New York, 2013. DOI.org 
(Crossref) , doi: 10.1007/978 -1-4614 -7138 -7. 
 
Jiang, Fei, et al. “Artificial Intelligence in Healthcare: Past, Present and Future.” Stroke and Vascular 
Neurology , vol. 2, no. 4, Dec.","Discuss the concept of primary graft failure after heart transplantation as outlined in the article by Iyer, Arjun, et al."
149,"DOI.org (Crossref) , 
doi:10.1016/j.athoracsur.2011.02.086 . 
 
Iyer, Arjun, et al. “Primary Graft Failure after Heart Transplantation.” Journal of Transplantation , 
vol. 2011, 2011, pp. 1 –9. DOI.org (Crossref) , doi: 10.1155/2011/175768 . 
 
James, Gareth, et al. An Introduction to Statistical Learning . Springer New York, 2013. DOI.org 
(Crossref) , doi: 10.1007/978 -1-4614 -7138 -7. 
 
Jiang, Fei, et al. “Artificial Intelligence in Healthcare: Past, Present and Future.” Stroke and Vascular 
Neurology , vol. 2, no. 4, Dec. 2017, pp. 230 –43. DOI.org (Crossref) , doi: 10.1136/svn -2017 -
000101 . 
 
Johnston, Stephen S., et al. “Using Machine Learning Applied to Real -World Healthcare Data for 
Predictive Analytics: An Applied Example in Bariatric Surgery.” Value in Health , vol. 22, no. 5, 
May 2019, pp. 580 –86. DOI.org (Crossref) , doi: 10.1016/j.jval.2019.01.011 . 
 
Kilic, Ahmet, et al. “Donor Selection in Heart Transplantation.” Journal of Thoracic Disease , vol. 6, 
no. 8, 2014, pp. 1097 –104. 
 
Medved, Dennis, et al. “Improving Prediction of Heart Transplantation Outcome Using Deep 
Learning Techniques.” Scientific Reports , vol. 8, no. 1, Feb. 2018, pp. 1 –9. DOI.org (Crossref) , 
doi:10.1038/s41598 -018-21417 -7. 
 
Miller, P. Elliott, et al.","Discuss the use of machine learning in predictive analytics for healthcare, as outlined in the article by Johnston et al. What specific example in bariatric surgery was provided to demonstrate this application?"
150,"22, no. 5, 
May 2019, pp. 580 –86. DOI.org (Crossref) , doi: 10.1016/j.jval.2019.01.011 . 
 
Kilic, Ahmet, et al. “Donor Selection in Heart Transplantation.” Journal of Thoracic Disease , vol. 6, 
no. 8, 2014, pp. 1097 –104. 
 
Medved, Dennis, et al. “Improving Prediction of Heart Transplantation Outcome Using Deep 
Learning Techniques.” Scientific Reports , vol. 8, no. 1, Feb. 2018, pp. 1 –9. DOI.org (Crossref) , 
doi:10.1038/s41598 -018-21417 -7. 
 
Miller, P. Elliott, et al. “Predictive Abilities of Machine Learning Techniques May Be Limited by 
Dataset Characteristics: Insights From the UNOS Database.” Journal of Cardiac Failure , vol. 
25, no. 6, June 2019 , pp. 479 –83. DOI.org (Crossref) , doi: 10.1016/j.cardfail.2019.01.018 . 
 
Mitchell, Rory, and Eibe Frank. “Accelerating the XGBoost  Algorithm Using GPU Computing.” 
PeerJ Computer Science , vol. 3, July 2017, pp. 1 –28. DOI.org (Crossref) , doi: 10.7717/peerj -
cs.127 .","In the study by Mitchell and Frank, how did they accelerate the XGBoost algorithm and what was the significance of using GPU computing?"
151,"Merekar 39  
NEJM Catalyst. “What Is Value -Based Healthcare?” Catalyst Carryover , vol. 3, no. 1, Massachusetts 
Medical Society, Jan. 2017. catalyst.nejm.org (Atypon) , doi: 10.1056/CAT.17.0558 . 
 
Parreco, Joshua, and Matthew Chatoor. Comparing Machine Learning Algorithms for Predicting 
Acute Kidney Injury . no. 7, July 2019, pp. 725 –29. 
 
Rodriguez, J. D., et al. “Sensitivity Analysis of K -Fold Cross Validation in Prediction Error 
Estimation.” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 32, no. 3, 
Mar. 2010, pp. 569 –75. DOI.org (Crossref) , doi: 10.1109/TPAMI.2009.187 . 
 
Roski, Joachim, et al. “Creating Value In Health Care Through Big Data: Opportunities And Policy 
Implications.” Health Affairs , vol. 33, no. 7, July 2 014, pp. 1115 –22. DOI.org (Crossref) , 
doi:10.1377/hlthaff.2014.0147 . 
 
Sakkis, Georgios, et al. Stacking Classifiers for Anti -Spam Filtering of e -Mail . 2001, pp. 1 –7. 
 
Sarle, Warren S. Neural Network s and Statistical Models . Apr. 1994, pp. 1 –12. 
 
Sherman, Rick. Business Intelligence Guidebook: From Data Integration to Analytics . Elsevier, 
Morgan Kaufmann, 2015.  
 
Smith, Grace L., et al. “Worsening Renal Function: What Is a Clinically Meaningful Change in 
Creatinine during Hospitalization with Heart Failure?” Journal of Cardiac Failure , vol. 9, no. 1, 
Feb. 2003, pp. 13 –25. DOI.org (Crossref) , doi: 10.1054/jcaf.2003.3 .","Discuss the significance of big data in creating value in healthcare, as outlined in the article by Roski et al. What are the opportunities and policy implications highlighted in the study?"
152,"Sakkis, Georgios, et al. Stacking Classifiers for Anti -Spam Filtering of e -Mail . 2001, pp. 1 –7. 
 
Sarle, Warren S. Neural Network s and Statistical Models . Apr. 1994, pp. 1 –12. 
 
Sherman, Rick. Business Intelligence Guidebook: From Data Integration to Analytics . Elsevier, 
Morgan Kaufmann, 2015.  
 
Smith, Grace L., et al. “Worsening Renal Function: What Is a Clinically Meaningful Change in 
Creatinine during Hospitalization with Heart Failure?” Journal of Cardiac Failure , vol. 9, no. 1, 
Feb. 2003, pp. 13 –25. DOI.org (Crossref) , doi: 10.1054/jcaf.2003.3 . 
 
Srivastava, Siddharth, et al. “Deep Learning for Health Informatics: Recent Trends and Future 
Directions.” 2017 International Conference on Advances in Computing, Communications and 
Informatics (ICACCI ), IEEE, 2017, pp. 1665 –70. DOI.org (Crossref) , 
doi:10.1109/ICACCI.2017.8126082 . 
 
United Network for Organ Sharing | UNOS | US Organ Transplantation . https://unos .org/. Accessed 
1 Mar. 2020.  
 
van Deursen, V. M., et al. “Abnormal Liver Function in Relation to Hemodynamic Profile in Heart 
Failure Patients.” Journal of Cardiac Failure , vol. 16, no. 1, Jan. 2010, pp. 84 –90. DOI.org 
(Crossref) , doi: 10.1016/j.cardfail.2009.08.002 . 
 
Wirth, Rüdiger, and Jochen Hipp. CRISP -DM: Towards a Standard Process Model for Data Mining . 
pp. 1 –10.","In the research by van Deursen et al., how is abnormal liver function related to the hemodynamic profile in heart failure patients?"
